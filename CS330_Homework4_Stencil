{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS330_Homework4_Stencil","provenance":[{"file_id":"1VUsEXSj9cT_PDDH1JPqowQwyShFwbLsj","timestamp":1629824995684}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZZx5QPwD0p5","executionInfo":{"status":"ok","timestamp":1633157736106,"user_tz":-180,"elapsed":812,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"1916ca8a-e575-407a-97f3-3b77034453a9"},"source":["import os\n","from google_drive_downloader import GoogleDriveDownloader as gdd\n","\n","# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n","if not os.path.isdir('./dream_template'):\n","    gdd.download_file_from_google_drive(file_id='1O8k6UWSbJOczjQm5-e9g3y1k3FDew1Yn',\n","                                        dest_path='./dream_template.zip',\n","                                        unzip=True)\n","    !mv dream_template/* ./\n","    \n","required_files = ['config.py', 'meta_exploration.py', 'replay.py', 'schedule.py',\n","                  'policy.py', 'requirements.txt', 'utils.py', 'relabel.py', \n","                  'rl.py', 'wrappers.py', 'grid.py', 'render.py', 'city.py']\n","for f in required_files:\n","  assert os.path.isfile(f)\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 1O8k6UWSbJOczjQm5-e9g3y1k3FDew1Yn into ./dream_template.zip... Done.\n","Unzipping...Done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fy9Sa3pZEUHW","executionInfo":{"elapsed":191509,"status":"ok","timestamp":1633020247538,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"},"user_tz":-180},"outputId":"5ecc3546-8ca6-4154-b00b-3d8ffe6e8680"},"source":["!pip install -r requirements.txt\n","!pip install -U tensorboard\n","%load_ext tensorboard"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Obtaining gym_minigrid from git+https://github.com/maximecb/gym-minigrid.git@a5f4ef3684875eba613087dcef21be57c186bdc8#egg=gym_minigrid (from -r requirements.txt (line 14))\n","  Cloning https://github.com/maximecb/gym-minigrid.git (to revision a5f4ef3684875eba613087dcef21be57c186bdc8) to ./src/gym-minigrid\n","  Running command git clone -q https://github.com/maximecb/gym-minigrid.git /content/src/gym-minigrid\n","  Running command git rev-parse -q --verify 'sha^a5f4ef3684875eba613087dcef21be57c186bdc8'\n","  Running command git fetch -q https://github.com/maximecb/gym-minigrid.git a5f4ef3684875eba613087dcef21be57c186bdc8\n","  Running command git checkout -q a5f4ef3684875eba613087dcef21be57c186bdc8\n","Obtaining gym_miniworld from git+https://github.com/maximecb/gym-miniworld.git@7531d6ca5a4de2361f6d00d1b24bfa135075736b#egg=gym_miniworld (from -r requirements.txt (line 15))\n","  Cloning https://github.com/maximecb/gym-miniworld.git (to revision 7531d6ca5a4de2361f6d00d1b24bfa135075736b) to ./src/gym-miniworld\n","  Running command git clone -q https://github.com/maximecb/gym-miniworld.git /content/src/gym-miniworld\n","  Running command git rev-parse -q --verify 'sha^7531d6ca5a4de2361f6d00d1b24bfa135075736b'\n","  Running command git fetch -q https://github.com/maximecb/gym-miniworld.git 7531d6ca5a4de2361f6d00d1b24bfa135075736b\n","  Running command git checkout -q 7531d6ca5a4de2361f6d00d1b24bfa135075736b\n","Collecting absl-py==0.9.0\n","  Downloading absl-py-0.9.0.tar.gz (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 10.1 MB/s \n","\u001b[?25hCollecting cachetools==4.0.0\n","  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\n","Collecting certifi==2019.11.28\n","  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 46.5 MB/s \n","\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.0.4)\n","Collecting cloudpickle==1.2.2\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.10.0)\n","Collecting future==0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 33.1 MB/s \n","\u001b[?25hCollecting gitdb==4.0.4\n","  Downloading gitdb-4.0.4-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n","\u001b[?25hCollecting GitPython==3.1.1\n","  Downloading GitPython-3.1.1-py3-none-any.whl (450 kB)\n","\u001b[K     |████████████████████████████████| 450 kB 44.3 MB/s \n","\u001b[?25hCollecting google-auth==1.11.2\n","  Downloading google_auth-1.11.2-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 6.0 MB/s \n","\u001b[?25hCollecting google-auth-oauthlib==0.4.1\n","  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n","Collecting grpcio==1.27.2\n","  Downloading grpcio-1.27.2-cp37-cp37m-manylinux2010_x86_64.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 30.2 MB/s \n","\u001b[?25hCollecting gym==0.16.0\n","  Downloading gym-0.16.0.tar.gz (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 46.1 MB/s \n","\u001b[?25hCollecting idna==2.9\n","  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 6.3 MB/s \n","\u001b[?25hCollecting joblib==0.14.1\n","  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 68.7 MB/s \n","\u001b[?25hCollecting kiwisolver==1.1.0\n","  Downloading kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 10.7 MB/s \n","\u001b[?25hCollecting Markdown==3.2.1\n","  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 8.8 MB/s \n","\u001b[?25hCollecting matplotlib==3.0.3\n","  Downloading matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0 MB)\n","\u001b[K     |████████████████████████████████| 13.0 MB 90 kB/s \n","\u001b[?25hCollecting numpy==1.18.1\n","  Downloading numpy-1.18.1-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n","\u001b[?25hCollecting oauthlib==3.1.0\n","  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 66.5 MB/s \n","\u001b[?25hCollecting Pillow==7.0.0\n","  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 42.8 MB/s \n","\u001b[?25hCollecting protobuf==3.11.3\n","  Downloading protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 37.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (0.4.8)\n","Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (0.2.8)\n","Collecting pycodestyle==2.5.0\n","  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (1.5.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (2.4.7)\n","Collecting python-dateutil==2.8.1\n","  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n","\u001b[K     |████████████████████████████████| 227 kB 67.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (2.23.0)\n","Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (1.3.0)\n","Collecting rsa==4.0\n","  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (0.22.2.post1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (1.4.1)\n","Collecting six==1.14.0\n","  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (0.0)\n","Collecting smmap==3.0.2\n","  Downloading smmap-3.0.2-py2.py3-none-any.whl (25 kB)\n","Collecting tensorboard==2.1.0\n","  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 38.5 MB/s \n","\u001b[?25hCollecting torch==1.4.0\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[K     |████████████████████████████████| 753.4 MB 7.1 kB/s \n","\u001b[?25hCollecting torchvision==0.5.0\n","  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 40.1 MB/s \n","\u001b[?25hCollecting tqdm==4.42.1\n","  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.1 MB/s \n","\u001b[?25hCollecting urllib3==1.25.8\n","  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 63.0 MB/s \n","\u001b[?25hCollecting Werkzeug==1.0.0\n","  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 52.6 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.11.2->-r requirements.txt (line 10)) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0->-r requirements.txt (line 39)) (0.37.0)\n","Building wheels for collected packages: absl-py, future, gym\n","  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121940 sha256=c8eee7374abc13a47fd490b3bfa3bb9e5d891aa92bf5bdd0c20d2bc080891e47\n","  Stored in directory: /root/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=2a19b7d868f44a31496474d94fe9b372acfec42e73821b7f0205c7401a2b505d\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.16.0-py3-none-any.whl size=1648671 sha256=ed8d38e57e98caddd9706343da1f9d711c6cc909b25a3641f180841dc8cfba01\n","  Stored in directory: /root/.cache/pip/wheels/eb/89/f3/c073e53d947fd5d420d799c854b35019265ad865031323c867\n","Successfully built absl-py future gym\n","Installing collected packages: urllib3, idna, certifi, six, rsa, oauthlib, numpy, future, cachetools, smmap, joblib, google-auth, cloudpickle, Werkzeug, torch, python-dateutil, protobuf, Pillow, Markdown, kiwisolver, gym, grpcio, google-auth-oauthlib, gitdb, absl-py, tqdm, torchvision, tensorboard, pycodestyle, matplotlib, gym-miniworld, gym-minigrid, GitPython\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: idna\n","    Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2021.5.30\n","    Uninstalling certifi-2021.5.30:\n","      Successfully uninstalled certifi-2021.5.30\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: rsa\n","    Found existing installation: rsa 4.7.2\n","    Uninstalling rsa-4.7.2:\n","      Successfully uninstalled rsa-4.7.2\n","  Attempting uninstall: oauthlib\n","    Found existing installation: oauthlib 3.1.1\n","    Uninstalling oauthlib-3.1.1:\n","      Successfully uninstalled oauthlib-3.1.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 4.2.2\n","    Uninstalling cachetools-4.2.2:\n","      Successfully uninstalled cachetools-4.2.2\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.0.1\n","    Uninstalling joblib-1.0.1:\n","      Successfully uninstalled joblib-1.0.1\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 1.35.0\n","    Uninstalling google-auth-1.35.0:\n","      Successfully uninstalled google-auth-1.35.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 1.0.1\n","    Uninstalling Werkzeug-1.0.1:\n","      Successfully uninstalled Werkzeug-1.0.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: Markdown\n","    Found existing installation: Markdown 3.3.4\n","    Uninstalling Markdown-3.3.4:\n","      Successfully uninstalled Markdown-3.3.4\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.3.2\n","    Uninstalling kiwisolver-1.3.2:\n","      Successfully uninstalled kiwisolver-1.3.2\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.40.0\n","    Uninstalling grpcio-1.40.0:\n","      Successfully uninstalled grpcio-1.40.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 0.12.0\n","    Uninstalling absl-py-0.12.0:\n","      Successfully uninstalled absl-py-0.12.0\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.62.3\n","    Uninstalling tqdm-4.62.3:\n","      Successfully uninstalled tqdm-4.62.3\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.6.0\n","    Uninstalling tensorboard-2.6.0:\n","      Successfully uninstalled tensorboard-2.6.0\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Running setup.py develop for gym-miniworld\n","  Running setup.py develop for gym-minigrid\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.4.0 which is incompatible.\n","tensorflow 2.6.0 requires absl-py~=0.10, but you have absl-py 0.9.0 which is incompatible.\n","tensorflow 2.6.0 requires grpcio<2.0,>=1.37.0, but you have grpcio 1.27.2 which is incompatible.\n","tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.1 which is incompatible.\n","tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n","tensorflow 2.6.0 requires tensorboard~=2.6, but you have tensorboard 2.1.0 which is incompatible.\n","tensorflow-probability 0.13.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\n","tensorflow-metadata 1.2.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.3 which is incompatible.\n","pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 4.0.0 which is incompatible.\n","plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.42.1 which is incompatible.\n","mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n","kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.18.1 which is incompatible.\n","googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.11.3 which is incompatible.\n","google-colab 1.0.0 requires google-auth>=1.17.2, but you have google-auth 1.11.2 which is incompatible.\n","google-colab 1.0.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n","google-api-python-client 1.12.8 requires google-auth>=1.16.0, but you have google-auth 1.11.2 which is incompatible.\n","google-api-core 1.26.3 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 1.11.2 which is incompatible.\n","google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.3 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 7.0.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.1 Markdown-3.2.1 Pillow-7.0.0 Werkzeug-1.0.0 absl-py-0.9.0 cachetools-4.0.0 certifi-2019.11.28 cloudpickle-1.2.2 future-0.18.2 gitdb-4.0.4 google-auth-1.11.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 gym-0.16.0 gym-minigrid-1.0.1 gym-miniworld-2018.8.1 idna-2.9 joblib-0.14.1 kiwisolver-1.1.0 matplotlib-3.0.3 numpy-1.18.1 oauthlib-3.1.0 protobuf-3.11.3 pycodestyle-2.5.0 python-dateutil-2.8.1 rsa-4.0 six-1.14.0 smmap-3.0.2 tensorboard-2.1.0 torch-1.4.0 torchvision-0.5.0 tqdm-4.42.1 urllib3-1.25.8\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","certifi","dateutil","google","idna","kiwisolver","matplotlib","mpl_toolkits","numpy","six","urllib3"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.1.0)\n","Collecting tensorboard\n","  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.11.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.9.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.11.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.2.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.18.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.27.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.14.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.0.0)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2019.11.28)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n","Installing collected packages: tensorboard\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.1.0\n","    Uninstalling tensorboard-2.1.0:\n","      Successfully uninstalled tensorboard-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.6.0 requires absl-py~=0.10, but you have absl-py 0.9.0 which is incompatible.\n","tensorflow 2.6.0 requires grpcio<2.0,>=1.37.0, but you have grpcio 1.27.2 which is incompatible.\n","tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.1 which is incompatible.\n","tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n","kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.18.1 which is incompatible.\u001b[0m\n","Successfully installed tensorboard-2.6.0\n"]}]},{"cell_type":"code","metadata":{"id":"T2VeXzosGdLP"},"source":["# embed.py\n","import abc\n","import collections\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import distributions as td\n","from torch.nn import functional as F\n","import grid\n","import relabel\n","import utils\n","\n","\n","class Embedder(abc.ABC, nn.Module):\n","  \"\"\"Defines the embedding of an object in the forward method.\n","\n","  Subclasses should register to the from_config method.\n","  \"\"\"\n","\n","  def __init__(self, embed_dim):\n","    \"\"\"Sets the embed dim.\n","\n","    Args:\n","      embed_dim (int): the dimension of the outputted embedding.\n","    \"\"\"\n","    super().__init__()\n","    self._embed_dim = embed_dim\n","\n","  @property\n","  def embed_dim(self):\n","    \"\"\"Returns the dimension of the output (int).\"\"\"\n","    return self._embed_dim\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    \"\"\"Constructs and returns Embedder from config.\n","\n","    Args:\n","      config (Config): parameters for constructing the Embedder.\n","\n","    Returns:\n","      Embedder\n","    \"\"\"\n","    config_type = config.get(\"type\")\n","    if config_type == \"simple_grid_state\":\n","      return SimpleGridStateEmbedder.from_config(config)\n","    elif config_type == \"fixed_vocab\":\n","      return FixedVocabEmbedder.from_config(config)\n","    elif config_type == \"linear\":\n","      return LinearEmbedder.from_config(config)\n","    else:\n","      raise ValueError(\"Config type {} not supported\".format(config_type))\n","\n","\n","def get_state_embedder(env):\n","  \"\"\"Returns the appropriate type of embedder given the environment type.\"\"\"\n","  env = env.unwrapped\n","  if isinstance(env.unwrapped, grid.GridEnv):\n","    return SimpleGridStateEmbedder\n","\n","  # Dependencies on OpenGL, so only load if absolutely necessary\n","  from envs.miniworld import sign\n","  if isinstance(env, sign.MiniWorldSign):\n","    return MiniWorldEmbedder\n","\n","  raise ValueError()\n","\n","\n","class TransitionEmbedder(Embedder):\n","  def __init__(self, state_embedder, action_embedder, reward_embedder, embed_dim):\n","    super().__init__(embed_dim)\n","\n","    self._state_embedder = state_embedder\n","    self._action_embedder = action_embedder\n","    self._reward_embedder = reward_embedder\n","    reward_embed_dim = (\n","        0 if reward_embedder is None else reward_embedder.embed_dim)\n","\n","    self._transition_embedder = nn.Sequential(\n","        nn.Linear(\n","          self._state_embedder.embed_dim * 2 +\n","          self._action_embedder.embed_dim + reward_embed_dim,\n","          128),\n","        nn.ReLU(),\n","        nn.Linear(128, embed_dim)\n","    )\n","\n","  def forward(self, experiences):\n","    state_embeds = self._state_embedder(\n","        [exp.state.observation for exp in experiences])\n","    next_state_embeds = self._state_embedder(\n","        [exp.next_state.observation for exp in experiences])\n","    action_embeds = self._action_embedder([exp.action for exp in experiences])\n","    embeddings = [state_embeds, next_state_embeds, action_embeds]\n","    if self._reward_embedder is not None:\n","      embeddings.append(self._reward_embedder(\n","          [exp.next_state.prev_reward for exp in experiences]))\n","    transition_embeds = self._transition_embedder(torch.cat(embeddings, -1))\n","    return transition_embeds\n","\n","  @classmethod\n","  def from_config(cls, config, env):\n","    state_embedder = get_state_embedder(env)(\n","        env.observation_space[\"observation\"],\n","        config.get(\"experience_embedder\").get(\"state_embed_dim\"))\n","    action_embedder = FixedVocabEmbedder(\n","        env.action_space.n,\n","        config.get(\"experience_embedder\").get(\"action_embedder\").get(\"embed_dim\"))\n","    return cls(state_embedder, action_embedder, config.get(\"embed_dim\"))\n","\n","\n","class TrajectoryEmbedder(Embedder, relabel.RewardLabeler):\n","  def __init__(self, transition_embedder, id_embedder, penalty, embed_dim):\n","    super().__init__(embed_dim)\n","\n","    self._transition_embedder = transition_embedder\n","    self._id_embedder = id_embedder\n","    self._transition_lstm = nn.LSTM(transition_embedder.embed_dim, 128)\n","    self._transition_fc_layer = nn.Linear(128, 128)\n","    self._transition_output_layer = nn.Linear(128, embed_dim)\n","    self._penalty = penalty\n","    self._use_ids = True\n","\n","  def use_ids(self, use):\n","    self._use_ids = use\n","\n","  def _compute_contexts(self, trajectories):\n","    \"\"\"Returns contexts and masks.\n","\n","    Args:\n","      trajectories (list[list[Experience]]): see forward().\n","\n","    Returns:\n","      id_contexts (torch.FloatTensor): tensor of shape (batch_size, embed_dim)\n","        embedding the id's in the trajectories.\n","      all_transition_contexts (torch.FloatTensor): tensor of shape\n","        (batch_size, max_len + 1, embed_dim) embedding the sequences of states\n","        and actions in the trajectories.\n","      transition_contexts (torch.FloatTensor): tensor of shape\n","        (batch_size, embed_dim) equal to the last unpadded value in\n","        all_transition_contexts.\n","      mask (torch.BoolTensor): tensor of shape (batch_size, max_len + 1).\n","        The value is False if the trajectory_contexts value should be masked.\n","    \"\"\"\n","    # trajectories: (batch_size, max_len)\n","    # mask: (batch_size, max_len)\n","    padded_trajectories, mask = utils.pad(trajectories)\n","    sequence_lengths = torch.tensor([len(traj) for traj in trajectories]).long()\n","\n","    # (batch_size * max_len, embed_dim)\n","    transition_embed = self._transition_embedder(\n","        [exp for traj in padded_trajectories for exp in traj])\n","\n","    # pack_padded_sequence relies on the default tensor type not\n","    # being a CUDA tensor.\n","    torch.set_default_tensor_type(torch.FloatTensor)\n","    # Sorted only required for ONNX\n","    padded_transitions = nn.utils.rnn.pack_padded_sequence(\n","        transition_embed.reshape(mask.shape[0], mask.shape[1], -1),\n","        sequence_lengths, batch_first=True, enforce_sorted=False)\n","    if torch.cuda.is_available():\n","      torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","\n","    transition_hidden_states = self._transition_lstm(padded_transitions)[0]\n","    # (batch_size, max_len, hidden_dim)\n","    transition_hidden_states, hidden_lengths = nn.utils.rnn.pad_packed_sequence(\n","        transition_hidden_states, batch_first=True)\n","    initial_hidden_states = torch.zeros(\n","        transition_hidden_states.shape[0], 1,\n","        transition_hidden_states.shape[-1])\n","    # (batch_size, max_len + 1, hidden_dim)\n","    transition_hidden_states = torch.cat(\n","        (initial_hidden_states, transition_hidden_states), 1)\n","    transition_hidden_states = F.relu(\n","        self._transition_fc_layer(transition_hidden_states))\n","    # (batch_size, max_len + 1, embed_dim)\n","    all_transition_contexts = self._transition_output_layer(\n","        transition_hidden_states)\n","\n","    # (batch_size, 1, embed_dim)\n","    # Don't need to subtract 1 off of hidden_lengths as transition_contexts is\n","    # padded with init hidden state at the beginning.\n","    indices = hidden_lengths.unsqueeze(-1).unsqueeze(-1).expand(\n","        hidden_lengths.shape[0], 1, all_transition_contexts.shape[2]).to(\n","            all_transition_contexts.device)\n","    transition_contexts = all_transition_contexts.gather(1, indices).squeeze(1)\n","\n","    # (batch_size, embed_dim)\n","    id_contexts = self._id_embedder(\n","        torch.tensor([traj[0].state.env_id for traj in trajectories]))\n","\n","    # don't mask the initial hidden states (batch_size, max_len + 1)\n","    mask = torch.cat(\n","        (torch.ones(transition_contexts.shape[0], 1).bool(), mask), -1)\n","    return id_contexts, all_transition_contexts, transition_contexts, mask\n","\n","  def _compute_losses(\n","      self, trajectories, id_contexts, all_transition_contexts,\n","      transition_contexts, mask):\n","    \"\"\"Computes losses based on the return values of _compute_contexts.\n","\n","    Args:\n","      See return values of _compute_contexts.\n","\n","    Returns:\n","      losses (dict(str: torch.FloatTensor)): see forward().\n","    \"\"\"\n","    del trajectories\n","\n","    ##################### TODO - ADD YOUR CODE HERE  #########################\n","\n","\n","    ##########################################################################\n","    transition_context_loss = (\n","        transition_context_loss * mask).sum() / mask.sum()\n","\n","    cutoff = torch.ones(id_contexts.shape[0]) * 10\n","\n","    losses = {\n","      \"transition_context_loss\": transition_context_loss,\n","      \"id_context_loss\": torch.max((id_contexts ** 2).sum(-1), cutoff).mean()\n","    }\n","    return losses\n","\n","  def forward(self, trajectories):\n","    \"\"\"Embeds a batch of trajectories.\n","\n","    Args:\n","      trajectories (list[list[Experience]]): batch of trajectories, where each\n","        trajectory comes from the same episode.\n","\n","    Returns:\n","      embedding (torch.FloatTensor): tensor of shape (batch_size, embed_dim)\n","        embedding the trajectories. This embedding is based on the ids if\n","        use_ids is True, otherwise based on the transitions.\n","      losses (dict(str: torch.FloatTensor)): maps auxiliary loss names to their\n","        values.\n","    \"\"\"\n","    id_contexts, all_transition_contexts, transition_contexts, mask = (\n","        self._compute_contexts(trajectories))\n","    contexts = (id_contexts + 0.1 * torch.randn_like(id_contexts)\n","                if self._use_ids else transition_contexts)\n","\n","    losses = self._compute_losses(\n","        trajectories, id_contexts, all_transition_contexts,\n","        transition_contexts, mask)\n","    return contexts, losses\n","\n","  def label_rewards(self, trajectories):\n","    \"\"\"Computes rewards for each experience in the trajectory.\n","\n","    Args:\n","      trajectories (list[list[Experience]]): batch of trajectories.\n","\n","    Returns:\n","      rewards (torch.FloatTensor): of shape (batch_size, max_seq_len) where\n","        rewards[i][j] is the rewards for the experience trajectories[i][j].\n","        This is padded with zeros and is detached from the graph.\n","      distances (torch.FloatTensor): of shape (batch_size, max_seq_len + 1)\n","        equal to ||f(e) - g(\\tau^e_{:t})|| for each t.\n","    \"\"\"\n","    id_contexts, all_transition_contexts, _, mask = self._compute_contexts(\n","        trajectories)\n","    \n","    ##################### TODO - ADD YOUR CODE HERE  #########################\n","\n","    ##########################################################################\n","    return (rewards * mask[:, 1:]).detach(), distances\n","\n","\n","class InstructionPolicyEmbedder(Embedder):\n","  \"\"\"Embeds (s, i, \\tau^e) where:\n","\n","    - s is the current state\n","    - i is the current instruction\n","    - \\tau^e is an exploration trajectory (s_0, a_0, s_1, ..., s_T)\n","  \"\"\"\n","\n","  def __init__(self, trajectory_embedder, obs_embedder, instruction_embedder,\n","               embed_dim):\n","    \"\"\"Constructs around embedders for each component.\n","\n","    Args:\n","      trajectory_embedder (TrajectoryEmbedder): embeds batches of \\tau^e\n","        (list[list[rl.Experience]]).\n","      obs_embedder (Embedder): embeds batches of states s.\n","      instruction_embedder (Embedder): embeds batches of instructions i.\n","      embed_dim (int): see Embedder.\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    self._obs_embedder = obs_embedder\n","    self._instruction_embedder = instruction_embedder\n","    self._trajectory_embedder = trajectory_embedder\n","    self._fc_layer = nn.Linear(\n","        obs_embedder.embed_dim + instruction_embedder.embed_dim +\n","        self._trajectory_embedder.embed_dim, 256)\n","    self._final_layer = nn.Linear(256, embed_dim)\n","\n","  def forward(self, states, hidden_state):\n","    del hidden_state\n","\n","    ##################### TODO - ADD YOUR CODE HERE  #########################\n","    obs_embed = self._obs_embedder([state.observation for state in states])\n","    instruction_embed = self._instruction_embedder(\n","          [torch.tensor(state.instructions) for state in states])\n","    traj_embeds, aux_losses = self._trajectory_embedder()\n","    input_embeddings = torch.cat((obj_embed, instruction_embed, self._trajectory_embedder), dim=-1)  \n","    ##########################################################################\n","    hidden = F.relu(self._fc_layer(input_embeddings))\n","    return self._final_layer(hidden), aux_losses\n","\n","  @classmethod\n","  def from_config(cls, config, env):\n","    \"\"\"Returns a configured InstructionPolicyEmbedder.\n","\n","    Args:\n","      config (Config): see Embedder.from_config.\n","      env (gym.Wrapper): the environment to run on. Expects this to be wrapped\n","        with an InstructionWrapper.\n","\n","    Returns:\n","      InstructionPolicyEmbedder: configured according to config.\n","    \"\"\"\n","    obs_embedder = get_state_embedder(env)(\n","        env.observation_space[\"observation\"],\n","        config.get(\"obs_embedder\").get(\"embed_dim\"))\n","    # Use SimpleGridEmbeder since these are just discrete vars\n","    instruction_embedder = SimpleGridStateEmbedder(\n","        env.observation_space[\"instructions\"],\n","        config.get(\"instruction_embedder\").get(\"embed_dim\"))\n","\n","    transition_config = config.get(\"transition_embedder\")\n","    state_embedder = get_state_embedder(env)(\n","        env.observation_space[\"observation\"],\n","        transition_config.get(\"state_embed_dim\"))\n","    action_embedder = FixedVocabEmbedder(\n","        env.action_space.n, transition_config.get(\"action_embed_dim\"))\n","    reward_embedder = None\n","    if transition_config.get(\"reward_embed_dim\") is not None:\n","      reward_embedder = LinearEmbedder(\n","          1, transition_config.get(\"reward_embed_dim\"))\n","    transition_embedder = TransitionEmbedder(\n","        state_embedder, action_embedder, reward_embedder,\n","        transition_config.get(\"embed_dim\"))\n","    id_embedder = IDEmbedder(\n","        env.observation_space[\"env_id\"].high,\n","        config.get(\"transition_embedder\").get(\"embed_dim\"))\n","    if config.get(\"trajectory_embedder\").get(\"type\") == \"ours\":\n","      trajectory_embedder = TrajectoryEmbedder(\n","          transition_embedder, id_embedder,\n","          config.get(\"trajectory_embedder\").get(\"penalty\"),\n","          transition_embedder.embed_dim)\n","    else:\n","      raise ValueError(\"Unsupported trajectory embedder {}\".format(\n","        config.get(\"trajectory_embedder\")))\n","    return cls(trajectory_embedder, obs_embedder, instruction_embedder,\n","               config.get(\"embed_dim\"))\n","\n","\n","class RecurrentAndTaskIDEmbedder(Embedder):\n","  \"\"\"Embedding used by IMPORT.\n","\n","  Compute both:\n","    - g(\\tau_{:t}) recurrently\n","    - f(e)\n","\n","  Full embedding is:\n","    \\phi(s_t, z), where z is randomly chosen from g(\\tau_{:t}) and f(e).\n","  \"\"\"\n","\n","  def __init__(\n","      self, recurrent_state_embedder, id_embedder, state_embedder, embed_dim):\n","    super().__init__(embed_dim)\n","    assert id_embedder.embed_dim == recurrent_state_embedder.embed_dim\n","    self._recurrent_state_embedder = recurrent_state_embedder\n","    self._id_embedder = id_embedder\n","    self._state_embedder = state_embedder\n","    self._final_layer = nn.Linear(\n","        id_embedder.embed_dim + state_embedder.embed_dim, embed_dim)\n","    self._use_id = False\n","\n","  def use_ids(self, use):\n","    self._use_id = use\n","\n","  def _compute_embeddings(self, states, hidden_state=None):\n","    # (batch_size, seq_len, embed_dim)\n","    recurrent_embedding, hidden_state = self._recurrent_state_embedder(\n","        states, hidden_state)\n","    # (batch_size, embed_dim)\n","    id_embedding = self._id_embedder(\n","        torch.tensor([seq[0].env_id for seq in states]))\n","\n","    if len(recurrent_embedding.shape) > 2:\n","      id_embedding = id_embedding.unsqueeze(1).expand_as(recurrent_embedding)\n","    return recurrent_embedding, id_embedding, hidden_state\n","\n","  def forward(self, states, hidden_state=None):\n","    recurrent_embedding, id_embedding, hidden_state = self._compute_embeddings(\n","        states, hidden_state)\n","\n","    history_embed = recurrent_embedding\n","    if self._use_id:\n","      history_embed = id_embedding\n","\n","    # (batch_size, seq_len, state_embed_dim) or (batch_size, state_embed_dim)\n","    state_embeds = self._state_embedder(\n","        [state for seq in states for state in seq])\n","    if len(history_embed.shape) > 2:\n","      state_embeds = state_embeds.reshape(\n","          history_embed.shape[0], history_embed.shape[1], -1)\n","    return self._final_layer(\n","        F.relu(torch.cat((history_embed, state_embeds), -1))), hidden_state\n","\n","  def aux_loss(self, trajectories):\n","    # (batch_size, max_seq_len)\n","    trajectories, mask = utils.pad(trajectories)\n","\n","    # (batch_size, max_seq_len, embed_dim)\n","    recurrent_embeddings, id_embeddings, hidden_state = self._compute_embeddings(\n","        [[exp.state for exp in traj] for traj in trajectories],\n","        [traj[0].agent_state for traj in trajectories])\n","\n","    return {\n","      \"embedding_distance\": (\n","          ((recurrent_embeddings - id_embeddings.detach()) ** 2)\n","          .mean(0).sum())\n","    }\n","\n","  @classmethod\n","  def from_config(cls, config, env):\n","    recurrent_state_embedder = RecurrentStateEmbedder.from_config(\n","        config.get(\"recurrent_embedder\"), env)\n","    state_embed_config = config.get(\"state_embedder\")\n","    state_embedder = get_state_embedder(env)(\n","      env.observation_space[\"observation\"],\n","      state_embed_config.get(\"embed_dim\"))\n","    instruction_embedder = SimpleGridStateEmbedder(\n","      env.observation_space[\"instructions\"],\n","      state_embed_config.get(\"embed_dim\"))\n","    state_embedder = StateInstructionEmbedder(\n","        state_embedder, instruction_embedder,\n","        state_embed_config.get(\"embed_dim\"))\n","\n","    id_embed_config = config.get(\"id_embedder\")\n","    id_embedder = IDEmbedder(\n","        env.observation_space[\"env_id\"].high,\n","        id_embed_config.get(\"embed_dim\"))\n","    return cls(\n","        recurrent_state_embedder, id_embedder, state_embedder,\n","        config.get(\"embed_dim\"))\n","\n","\n","class VariBADEmbedder(Embedder):\n","  \"\"\"Embedding used by VariBAD.\n","\n","  Computes:\n","    - g(\\tau_{:t}) recurrently and applies fully connected heads on top to\n","      produce q(z_t | \\tau_{:t}) = N(head1(g(\\tau_{:t})), head2(g(\\tau_{:t})))\n","    - embedding = \\phi(z_t.detach(), embed(s_t))\n","\n","  Decoding auxiliary loss:\n","    - \\sum_t \\sum_i ||decoder(z_i, e(s_t), e(a_t)) - r_t||_2^2\n","    - \\sum_t \\sum_i ||decoder(z_i, e(s_t), e(a_t)) - s_{t + 1}||_2^2\n","  \"\"\"\n","\n","  def __init__(\n","      self, recurrent_state_embedder, z_dim, state_embedder, action_embedder,\n","      state_dim, embed_dim, predict_state=True):\n","    super().__init__(embed_dim)\n","    self._recurrent_state_embedder = recurrent_state_embedder\n","    self._fc_mu = nn.Linear(recurrent_state_embedder.embed_dim, z_dim)\n","    self._fc_logvar = nn.Linear(recurrent_state_embedder.embed_dim, z_dim)\n","    self._state_embedder = state_embedder\n","    self._phi = nn.Linear(\n","        z_dim + state_embedder.embed_dim, embed_dim)\n","    self._action_embedder = action_embedder\n","    self._decoder = nn.Sequential(\n","      nn.Linear(z_dim + state_embedder.embed_dim + action_embedder.embed_dim,\n","                128),\n","      nn.ReLU(),\n","      nn.Linear(128, 128),\n","    )\n","\n","    # Predicts reward / state\n","    self._reward_head = nn.Linear(128, 1)\n","    self._state_head = nn.Linear(128, state_dim)\n","\n","    # If False, does not do state prediction\n","    self._predict_state = predict_state\n","    self._z_dim = z_dim\n","\n","  def _compute_z_distr(self, states, hidden_state=None):\n","    embeddings, hidden_state = self._recurrent_state_embedder(\n","        states, hidden_state=hidden_state)\n","\n","    # (batch_size, sequence_length, embed_dim)\n","    mu = embeddings\n","    std = torch.ones_like(mu) * 1e-6\n","\n","    q = td.Independent(td.Normal(mu, std), 1)\n","    return q, hidden_state\n","\n","  def forward(self, states, hidden_state=None):\n","    q, hidden_state = self._compute_z_distr(states, hidden_state)\n","    # Don't backprop through encoder\n","    z = q.rsample()\n","\n","    # (batch_size, seq_len, state_embed_dim) or (batch_size, state_embed_dim)\n","    state_embeds = self._state_embedder(\n","        [state for seq in states for state in seq])\n","    if len(z.shape) > 2:\n","      state_embeds = state_embeds.reshape(z.shape[0], z.shape[1], -1)\n","    return self._phi(F.relu(torch.cat((z, state_embeds), -1))), hidden_state\n","\n","  def aux_loss(self, trajectories):\n","    # The trajectories that we will try to decode\n","    # (batch_size, max_trajectory_len)\n","    trajectories_to_predict, predict_mask = utils.pad(\n","        [traj[0].trajectory for traj in trajectories])\n","\n","    # The trajectories we're using to encode z\n","    # They differ when we sample not the full trajectory\n","    # (batch_size, max_sequence_len)\n","    padded_trajectories, mask = utils.pad(trajectories)\n","\n","    q = self._compute_z_distr(\n","        [[exp.state for exp in traj] for traj in padded_trajectories],\n","        [traj[0].agent_state for traj in padded_trajectories])[0]\n","    # (batch_size, max_sequence_len, z_dim)\n","    z = q.rsample()\n","    # (batch_size, max_trajectory_len, max_sequence_len, z_dim)\n","    z = z.unsqueeze(1).expand(-1, predict_mask.shape[1], -1, -1)\n","\n","    # (batch_size, max_trajectory_len, embed_dim)\n","    # e(s)\n","    state_embeds = self._state_embedder(\n","        [exp.state for trajectory in trajectories_to_predict\n","         for exp in trajectory]).reshape(z.shape[0], z.shape[1], -1)\n","    # e(a)\n","    action_embeds = self._action_embedder(\n","        [exp.action for trajectory in trajectories_to_predict\n","         for exp in trajectory]).reshape(z.shape[0], z.shape[1], -1)\n","\n","    # (batch_size, max_trajectory_len, max_sequence_len, embed_dim)\n","    state_embeds = state_embeds.unsqueeze(2).expand(-1, -1, z.shape[2], -1)\n","    action_embeds = action_embeds.unsqueeze(2).expand(-1, -1, z.shape[2], -1)\n","\n","    decoder_input = torch.cat((z, state_embeds, action_embeds), -1)\n","    decoder_embed = self._decoder(decoder_input)\n","\n","    # (batch_size, max_trajectory_len, max_sequence_len, 1)\n","    predicted_rewards = self._reward_head(F.relu(decoder_embed))\n","\n","    # (batch_size, max_trajectory_len)\n","    true_rewards = torch.tensor(\n","        [[exp.next_state.prev_reward for exp in trajectory]\n","         for trajectory in trajectories_to_predict])\n","\n","    # (batch_size, max_trajectory_len, max_sequence_len, 1)\n","    true_rewards = true_rewards.unsqueeze(-1).unsqueeze(-1).expand_as(\n","        predicted_rewards)\n","\n","    # (batch_size, max_trajectory_len, max_sequence_len, 1)\n","    reward_decoding_loss = ((predicted_rewards - true_rewards) ** 2)\n","\n","    predict_mask = predict_mask.unsqueeze(2).expand(-1, -1, mask.shape[-1])\n","    mask = mask.unsqueeze(1).expand_as(predict_mask)\n","    # (batch_size, max_trajectory_len, max_sequence_len, 1)\n","    aggregate_mask = (predict_mask * mask).unsqueeze(-1)\n","    reward_decoding_loss = ((reward_decoding_loss * aggregate_mask).sum() /\n","                            reward_decoding_loss.shape[0])\n","\n","    state_decoding_loss = torch.tensor(0).float()\n","    if self._predict_state:\n","      # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n","      predicted_states = self._state_head(F.relu(decoder_embed))\n","\n","      # (batch_size, max_trajectory_len, state_dim)\n","      next_states_to_predict = torch.stack(\n","          [torch.stack([exp.next_state.observation for exp in trajectory])\n","           for trajectory in trajectories_to_predict])\n","\n","      # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n","      next_states_to_predict = next_states_to_predict.unsqueeze(2).expand_as(\n","          predicted_states)\n","\n","      # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n","      state_decoding_loss = ((predicted_states - next_states_to_predict) ** 2)\n","      state_decoding_loss = ((state_decoding_loss * aggregate_mask).sum() /\n","                             state_decoding_loss.shape[0])\n","\n","    #kl_loss = td.kl_divergence(q, self._prior(mask.shape[0], mask.shape[1]))\n","    return {\n","      \"reward_decoding_loss\": reward_decoding_loss,\n","      \"state_decoding_loss\": state_decoding_loss * 0.01,\n","      #\"kl_loss\": kl_loss * 0.1,\n","    }\n","\n","  def _prior(self, batch_size, sequence_len):\n","    mu = torch.zeros(batch_size, sequence_len, self._z_dim)\n","    std = torch.ones_like(mu)\n","    return td.Independent(td.Normal(mu, std), 1)\n","\n","  @classmethod\n","  def from_config(cls, config, env):\n","    recurrent_state_embedder = RecurrentStateEmbedder.from_config(\n","        config.get(\"recurrent_embedder\"), env)\n","    state_embed_config = config.get(\"state_embedder\")\n","    state_embedder = get_state_embedder(env)(\n","      env.observation_space[\"observation\"],\n","      state_embed_config.get(\"embed_dim\"))\n","    instruction_embedder = SimpleGridStateEmbedder(\n","      env.observation_space[\"instructions\"],\n","      state_embed_config.get(\"embed_dim\"))\n","    state_embedder = StateInstructionEmbedder(\n","        state_embedder, instruction_embedder,\n","        state_embed_config.get(\"embed_dim\"))\n","\n","    action_embed_config = config.get(\"action_embedder\")\n","    action_embedder = FixedVocabEmbedder(\n","        env.action_space.n, action_embed_config.get(\"embed_dim\"))\n","    state_dim = len(env.observation_space[\"observation\"].high)\n","    return cls(\n","        recurrent_state_embedder, config.get(\"z_dim\"), state_embedder,\n","        action_embedder, state_dim, config.get(\"embed_dim\"),\n","        config.get(\"predict_states\"))\n","\n","\n","class RecurrentStateEmbedder(Embedder):\n","  \"\"\"Applies an LSTM on top of a state embedding.\"\"\"\n","\n","  def __init__(self, state_embedder, embed_dim):\n","    super().__init__(embed_dim)\n","\n","    self._state_embedder = state_embedder\n","    self._lstm_cell = nn.LSTMCell(state_embedder.embed_dim, embed_dim)\n","\n","  def forward(self, states, hidden_state=None):\n","    \"\"\"Embeds a batch of sequences of contiguous states.\n","\n","    Args:\n","      states (list[list[np.array]]): of shape\n","        (batch_size, sequence_length, state_dim).\n","      hidden_state (list[object] | None): batch of initial hidden states\n","        to use with the LSTM. During inference, this should just be the\n","        previously returned hidden state.\n","\n","    Returns:\n","      embedding (torch.tensor): shape (batch_size, sequence_length, embed_dim)\n","      hidden_state (object): hidden state after embedding every element in the\n","        sequence.\n","    \"\"\"\n","    batch_size = len(states)\n","    sequence_len = len(states[0])\n","\n","    # Stack batched hidden state\n","    if batch_size > 1 and hidden_state is not None:\n","      hs = []\n","      cs = []\n","      for hidden in hidden_state:\n","        if hidden is None:\n","          hs.append(torch.zeros(1, self.embed_dim))\n","          cs.append(torch.zeros(1, self.embed_dim))\n","        else:\n","          hs.append(hidden[0])\n","          cs.append(hidden[1])\n","      hidden_state = (torch.cat(hs, 0), torch.cat(cs, 0))\n","\n","    flattened = [state for seq in states for state in seq]\n","\n","    # (batch_size * sequence_len, embed_dim)\n","    state_embeds = self._state_embedder(flattened)\n","    state_embeds = state_embeds.reshape(batch_size, sequence_len, -1)\n","\n","    embeddings = []\n","    for seq_index in range(sequence_len):\n","      hidden_state = self._lstm_cell(\n","          state_embeds[:, seq_index, :], hidden_state)\n","\n","      # (batch_size, 1, embed_dim)\n","      embeddings.append(hidden_state[0].unsqueeze(1))\n","\n","    # (batch_size, sequence_len, embed_dim)\n","    # squeezed to (batch_size, embed_dim) if sequence_len == 1\n","    embeddings = torch.cat(embeddings, 1).squeeze(1)\n","\n","    # Detach to save GPU memory.\n","    detached_hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n","    return embeddings, detached_hidden_state\n","\n","  @classmethod\n","  def from_config(cls, config, env):\n","    experience_embed_config = config.get(\"experience_embedder\")\n","    state_embedder = get_state_embedder(env)(\n","        env.observation_space[\"observation\"],\n","        experience_embed_config.get(\"state_embed_dim\"))\n","    action_embedder = FixedVocabEmbedder(\n","        env.action_space.n + 1, experience_embed_config.get(\"action_embed_dim\"))\n","    instruction_embedder = None\n","    if experience_embed_config.get(\"instruction_embed_dim\") is not None:\n","      # Use SimpleGridEmbedder since these are just discrete vars\n","      instruction_embedder = SimpleGridStateEmbedder(\n","          env.observation_space[\"instructions\"],\n","          experience_embed_config.get(\"instruction_embed_dim\"))\n","\n","    reward_embedder = None\n","    if experience_embed_config.get(\"reward_embed_dim\") is not None:\n","      reward_embedder = LinearEmbedder(\n","          1, experience_embed_config.get(\"reward_embed_dim\"))\n","\n","    done_embedder = None\n","    if experience_embed_config.get(\"done_embed_dim\") is not None:\n","      done_embedder = FixedVocabEmbedder(\n","          2, experience_embed_config.get(\"done_embed_dim\"))\n","\n","    experience_embedder = ExperienceEmbedder(\n","        state_embedder, instruction_embedder, action_embedder,\n","        reward_embedder, done_embedder,\n","        experience_embed_config.get(\"embed_dim\"))\n","    return cls(experience_embedder, config.get(\"embed_dim\"))\n","\n","\n","class StateInstructionEmbedder(Embedder):\n","  \"\"\"Embeds instructions and states and applies a linear layer on top.\"\"\"\n","\n","  def __init__(self, state_embedder, instruction_embedder, embed_dim):\n","    super().__init__(embed_dim)\n","    self._state_embedder = state_embedder\n","    self._instruction_embedder = instruction_embedder\n","    if instruction_embedder is not None:\n","      self._final_layer = nn.Linear(\n","          state_embedder.embed_dim + instruction_embedder.embed_dim, embed_dim)\n","      assert self._state_embedder.embed_dim == embed_dim\n","\n","  def forward(self, states):\n","    state_embeds = self._state_embedder([state.observation for state in states])\n","    if self._instruction_embedder is not None:\n","      instruction_embeds = self._instruction_embedder(\n","          [torch.tensor(state.instructions) for state in states])\n","      return self._final_layer(\n","          F.relu(torch.cat((state_embeds, instruction_embeds), -1)))\n","    return state_embeds\n","\n","\n","def init(module, weight_init, bias_init, gain=1):\n","    weight_init(module.weight.data, gain=gain)\n","    bias_init(module.bias.data)\n","    return module\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class MiniWorldEmbedder(Embedder):\n","  \"\"\"Embeds 80x60 MiniWorld inputs.\n","\n","  Network taken from gym-miniworld/.\n","  \"\"\"\n","  def __init__(self, observation_space, embed_dim):\n","    super().__init__(embed_dim)\n","\n","    # Architecture from gym-miniworld\n","    # For 80x60 input\n","    num_inputs = observation_space.shape[0]\n","\n","    self._network = nn.Sequential(\n","        nn.Conv2d(num_inputs, 32, kernel_size=5, stride=2),\n","        nn.ReLU(),\n","\n","        nn.Conv2d(32, 32, kernel_size=5, stride=2),\n","        nn.ReLU(),\n","\n","        nn.Conv2d(32, 32, kernel_size=4, stride=2),\n","        nn.ReLU(),\n","\n","        Flatten(),\n","\n","        nn.Linear(32 * 7 * 5, embed_dim),\n","    )\n","\n","  def forward(self, obs):\n","    # (batch_size, 80, 60, 3)\n","    tensor = torch.stack(obs) / 255.\n","    return self._network(tensor)\n","\n","\n","class SimpleGridStateEmbedder(Embedder):\n","  \"\"\"Embedder for SimpleGridEnv states.\n","\n","  Concretely, embeds (x, y) separately with different embeddings for each cell.\n","  \"\"\"\n","\n","  def __init__(self, observation_space, embed_dim):\n","    \"\"\"Constructs for SimpleGridEnv.\n","\n","    Args:\n","      observation_space (spaces.Box): limits for the observations to embed.\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    assert all(dim == 0 for dim in observation_space.low)\n","    assert observation_space.dtype == np.int\n","\n","    hidden_size = 32\n","    self._embedders = nn.ModuleList(\n","        [nn.Embedding(dim, hidden_size) for dim in observation_space.high])\n","    self._fc_layer = nn.Linear(hidden_size * len(observation_space.high), 256)\n","    self._final_fc_layer = nn.Linear(256, embed_dim)\n","\n","  def forward(self, obs):\n","    tensor = torch.stack(obs)\n","    embeds = []\n","    for i in range(tensor.shape[1]):\n","      embeds.append(self._embedders[i](tensor[:, i]))\n","    return self._final_fc_layer(F.relu(self._fc_layer(torch.cat(embeds, -1))))\n","\n","\n","class IDEmbedder(Embedder):\n","  \"\"\"Embeds N-dim IDs by embedding each component and applying a linear\n","  layer.\"\"\"\n","\n","  def __init__(self, observation_space, embed_dim):\n","    \"\"\"Constructs for SimpleGridEnv.\n","\n","    Args:\n","      observation_space (np.array): discrete max limits for each dimension of the\n","        state (expects min is 0).\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    hidden_size = 32\n","    self._embedders = nn.ModuleList(\n","        [nn.Embedding(dim, hidden_size) for dim in observation_space])\n","    self._fc_layer = nn.Linear(hidden_size * len(observation_space), embed_dim)\n","\n","  @classmethod\n","  def from_config(cls, config, observation_space):\n","    return cls(observation_space, config.get(\"embed_dim\"))\n","\n","  def forward(self, obs):\n","    tensor = obs\n","    if len(tensor.shape) == 1:  # 1-d IDs\n","      tensor = tensor.unsqueeze(-1)\n","\n","    embeds = []\n","    for i in range(tensor.shape[1]):\n","      embeds.append(self._embedders[i](tensor[:, i]))\n","    return self._fc_layer(torch.cat(embeds, -1))\n","\n","\n","class FixedVocabEmbedder(Embedder):\n","  \"\"\"Wrapper around nn.Embedding obeying the Embedder interface.\"\"\"\n","\n","  def __init__(self, vocab_size, embed_dim):\n","    \"\"\"Constructs.\n","\n","    Args:\n","      vocab_size (int): number of unique embeddings.\n","      embed_dim (int): dimension of output embedding.\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    self._embedder = nn.Embedding(vocab_size, embed_dim)\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    return cls(config.get(\"vocab_size\"), config.get(\"embed_dim\"))\n","\n","  def forward(self, inputs):\n","    \"\"\"Embeds inputs according to the underlying nn.Embedding.\n","\n","    Args:\n","      inputs (list[int]): list of inputs of length batch.\n","\n","    Returns:\n","      embedding (torch.Tensor): of shape (batch, embed_dim)\n","    \"\"\"\n","    tensor_inputs = torch.tensor(np.stack(inputs)).long()\n","    return self._embedder(tensor_inputs)\n","\n","\n","class LinearEmbedder(Embedder):\n","  \"\"\"Wrapper around nn.Linear obeying the Embedder interface.\"\"\"\n","\n","  def __init__(self, input_dim, embed_dim):\n","    \"\"\"Wraps a nn.Linear(input_dim, embed_dim).\n","\n","    Args:\n","      input_dim (int): dimension of inputs to embed.\n","      embed_dim (int): dimension of output embedding.\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    self._embedder = nn.Linear(input_dim, embed_dim)\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    return cls(config.get(\"input_dim\"), config.get(\"embed_dim\"))\n","\n","  def forward(self, inputs):\n","    \"\"\"Embeds inputs according to the underlying nn.Linear.\n","\n","    Args:\n","      inputs (list[np.array]): list of inputs of length batch.\n","        Each input is an array of shape (input_dim).\n","\n","    Returns:\n","      embedding (torch.Tensor): of shape (batch, embed_dim)\n","    \"\"\"\n","    inputs = np.stack(inputs)\n","    if len(inputs.shape) == 1:\n","      inputs = np.expand_dims(inputs, 1)\n","    tensor_inputs = torch.tensor(inputs).float()\n","    return self._embedder(tensor_inputs)\n","\n","\n","class ExperienceEmbedder(Embedder):\n","  \"\"\"Optionally embeds each of:\n","\n","    - state s\n","    - instructions i\n","    - actions a\n","    - rewards r\n","    - done d\n","\n","  Then passes a single linear layer over their concatenation.\n","  \"\"\"\n","\n","  def __init__(self, state_embedder, instruction_embedder, action_embedder,\n","               reward_embedder, done_embedder, embed_dim):\n","    \"\"\"Constructs.\n","\n","    Args:\n","      state_embedder (Embedder | None)\n","      instruction_embedder (Embedder | None)\n","      action_embedder (Embedder | None)\n","      reward_embedder (Embedder | None)\n","      done_embedder (Embedder | None)\n","      embed_dim (int): dimension of the output\n","    \"\"\"\n","    super().__init__(embed_dim)\n","\n","    self._embedders = collections.OrderedDict()\n","    if state_embedder is not None:\n","      self._embedders[\"state\"] = state_embedder\n","    if instruction_embedder is not None:\n","      self._embedders[\"instruction\"] = instruction_embedder\n","    if action_embedder is not None:\n","      self._embedders[\"action\"] = action_embedder\n","    if reward_embedder is not None:\n","      self._embedders[\"reward\"] = reward_embedder\n","    if done_embedder is not None:\n","      self._embedders[\"done\"] = done_embedder\n","\n","    # Register the embedders so they get gradients\n","    self._register_embedders = nn.ModuleList(self._embedders.values())\n","    self._final_layer = nn.Linear(\n","        sum(embedder.embed_dim for embedder in self._embedders.values()),\n","        embed_dim)\n","\n","  def forward(self, instruction_states):\n","    \"\"\"Embeds the components for which this has embedders.\n","\n","    Args:\n","      instruction_states (list[InstructionState]): batch of states.\n","\n","    Returns:\n","      embedding (torch.Tensor): of shape (batch, embed_dim)\n","    \"\"\"\n","    def get_inputs(key, states):\n","      if key == \"state\":\n","        return [state.observation for state in states]\n","      elif key == \"instruction\":\n","        return [torch.tensor(state.instructions) for state in states]\n","      elif key == \"action\":\n","        actions = np.array(\n","            [state.prev_action if state.prev_action is not None else -1\n","             for state in states])\n","        return actions + 1\n","      elif key == \"reward\":\n","        return [state.prev_reward for state in states]\n","      elif key == \"done\":\n","        return [state.done for state in states]\n","      else:\n","        raise ValueError(\"Unsupported key: {}\".format(key))\n","\n","    embeddings = []\n","    for key, embedder in self._embedders.items():\n","      inputs = get_inputs(key, instruction_states)\n","      embeddings.append(embedder(inputs))\n","    return self._final_layer(F.relu(torch.cat(embeddings, -1)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0l94hP6Gi1m"},"source":["# dqn.py\n","import collections\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.nn import utils as torch_utils\n","import schedule\n","import replay\n","import utils\n","\n","\n","class DQNAgent(object):\n","  @classmethod\n","  def from_config(cls, config, env):\n","    dqn = DQNPolicy.from_config(config.get(\"policy\"), env)\n","    replay_buffer = replay.ReplayBuffer.from_config(config.get(\"buffer\"))\n","    optimizer = optim.Adam(dqn.parameters(), lr=config.get(\"learning_rate\"))\n","    return cls(dqn, replay_buffer, optimizer, config.get(\"sync_target_freq\"),\n","               config.get(\"min_buffer_size\"), config.get(\"batch_size\"),\n","               config.get(\"update_freq\"), config.get(\"max_grad_norm\"))\n","\n","  def __init__(self, dqn, replay_buffer, optimizer, sync_freq,\n","               min_buffer_size, batch_size, update_freq, max_grad_norm):\n","    \"\"\"\n","    Args:\n","      dqn (DQNPolicy)\n","      replay_buffer (ReplayBuffer)\n","      optimizer (torch.Optimizer)\n","      sync_freq (int): number of updates between syncing the\n","        DQN target Q network\n","      min_buffer_size (int): replay buffer must be at least this large\n","        before taking grad updates\n","      batch_size (int): number of experience to sample per grad step\n","      update_freq (int): number of update calls per parameter update.\n","      max_grad_norm (float): gradient is clipped to this norm on each\n","        update\n","    \"\"\"\n","    self._dqn = dqn\n","    self._replay_buffer = replay_buffer\n","    self._optimizer = optimizer\n","    self._sync_freq = sync_freq\n","    self._min_buffer_size = min_buffer_size\n","    self._batch_size = batch_size\n","    self._update_freq = update_freq\n","    self._max_grad_norm = max_grad_norm\n","    self._updates = 0\n","\n","    self._losses = collections.deque(maxlen=100)\n","    self._grad_norms = collections.deque(maxlen=100)\n","\n","  def update(self, experience):\n","    \"\"\"Updates agent on this experience.\n","\n","    Args:\n","      experience (Experience): experience to update on.\n","    \"\"\"\n","    self._replay_buffer.add(experience)\n","\n","    if len(self._replay_buffer) >= self._min_buffer_size:\n","      if self._updates % self._update_freq == 0:\n","        experiences = self._replay_buffer.sample(self._batch_size)\n","\n","        self._optimizer.zero_grad()\n","        loss = self._dqn.loss(experiences, np.ones(self._batch_size))\n","        loss.backward()\n","        self._losses.append(loss.item())\n","\n","        # clip according to the max allowed grad norm\n","        grad_norm = torch_utils.clip_grad_norm_(\n","            self._dqn.parameters(), self._max_grad_norm, norm_type=2)\n","        self._grad_norms.append(grad_norm)\n","        self._optimizer.step()\n","\n","      if self._updates % self._sync_freq == 0:\n","        self._dqn.sync_target()\n","\n","    self._updates += 1\n","\n","  def act(self, state, prev_hidden_state=None, test=False):\n","    \"\"\"Given the current state, returns an action.\n","\n","    Args:\n","      state (State)\n","\n","    Returns:\n","      action (int)\n","      hidden_state (object)\n","    \"\"\"\n","    return self._dqn.act(state, prev_hidden_state=prev_hidden_state, test=test)\n","\n","  @property\n","  def stats(self):\n","    def mean_with_default(l, default):\n","      if len(l) == 0:\n","        return default\n","      return np.mean(l)\n","\n","    stats = self._dqn.stats\n","    stats[\"loss\"] = mean_with_default(self._losses, None)\n","    stats[\"grad_norm\"] = mean_with_default(self._grad_norms, None)\n","    return {\"DQN/{}\".format(k): v for k, v in stats.items()}\n","\n","  def state_dict(self):\n","    \"\"\"Returns a serializable dictionary containing all the relevant\n","    details from the class.\n","\n","    Returns:\n","      state_dict (dict)\n","    \"\"\"\n","    # Currently doesn't serialize replay buffer to save memory\n","    return {\n","        \"dqn\": self._dqn.state_dict(),\n","        #\"replay_buffer\": self._replay_buffer,\n","        \"optimizer\": self._optimizer.state_dict(),\n","        \"sync_freq\": self._sync_freq,\n","        \"min_buffer_size\": self._min_buffer_size,\n","        \"batch_size\": self._batch_size,\n","        \"update_freq\": self._update_freq,\n","        \"max_grad_norm\": self._max_grad_norm,\n","        \"updates\": self._updates,\n","    }\n","\n","  def load_state_dict(self, state_dict):\n","    self._dqn.load_state_dict(state_dict[\"dqn\"])\n","    #self._replay_buffer = state_dict[\"replay_buffer\"]\n","    self._optimizer.load_state_dict(state_dict[\"optimizer\"])\n","    self._sync_freq = state_dict[\"sync_freq\"]\n","    self._min_buffer_size = state_dict[\"min_buffer_size\"]\n","    self._batch_size = state_dict[\"batch_size\"]\n","    self._update_freq = state_dict[\"update_freq\"]\n","    self._max_grad_norm = state_dict[\"max_grad_norm\"]\n","    self._updates = state_dict[\"updates\"]\n","\n","  def set_reward_relabeler(self, reward_relabeler):\n","    \"\"\"See DQNPolicy.reward_relabeler.\"\"\"\n","    self._dqn.set_reward_relabeler(reward_relabeler)\n","\n","\n","class DQNPolicy(nn.Module):\n","  @classmethod\n","  def from_config(cls, config, env):\n","    def embedder_factory():\n","      embedder_config = config.get(\"embedder\")\n","      embed_type = embedder_config.get(\"type\")\n","      if embed_type == \"instruction\":\n","        return InstructionPolicyEmbedder.from_config(embedder_config, env)\n","      elif embed_type == \"recurrent\":\n","        return RecurrentStateEmbedder.from_config(embedder_config, env)\n","      elif embedder_config.get(\"type\") == \"varibad\":\n","        return VariBADEmbedder.from_config(embedder_config, env)\n","      elif embedder_config.get(\"type\") == \"import\":\n","        return RecurrentAndTaskIDEmbedder.from_config(\n","            embedder_config, env)\n","      else:\n","        raise ValueError(\"Unsupported embedding type: {}\".format(embed_type))\n","\n","    policy_type = config.get(\"type\")\n","    if policy_type == \"vanilla\":\n","      pass\n","    elif policy_type == \"recurrent\":\n","      cls = RecurrentDQNPolicy\n","    else:\n","      raise ValueError(\"Unsupported policy type: {}\".format(policy_type))\n","\n","    epsilon_schedule = schedule.LinearSchedule.from_config(\n","        config.get(\"epsilon_schedule\"))\n","    return cls(env.action_space.n, epsilon_schedule, config.get(\"test_epsilon\"),\n","               embedder_factory, config.get(\"discount\"))\n","\n","  def __init__(self, num_actions, epsilon_schedule, test_epsilon,\n","               state_embedder_factory, gamma=0.99):\n","    \"\"\"DQNPolicy should typically be constructed via from_config, and not\n","    through the constructor.\n","\n","    Args:\n","      num_actions (int): the number of possible actions to take at each\n","        state\n","      epsilon_schedule (Schedule): defines rate at which epsilon decays\n","      test_epsilon (float): epsilon to use during test time (when test is\n","        True in act)\n","      state_embedder_factory (Callable --> StateEmbedder): type of state\n","        embedder to use\n","      gamma (float): discount factor\n","    \"\"\"\n","    super().__init__()\n","    self._Q = DuelingNetwork(num_actions, state_embedder_factory())\n","    self._target_Q = DuelingNetwork(num_actions, state_embedder_factory())\n","    self._num_actions = num_actions\n","    self._epsilon_schedule = epsilon_schedule\n","    self._test_epsilon = test_epsilon\n","    self._gamma = gamma\n","    self._reward_relabeler = None\n","\n","    # Used for generating statistics about the policy\n","    # Average of max Q values\n","    self._max_q = collections.deque(maxlen=1000)\n","    self._min_q = collections.deque(maxlen=1000)\n","    self._losses = collections.defaultdict(lambda: collections.deque(maxlen=1000))\n","\n","  def act(self, state, prev_hidden_state=None, test=False):\n","    \"\"\"\n","    Args:\n","      state (State)\n","      test (bool): if True, takes on the test epsilon value\n","      prev_hidden_state (object | None): unused agent state.\n","      epsilon (float | None): if not None, overrides the epsilon greedy\n","      schedule with this epsilon value. Mutually exclusive with test\n","      flag\n","\n","    Returns:\n","      int: action\n","      hidden_state (None)\n","    \"\"\"\n","    del prev_hidden_state\n","\n","    q_values, hidden_state = self._Q([state], None)\n","    if test:\n","      epsilon = self._test_epsilon\n","    else:\n","      epsilon = self._epsilon_schedule.step()\n","    self._max_q.append(torch.max(q_values).item())\n","    self._min_q.append(torch.min(q_values).item())\n","    return epsilon_greedy(q_values, epsilon)[0], None\n","\n","  def loss(self, experiences, weights):\n","    \"\"\"Updates parameters from a batch of experiences\n","\n","    Minimizing the loss:\n","\n","      (target - Q(s, a))^2\n","\n","      target = r if done\n","           r + \\gamma * max_a' Q(s', a')\n","\n","    Args:\n","      experiences (list[Experience]): batch of experiences, state and\n","        next_state may be LazyFrames or np.arrays\n","      weights (list[float]): importance weights on each experience\n","\n","    Returns:\n","      loss (torch.tensor): MSE loss on the experiences.\n","    \"\"\"\n","    batch_size = len(experiences)\n","    states = [e.state for e in experiences]\n","    actions = torch.tensor([e.action for e in experiences]).long()\n","    next_states = [e.next_state for e in experiences]\n","    rewards = torch.tensor([e.reward for e in experiences]).float()\n","\n","    # (batch_size,) 1 if was not done, otherwise 0\n","    not_done_mask = torch.tensor([1 - e.done for e in experiences]).byte()\n","    weights = torch.tensor(weights).float()\n","\n","    current_state_q_values, aux_losses = self._Q(states, None)\n","    if isinstance(aux_losses, dict):\n","      for name, loss in aux_losses.items():\n","        self._losses[name].append(loss.detach().cpu().data.numpy())\n","    current_state_q_values = current_state_q_values.gather(\n","        1, actions.unsqueeze(1))\n","\n","    # DDQN\n","    best_actions = torch.max(self._Q(next_states, None)[0], 1)[1].unsqueeze(1)\n","    next_state_q_values = self._target_Q(next_states, None)[0].gather(\n","        1, best_actions).squeeze(1)\n","    targets = rewards + self._gamma * (\n","      next_state_q_values * not_done_mask.float())\n","    targets.detach_()  # Don't backprop through targets\n","\n","    td_error = current_state_q_values.squeeze() - targets\n","    loss = torch.mean((td_error ** 2) * weights)\n","    self._losses[\"td_error\"].append(loss.detach().cpu().data.numpy())\n","    aux_loss = 0\n","    if isinstance(aux_losses, dict):\n","      aux_loss = sum(aux_losses.values())\n","    return loss + aux_loss\n","\n","  def sync_target(self):\n","    \"\"\"Syncs the target Q values with the current Q values\"\"\"\n","    self._target_Q.load_state_dict(self._Q.state_dict())\n","\n","  def set_reward_relabeler(self, reward_relabeler):\n","    \"\"\"Sets the reward relabeler when computing the loss.\n","\n","    Args:\n","      reward_relabeler (RewardLabeler)\n","\n","    Raises:\n","      ValueError: when the reward relabeler has already been set.\n","    \"\"\"\n","    if self._reward_relabeler is not None:\n","      raise ValueError(\"Reward relabeler already set.\")\n","    self._reward_relabeler = reward_relabeler\n","\n","  @property\n","  def stats(self):\n","    \"\"\"See comments in constructor for more details about what these stats\n","    are\"\"\"\n","    def mean_with_default(l, default):\n","      if len(l) == 0:\n","        return default\n","      return np.mean(l)\n","\n","    stats = {\n","        \"epsilon\": self._epsilon_schedule.step(take_step=False),\n","        \"Max Q\": mean_with_default(self._max_q, None),\n","        \"Min Q\": mean_with_default(self._min_q, None),\n","    }\n","    for name, losses in self._losses.items():\n","      stats[name] = np.mean(losses)\n","    return stats\n","\n","\n","class RecurrentDQNPolicy(DQNPolicy):\n","  \"\"\"Implements a DQN policy that uses an RNN on the observations.\"\"\"\n","\n","  def loss(self, experiences, weights):\n","    \"\"\"Updates recurrent parameters from a batch of sequential experiences\n","\n","    Minimizing the DQN loss:\n","\n","      (target - Q(s, a))^2\n","\n","      target = r if done\n","           r + \\gamma * max_a' Q(s', a')\n","\n","    Args:\n","      experiences (list[list[Experience]]): batch of sequences of experiences.\n","      weights (list[float]): importance weights on each experience\n","\n","    Returns:\n","      loss (torch.tensor): MSE loss on the experiences.\n","    \"\"\"\n","    unpadded_experiences = experiences\n","    experiences, mask = utils.pad(experiences)\n","    batch_size = len(experiences)\n","    seq_len = len(experiences[0])\n","\n","    hidden_states = [seq[0].agent_state for seq in experiences]\n","    # Include the next states in here to minimize calls to _Q\n","    states = [\n","        [e.state for e in seq] + [seq[-1].next_state] for seq in experiences]\n","    actions = torch.tensor(\n","        [e.action for seq in experiences for e in seq]).long()\n","    next_hidden_states = [seq[0].next_agent_state for seq in experiences]\n","    next_states = [[e.next_state for e in seq] for seq in experiences]\n","    rewards = torch.tensor(\n","        [e.reward for seq in experiences for e in seq]).float()\n","\n","    # Relabel the rewards on the fly\n","    if self._reward_relabeler is not None:\n","      trajectories = [seq[0].trajectory for seq in experiences]\n","      # (batch_size, max_seq_len)\n","      indices = torch.tensor(\n","          [[e.index for e in seq] for seq in experiences]).long()\n","\n","      # (batch_size * max_trajectory_len)\n","      rewards = self._reward_relabeler.label_rewards(\n","          trajectories)[0].gather(-1, indices).reshape(-1)\n","\n","    # (batch_size,) 1 if was not done, otherwise 0\n","    not_done_mask = ~(torch.tensor(\n","        [e.done for seq in experiences for e in seq]).bool())\n","    weights = torch.tensor(weights).float()\n","\n","    # (batch_size, seq_len + 1, actions)\n","    q_values, _ = self._Q(states, hidden_states)\n","    current_q_values = q_values[:, :-1, :]\n","    current_q_values = current_q_values.reshape(batch_size * seq_len, -1)\n","    # (batch_size * seq_len, 1)\n","    current_state_q_values = current_q_values.gather(1, actions.unsqueeze(1))\n","\n","    aux_losses = {}\n","    if hasattr(self._Q._state_embedder, \"aux_loss\"):\n","      aux_losses = self._Q._state_embedder.aux_loss(unpadded_experiences)\n","      if isinstance(aux_losses, dict):\n","        for name, loss in aux_losses.items():\n","          self._losses[name].append(loss.detach().cpu().data.numpy())\n","\n","    # DDQN\n","    next_q_values = q_values[:, 1:, :]\n","    # (batch_size * seq_len, actions)\n","    next_q_values = next_q_values.reshape(batch_size * seq_len, -1)\n","    best_actions = torch.max(next_q_values, 1)[1].unsqueeze(1)\n","    # Using the same hidden states for target\n","    target_q_values, _ = self._target_Q(next_states, next_hidden_states)\n","    target_q_values = target_q_values.reshape(batch_size * seq_len, -1)\n","    next_state_q_values = target_q_values.gather(1, best_actions).squeeze(1)\n","    targets = rewards + self._gamma * (\n","        next_state_q_values * not_done_mask.float())\n","    targets.detach_()  # Don't backprop through targets\n","\n","    td_error = current_state_q_values.squeeze() - targets\n","    weights = weights.unsqueeze(1) * mask.float()\n","    loss = (td_error ** 2).reshape(batch_size, seq_len) * weights\n","    loss = loss.sum() / mask.sum()  # masked mean\n","    return loss + sum(aux_losses.values())\n","\n","  def act(self, state, prev_hidden_state=None, test=False):\n","    \"\"\"\n","    Args:\n","      state (State)\n","      test (bool): if True, takes on the test epsilon value\n","      prev_hidden_state (object | None): unused agent state.\n","      epsilon (float | None): if not None, overrides the epsilon greedy\n","      schedule with this epsilon value. Mutually exclusive with test\n","      flag\n","\n","    Returns:\n","      int: action\n","      hidden_state (None)\n","    \"\"\"\n","    q_values, hidden_state = self._Q([[state]], prev_hidden_state)\n","    if test:\n","      epsilon = self._test_epsilon\n","    else:\n","      epsilon = self._epsilon_schedule.step()\n","    self._max_q.append(torch.max(q_values).item())\n","    self._min_q.append(torch.min(q_values).item())\n","    return epsilon_greedy(q_values, epsilon)[0], hidden_state\n","\n","\n","class DQN(nn.Module):\n","  \"\"\"Implements the Q-function.\"\"\"\n","  def __init__(self, num_actions, state_embedder):\n","    \"\"\"\n","    Args:\n","      num_actions (int): the number of possible actions at each state\n","      state_embedder (StateEmbedder): the state embedder to use\n","    \"\"\"\n","    super(DQN, self).__init__()\n","    self._state_embedder = state_embedder\n","    self._q_values = nn.Linear(self._state_embedder.embed_dim, num_actions)\n","\n","  def forward(self, states, hidden_states=None):\n","    \"\"\"Returns Q-values for each of the states.\n","\n","    Args:\n","      states (FloatTensor): shape (batch_size, 84, 84, 4)\n","      hidden_states (object | None): hidden state returned by previous call to\n","        forward. Must be called on constiguous states.\n","\n","    Returns:\n","      FloatTensor: (batch_size, num_actions)\n","      hidden_state (object)\n","    \"\"\"\n","    state_embed, hidden_state = self._state_embedder(states, hidden_states)\n","    return self._q_values(state_embed), hidden_state\n","\n","\n","class DuelingNetwork(DQN):\n","  \"\"\"Implements the following Q-network:\n","\n","    Q(s, a) = V(s) + A(s, a) - avg_a' A(s, a')\n","  \"\"\"\n","  def __init__(self, num_actions, state_embedder):\n","    super(DuelingNetwork, self).__init__(num_actions, state_embedder)\n","    self._V = nn.Linear(self._state_embedder.embed_dim, 1)\n","    self._A = nn.Linear(self._state_embedder.embed_dim, num_actions)\n","\n","  def forward(self, states, hidden_states=None):\n","    state_embedding, hidden_state = self._state_embedder(states, hidden_states)\n","    V = self._V(state_embedding)\n","    advantage = self._A(state_embedding)\n","    mean_advantage = torch.mean(advantage)\n","    return V + advantage - mean_advantage, hidden_state\n","\n","\n","def epsilon_greedy(q_values, epsilon):\n","  \"\"\"Returns the index of the highest q value with prob 1 - epsilon,\n","  otherwise uniformly at random with prob epsilon.\n","\n","  Args:\n","    q_values (Variable[FloatTensor]): (batch_size, num_actions)\n","    epsilon (float)\n","\n","  Returns:\n","    list[int]: actions\n","  \"\"\"\n","  batch_size, num_actions = q_values.size()\n","  _, max_indices = torch.max(q_values, 1)\n","  max_indices = max_indices.cpu().data.numpy()\n","  actions = []\n","  for i in range(batch_size):\n","    if np.random.random() > epsilon:\n","      actions.append(max_indices[i])\n","    else:\n","      actions.append(np.random.randint(0, num_actions))\n","  return actions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Srk8W8KVH1kw"},"source":["import numpy as np\n","import torch\n","import tqdm\n","\n","import config as cfg\n","import grid\n","import city\n","import policy\n","import relabel\n","import rl\n","import utils\n","\n","\n","def run_episode(env, policy, experience_observers=None, test=False):\n","  \"\"\"Runs a single episode on the environment following the policy.\n","\n","  Args:\n","    env (gym.Environment): environment to run on.\n","    policy (Policy): policy to follow.\n","    experience_observers (list[Callable] | None): each observer is called with\n","      with each experience at each timestep.\n","\n","  Returns:\n","    episode (list[Experience]): experiences from the episode.\n","    renders (list[object | None]): renderings of the episode, only rendered if\n","      test=True. Otherwise, returns list of Nones.\n","  \"\"\"\n","  # Optimization: rendering takes a lot of time.\n","  def maybe_render(env, action, reward, timestep):\n","    if test:\n","      render = env.render()\n","      render.write_text(\"Action: {}\".format(str(action)))\n","      render.write_text(\"Reward: {}\".format(reward))\n","      render.write_text(\"Timestep: {}\".format(timestep))\n","      return render\n","    return None\n","\n","  if experience_observers is None:\n","    experience_observers = []\n","\n","  episode = []\n","  state = env.reset()\n","  timestep = 0\n","  renders = [maybe_render(env, None, 0, timestep)]\n","  hidden_state = None\n","  while True:\n","    action, next_hidden_state = policy.act(\n","        state, hidden_state, test=test)\n","    next_state, reward, done, info = env.step(action)\n","    timestep += 1\n","    renders.append(\n","        maybe_render(env, grid.Action(action), reward, timestep))\n","    experience = rl.Experience(\n","        state, action, reward, next_state, done, info, hidden_state,\n","        next_hidden_state)\n","    episode.append(experience)\n","    for observer in experience_observers:\n","      observer(experience)\n","\n","    state = next_state\n","    hidden_state = next_hidden_state\n","    if done:\n","      return episode, renders\n","\n","\n","def get_env_class(environment_type):\n","  \"\"\"Returns the environment class specified by the type.\n","\n","  Args:\n","    environment_type (str): a valid environment type.\n","\n","  Returns:\n","    environment_class (type): type specified.\n","  \"\"\"\n","  if environment_type == \"vanilla\":\n","    return city.CityGridEnv\n","  elif environment_type == \"distraction\":\n","    return city.DistractionGridEnv\n","  elif environment_type == \"map\":\n","    return city.MapGridEnv\n","  elif environment_type == \"cooking\":\n","    return cooking.CookingGridEnv\n","  elif environment_type == \"miniworld_sign\":\n","    # Dependencies on OpenGL, so only load if absolutely necessary\n","    from envs.miniworld import sign\n","    return sign.MiniWorldSign\n","  else:\n","    raise ValueError(\n","        \"Unsupported environment type: {}\".format(environment_type))\n","\n","\n","def get_instruction_agent(instruction_config, instruction_env):\n","  if instruction_config.get(\"type\") == \"learned\":\n","    return DQNAgent.from_config(instruction_config, instruction_env)\n","  else:\n","    raise ValueError(\n","        \"Invalid instruction agent: {}\".format(instruction_config.get(\"type\")))\n","\n","\n","def get_exploration_agent(exploration_config, exploration_env):\n","  if exploration_config.get(\"type\") == \"learned\":\n","    return DQNAgent.from_config(exploration_config, exploration_env)\n","  elif exploration_config.get(\"type\") == \"random\":\n","    return policy.RandomPolicy(exploration_env.action_space)\n","  elif exploration_config.get(\"type\") == \"none\":\n","    return policy.ConstantActionPolicy(grid.Action.end_episode)\n","  else:\n","    raise ValueError(\"Invalid exploration agent: {}\".format(\n","      exploration_config.get(\"type\")))\n","\n","\n","def log_episode(exploration_episode, exploration_rewards, distances, path):\n","  with open(path, \"w+\") as f:\n","    f.write(\"Env ID: {}\\n\".format(exploration_episode[0].state.env_id))\n","    for t, (exp, exploration_reward, distance) in enumerate(\n","        zip(exploration_episode, exploration_rewards, distances)):\n","      f.write(\"=\" * 80 + \"\\n\")\n","      f.write(\"Timestep: {}\\n\".format(t))\n","      f.write(\"State: {}\\n\".format(exp.state.observation))\n","      f.write(\"Action: {}\\n\".format(grid.Action(exp.action).name))\n","      f.write(\"Reward: {}\\n\".format(exploration_reward))\n","      f.write(\"Distance: {}\\n\".format(distance))\n","      f.write(\"Next state: {}\\n\".format(exp.next_state.observation))\n","      f.write(\"=\" * 80 + \"\\n\")\n","      f.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjwdtyT5GdQS"},"source":["# configs.py\n","import json\n","\n","\n","_dream_config_tree = {\n","  \"environment\": \"vanilla\",\n","  \"instruction_agent\": {\n","    \"type\": \"learned\",\n","    \"policy\": {\n","      \"type\": \"vanilla\",\n","      \"epsilon_schedule\": {\n","        \"begin\": 1.0,\n","        \"end\": 0.01,\n","        \"total_steps\": 250000\n","      },\n","      \"embedder\": {\n","        \"type\": \"instruction\",\n","        \"obs_embedder\": {\n","          \"embed_dim\": 64\n","        },\n","        \"instruction_embedder\": {\n","          \"embed_dim\": 64\n","        },\n","        \"transition_embedder\": {\n","          \"state_embed_dim\": 64,\n","          \"action_embed_dim\": 32,\n","          \"embed_dim\": 64\n","        },\n","        \"trajectory_embedder\": {\n","          \"type\": \"ours\",\n","          \"penalty\": 0.1\n","        },\n","        \"attention_query_dim\": 64,\n","        \"embed_dim\": 64\n","      },\n","      \"test_epsilon\": 0,\n","      \"discount\": 0.99\n","    },\n","    \"buffer\": {\n","      \"type\": \"vanilla\",\n","      \"max_buffer_size\": 100000\n","    },\n","    \"learning_rate\": 0.0001,\n","    \"sync_target_freq\": 5000,\n","    \"min_buffer_size\": 500,\n","    \"batch_size\": 32,\n","    \"update_freq\": 4,\n","    \"max_grad_norm\": 10\n","  },\n","  \"exploration_agent\": {\n","    \"type\": \"learned\",\n","    # \"type\": \"random\",\n","    \"policy\": {\n","      \"type\": \"recurrent\",\n","      \"epsilon_schedule\": {\n","        \"begin\": 1.0,\n","        \"end\": 0.01,\n","        \"total_steps\": 250000\n","      },\n","      \"embedder\": {\n","        \"type\": \"recurrent\",\n","        \"experience_embedder\": {\n","          \"state_embed_dim\": 64,\n","          \"action_embed_dim\": 16,\n","          \"embed_dim\": 64\n","        },\n","        \"embed_dim\": 64\n","      },\n","      \"test_epsilon\": 0,\n","      \"discount\": 0.99\n","    },\n","    \"buffer\": {\n","      \"type\": \"sequential\",\n","      \"max_buffer_size\": 16000,\n","      \"sequence_length\": 50\n","    },\n","    \"learning_rate\": 0.0001,\n","    \"sync_target_freq\": 5000,\n","    \"min_buffer_size\": 8000,\n","    \"batch_size\": 32,\n","    \"update_freq\": 4,\n","    \"max_grad_norm\": 10\n","  }\n","}\n","\n","with open(\"./dream.json\", \"w\") as f:\n","  json.dump(_dream_config_tree, f, indent=4, sort_keys=True)\n","\n","_rl2_config_tree = {\n","  \"environment\": \"vanilla\",\n","  \"agent\": {\n","    # \"type\": \"learned\",\n","    \"type\": \"random\",\n","    \"policy\": {\n","      \"type\": \"recurrent\",\n","      \"epsilon_schedule\": {\n","        \"begin\": 1.0,\n","        \"end\": 0.01,\n","        \"total_steps\": 500000\n","      },\n","      \"embedder\": {\n","        \"type\": \"recurrent\",\n","        \"experience_embedder\": {\n","          \"state_embed_dim\": 64,\n","          \"instruction_embed_dim\": 64,\n","          \"action_embed_dim\": 16,\n","          \"reward_embed_dim\": 16,\n","          \"done_embed_dim\": 16,\n","          \"embed_dim\": 64\n","        },\n","        \"embed_dim\": 64\n","      },\n","      \"test_epsilon\": 0,\n","      \"discount\": 0.99\n","    },\n","    \"buffer\": {\n","      \"type\": \"sequential\",\n","      \"max_buffer_size\": 16000,\n","      \"sequence_length\": 50\n","    },\n","    \"learning_rate\": 0.0001,\n","    \"sync_target_freq\": 5000,\n","    \"min_buffer_size\": 800,\n","    \"batch_size\": 32,\n","    \"update_freq\": 4,\n","    \"max_grad_norm\": 10\n","  }\n","}\n","\n","\n","with open(\"./rl2.json\", \"w\") as f:\n","  json.dump(_rl2_config_tree, f, indent=4, sort_keys=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZj7lHFYGdSw"},"source":["%tensorboard --logdir ./RL2/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"tWjBkzz_GdUk","outputId":"affb2443-e7a2-44a0-d580-52a66cdaf386"},"source":["# rl2.py\n","import argparse\n","import collections\n","import os\n","import shutil\n","\n","import numpy as np\n","import torch\n","import tqdm\n","\n","import config as cfg\n","import rl\n","import relabel\n","import utils\n","import wrappers\n","\n","\n","def main():\n","  arg_parser = argparse.ArgumentParser()\n","  arg_parser.add_argument(\n","      '-c', '--configs', action='append', default=[\"./rl2.json\"])\n","  arg_parser.add_argument(\n","      '-b', '--config_bindings', action='append', default=[],\n","      help=\"bindings to overwrite in the configs.\")\n","  arg_parser.add_argument(\n","      \"-x\", \"--base_dir\", default=\"experiments\",\n","      help=\"directory to log experiments\")\n","  arg_parser.add_argument(\n","      \"-p\", \"--checkpoint\", default=None,\n","      help=\"path to checkpoint directory to load from or None\")\n","  arg_parser.add_argument(\n","      \"-f\", \"--force_overwrite\", action=\"store_true\",\n","      help=\"Overwrites experiment under this experiment name, if it exists.\")\n","  arg_parser.add_argument(\n","      \"-s\", \"--seed\", default=0, help=\"random seed to use.\", type=int)\n","  arg_parser.add_argument(\"exp_name\", help=\"name of the experiment to run\")\n","  args = arg_parser.parse_args()\n","  config = cfg.Config.from_files_and_bindings(\n","      args.configs, args.config_bindings)\n","\n","  np.random.seed(args.seed)\n","  torch.manual_seed(args.seed)\n","\n","  exp_dir = './RL2'\n","  if os.path.exists(exp_dir) and not args.force_overwrite:\n","    raise ValueError(\"Experiment already exists at: {}\".format(exp_dir))\n","  shutil.rmtree(exp_dir, ignore_errors=True)  # remove directory if exists\n","  os.makedirs(exp_dir)\n","\n","  with open(os.path.join(exp_dir, \"config.json\"), \"w+\") as f:\n","    config.to_file(f)\n","  print(config)\n","\n","  env_class = get_env_class(config.get(\"environment\"))\n","\n","  # Use GPU if possible\n","  device = torch.device(\"cpu\")\n","  if torch.cuda.is_available():\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","    device = torch.device(\"cuda:0\")\n","\n","  print(\"Device: {}\".format(device))\n","  tb_writer = utils.EpisodeAndStepWriter(os.path.join(exp_dir, \"tensorboard\"))\n","\n","  text_dir = os.path.join(exp_dir, \"text\")\n","  os.makedirs(text_dir)\n","\n","  checkpoint_dir = os.path.join(exp_dir, \"checkpoints\")\n","  os.makedirs(checkpoint_dir)\n","\n","  create_env = env_class.create_env\n","  exploration_env = create_env(0)\n","  instruction_env = env_class.instruction_wrapper()(exploration_env, [])\n","  multi_episode_env = wrappers.MultiEpisodeWrapper(instruction_env)\n","  if config.get(\"agent\").get(\"type\") == \"random\":\n","    agent = policy.RandomPolicy(exploration_env.action_space)\n","  else:\n","    agent = DQNAgent.from_config(config.get(\"agent\"), multi_episode_env)\n","\n","  if args.checkpoint is not None:\n","    print(\"Loading checkpoint: {}\".format(args.checkpoint))\n","    agent.load_state_dict(torch.load(os.path.join(args.checkpoint, \"agent.pt\")))\n","\n","  batch_size = 32\n","  rewards = collections.deque(maxlen=200)\n","  episode_lengths = collections.deque(maxlen=200)\n","  total_steps = 0\n","  for episode_num in tqdm.tqdm(range(10000)):\n","    exploration_env = create_env(episode_num)\n","    instruction_env = env_class.instruction_wrapper()(\n","        exploration_env, [], seed=episode_num + 1,\n","        first_episode_no_optimization=True)\n","    multi_episode_env = wrappers.MultiEpisodeWrapper(instruction_env, 2)\n","\n","    # Switch between IDs and not IDs for methods that use IDs\n","    # Otherwise, no-op\n","    if episode_num % 2 == 0:\n","      if hasattr(agent, \"_dqn\"):\n","        if hasattr(agent._dqn._Q._state_embedder, \"use_ids\"):\n","          agent._dqn._Q._state_embedder.use_ids(True)\n","\n","    episode, _ = run_episode(multi_episode_env, agent)\n","\n","    for index, exp in enumerate(episode):\n","      agent.update(relabel.TrajectoryExperience(exp, episode, index))\n","\n","    if hasattr(agent, \"_dqn\"):\n","      if hasattr(agent._dqn._Q._state_embedder, \"use_ids\"):\n","        agent._dqn._Q._state_embedder.use_ids(False)\n","\n","    total_steps += len(episode)\n","    episode_lengths.append(len(episode))\n","    rewards.append(sum(exp.reward for exp in episode))\n","\n","    if episode_num % 100 == 0:\n","      for k, v in agent.stats.items():\n","        if v is not None:\n","          tb_writer.add_scalar(\n","              \"{}_{}\".format(\"agent\", k), v, episode_num, total_steps)\n","\n","      tb_writer.add_scalar(\"steps/total\", total_steps, episode_num, total_steps)\n","      tb_writer.add_scalar(\n","          \"reward/train\", np.mean(rewards), episode_num, total_steps)\n","      tb_writer.add_scalar(\n","          \"steps/steps_per_episode\", np.mean(episode_lengths), episode_num,\n","          total_steps)\n","\n","    if episode_num % 2000 == 0:\n","      visualize_dir = os.path.join(exp_dir, \"visualize\", str(episode_num))\n","      os.makedirs(visualize_dir, exist_ok=True)\n","\n","      test_rewards = []\n","      test_episode_lengths = []\n","      for test_index in tqdm.tqdm(range(100)):\n","        exploration_env = create_env(test_index, test=True)\n","        instruction_env = env_class.instruction_wrapper()(\n","            exploration_env, [], seed=test_index + 1, test=True,\n","            first_episode_no_optimization=True)\n","        multi_episode_env = wrappers.MultiEpisodeWrapper(instruction_env, 2)\n","        episode, render = run_episode(\n","            multi_episode_env, agent, test=True)\n","        test_episode_lengths.append(len(episode))\n","\n","        test_rewards.append(sum(exp.reward for exp in episode))\n","\n","        if test_index < 10:\n","          frames = [frame.image() for frame in render]\n","          save_path = os.path.join(visualize_dir, \"{}.gif\".format(test_index))\n","          frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                         duration=750, loop=0)\n","\n","      tb_writer.add_scalar(\n","          \"reward/test\", np.mean(test_rewards), episode_num, total_steps)\n","      tb_writer.add_scalar(\n","          \"steps/test_steps_per_episode\", np.mean(test_episode_lengths),\n","          episode_num, total_steps)\n","\n","      # Visualize training split\n","      visualize_dir = os.path.join(\n","          exp_dir, \"visualize\", str(episode_num), \"train\")\n","      os.makedirs(visualize_dir, exist_ok=True)\n","      for train_index in tqdm.tqdm(range(10)):\n","        exploration_env = create_env(train_index)\n","        # Test flags here only refer to making agent act with test flag and\n","        # not test split environments\n","        instruction_env = env_class.instruction_wrapper()(\n","            exploration_env, [], seed=train_index + 1,\n","            first_episode_no_optimization=True)\n","        multi_episode_env = wrappers.MultiEpisodeWrapper(instruction_env, 2)\n","        episode, render = run_episode(\n","            multi_episode_env, agent, test=True)\n","\n","        frames = [frame.image() for frame in render]\n","        save_path = os.path.join(visualize_dir, \"{}.gif\".format(train_index))\n","        frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                       duration=750, loop=0)\n","\n","      if total_steps > int(5e6):\n","        return\n","\n","    if episode_num != 0 and episode_num % 20000 == 0:\n","      print(\"Saving checkpoint\")\n","      save_dir = os.path.join(checkpoint_dir, str(episode_num))\n","      os.makedirs(save_dir)\n","\n","      torch.save(agent.state_dict(), os.path.join(save_dir, \"agent.pt\"))\n","\n","\n","if __name__ == '__main__':\n","  main()\n"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/10000 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["{\n","    \"agent\": {\n","        \"batch_size\": 32,\n","        \"buffer\": {\n","            \"max_buffer_size\": 16000,\n","            \"sequence_length\": 50,\n","            \"type\": \"sequential\"\n","        },\n","        \"learning_rate\": 0.0001,\n","        \"max_grad_norm\": 10,\n","        \"min_buffer_size\": 800,\n","        \"policy\": {\n","            \"discount\": 0.99,\n","            \"embedder\": {\n","                \"embed_dim\": 64,\n","                \"experience_embedder\": {\n","                    \"action_embed_dim\": 16,\n","                    \"done_embed_dim\": 16,\n","                    \"embed_dim\": 64,\n","                    \"instruction_embed_dim\": 64,\n","                    \"reward_embed_dim\": 16,\n","                    \"state_embed_dim\": 64\n","                },\n","                \"type\": \"recurrent\"\n","            },\n","            \"epsilon_schedule\": {\n","                \"begin\": 1.0,\n","                \"end\": 0.01,\n","                \"total_steps\": 500000\n","            },\n","            \"test_epsilon\": 0,\n","            \"type\": \"recurrent\"\n","        },\n","        \"sync_target_freq\": 5000,\n","        \"type\": \"learned\",\n","        \"update_freq\": 4\n","    },\n","    \"environment\": \"vanilla\"\n","}\n","Device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","  1%|          | 1/100 [00:01<02:42,  1.65s/it]\u001b[A\n","  2%|▏         | 2/100 [00:03<02:41,  1.64s/it]\u001b[A\n","  3%|▎         | 3/100 [00:04<02:32,  1.58s/it]\u001b[A\n","  4%|▍         | 4/100 [00:06<02:26,  1.52s/it]\u001b[A\n","  5%|▌         | 5/100 [00:07<02:20,  1.48s/it]\u001b[A\n","  6%|▌         | 6/100 [00:08<02:14,  1.43s/it]\u001b[A\n","  7%|▋         | 7/100 [00:10<02:12,  1.42s/it]\u001b[A\n","  8%|▊         | 8/100 [00:11<02:11,  1.43s/it]\u001b[A\n","  9%|▉         | 9/100 [00:13<02:08,  1.41s/it]\u001b[A\n"," 10%|█         | 10/100 [00:14<02:06,  1.40s/it]\u001b[A\n"," 11%|█         | 11/100 [00:14<01:32,  1.04s/it]\u001b[A\n"," 12%|█▏        | 12/100 [00:14<01:09,  1.27it/s]\u001b[A\n"," 13%|█▎        | 13/100 [00:15<00:53,  1.63it/s]\u001b[A\n"," 14%|█▍        | 14/100 [00:15<00:42,  2.04it/s]\u001b[A\n"," 15%|█▌        | 15/100 [00:15<00:34,  2.43it/s]\u001b[A\n"," 16%|█▌        | 16/100 [00:15<00:29,  2.81it/s]\u001b[A\n"," 17%|█▋        | 17/100 [00:15<00:26,  3.14it/s]\u001b[A\n"," 18%|█▊        | 18/100 [00:16<00:23,  3.50it/s]\u001b[A\n"," 19%|█▉        | 19/100 [00:16<00:21,  3.71it/s]\u001b[A\n"," 20%|██        | 20/100 [00:16<00:20,  3.90it/s]\u001b[A\n"," 21%|██        | 21/100 [00:16<00:19,  4.11it/s]\u001b[A\n"," 22%|██▏       | 22/100 [00:16<00:18,  4.25it/s]\u001b[A\n"," 23%|██▎       | 23/100 [00:17<00:17,  4.37it/s]\u001b[A\n"," 24%|██▍       | 24/100 [00:17<00:17,  4.43it/s]\u001b[A\n"," 25%|██▌       | 25/100 [00:17<00:16,  4.60it/s]\u001b[A\n"," 26%|██▌       | 26/100 [00:17<00:15,  4.73it/s]\u001b[A\n"," 27%|██▋       | 27/100 [00:18<00:15,  4.76it/s]\u001b[A\n"," 28%|██▊       | 28/100 [00:18<00:14,  4.84it/s]\u001b[A\n"," 29%|██▉       | 29/100 [00:18<00:14,  4.86it/s]\u001b[A\n"," 30%|███       | 30/100 [00:18<00:14,  4.91it/s]\u001b[A\n"," 31%|███       | 31/100 [00:18<00:13,  4.93it/s]\u001b[A\n"," 32%|███▏      | 32/100 [00:19<00:13,  4.89it/s]\u001b[A\n"," 33%|███▎      | 33/100 [00:19<00:13,  4.93it/s]\u001b[A\n"," 34%|███▍      | 34/100 [00:19<00:13,  4.89it/s]\u001b[A\n"," 35%|███▌      | 35/100 [00:19<00:13,  4.85it/s]\u001b[A\n"," 36%|███▌      | 36/100 [00:19<00:13,  4.75it/s]\u001b[A\n"," 37%|███▋      | 37/100 [00:20<00:13,  4.60it/s]\u001b[A\n"," 38%|███▊      | 38/100 [00:20<00:13,  4.47it/s]\u001b[A\n"," 39%|███▉      | 39/100 [00:20<00:13,  4.47it/s]\u001b[A\n"," 40%|████      | 40/100 [00:20<00:13,  4.57it/s]\u001b[A\n"," 41%|████      | 41/100 [00:20<00:12,  4.57it/s]\u001b[A\n"," 42%|████▏     | 42/100 [00:21<00:12,  4.63it/s]\u001b[A\n"," 43%|████▎     | 43/100 [00:21<00:12,  4.64it/s]\u001b[A\n"," 44%|████▍     | 44/100 [00:21<00:12,  4.57it/s]\u001b[A\n"," 45%|████▌     | 45/100 [00:21<00:12,  4.51it/s]\u001b[A\n"," 46%|████▌     | 46/100 [00:22<00:12,  4.41it/s]\u001b[A\n"," 47%|████▋     | 47/100 [00:22<00:12,  4.40it/s]\u001b[A\n"," 48%|████▊     | 48/100 [00:22<00:12,  4.28it/s]\u001b[A\n"," 49%|████▉     | 49/100 [00:22<00:11,  4.31it/s]\u001b[A\n"," 50%|█████     | 50/100 [00:23<00:11,  4.33it/s]\u001b[A\n"," 51%|█████     | 51/100 [00:23<00:11,  4.36it/s]\u001b[A\n"," 52%|█████▏    | 52/100 [00:23<00:11,  4.33it/s]\u001b[A\n"," 53%|█████▎    | 53/100 [00:23<00:10,  4.48it/s]\u001b[A\n"," 54%|█████▍    | 54/100 [00:23<00:09,  4.61it/s]\u001b[A\n"," 55%|█████▌    | 55/100 [00:24<00:09,  4.55it/s]\u001b[A\n"," 56%|█████▌    | 56/100 [00:24<00:09,  4.57it/s]\u001b[A\n"," 57%|█████▋    | 57/100 [00:24<00:09,  4.62it/s]\u001b[A\n"," 58%|█████▊    | 58/100 [00:24<00:09,  4.61it/s]\u001b[A\n"," 59%|█████▉    | 59/100 [00:25<00:08,  4.59it/s]\u001b[A\n"," 60%|██████    | 60/100 [00:25<00:08,  4.49it/s]\u001b[A\n"," 61%|██████    | 61/100 [00:25<00:08,  4.46it/s]\u001b[A\n"," 62%|██████▏   | 62/100 [00:25<00:08,  4.40it/s]\u001b[A\n"," 63%|██████▎   | 63/100 [00:25<00:08,  4.39it/s]\u001b[A\n"," 64%|██████▍   | 64/100 [00:26<00:08,  4.26it/s]\u001b[A\n"," 65%|██████▌   | 65/100 [00:26<00:08,  4.26it/s]\u001b[A\n"," 66%|██████▌   | 66/100 [00:26<00:07,  4.33it/s]\u001b[A\n"," 67%|██████▋   | 67/100 [00:26<00:07,  4.43it/s]\u001b[A\n"," 68%|██████▊   | 68/100 [00:27<00:07,  4.39it/s]\u001b[A\n"," 69%|██████▉   | 69/100 [00:27<00:06,  4.44it/s]\u001b[A\n"," 70%|███████   | 70/100 [00:27<00:06,  4.48it/s]\u001b[A\n"," 71%|███████   | 71/100 [00:27<00:06,  4.55it/s]\u001b[A\n"," 72%|███████▏  | 72/100 [00:27<00:06,  4.58it/s]\u001b[A\n"," 73%|███████▎  | 73/100 [00:28<00:06,  4.44it/s]\u001b[A\n"," 74%|███████▍  | 74/100 [00:28<00:05,  4.39it/s]\u001b[A\n"," 75%|███████▌  | 75/100 [00:28<00:05,  4.46it/s]\u001b[A\n"," 76%|███████▌  | 76/100 [00:28<00:05,  4.60it/s]\u001b[A\n"," 77%|███████▋  | 77/100 [00:29<00:04,  4.63it/s]\u001b[A\n"," 78%|███████▊  | 78/100 [00:29<00:04,  4.65it/s]\u001b[A\n"," 79%|███████▉  | 79/100 [00:29<00:04,  4.63it/s]\u001b[A\n"," 80%|████████  | 80/100 [00:29<00:04,  4.59it/s]\u001b[A\n"," 81%|████████  | 81/100 [00:29<00:04,  4.52it/s]\u001b[A\n"," 82%|████████▏ | 82/100 [00:30<00:04,  4.41it/s]\u001b[A\n"," 83%|████████▎ | 83/100 [00:30<00:03,  4.48it/s]\u001b[A\n"," 84%|████████▍ | 84/100 [00:30<00:03,  4.55it/s]\u001b[A\n"," 85%|████████▌ | 85/100 [00:30<00:03,  4.68it/s]\u001b[A\n"," 86%|████████▌ | 86/100 [00:30<00:02,  4.78it/s]\u001b[A\n"," 87%|████████▋ | 87/100 [00:31<00:02,  4.86it/s]\u001b[A\n"," 88%|████████▊ | 88/100 [00:31<00:02,  4.93it/s]\u001b[A\n"," 89%|████████▉ | 89/100 [00:31<00:02,  4.92it/s]\u001b[A\n"," 90%|█████████ | 90/100 [00:31<00:02,  4.93it/s]\u001b[A\n"," 91%|█████████ | 91/100 [00:32<00:01,  4.90it/s]\u001b[A\n"," 92%|█████████▏| 92/100 [00:32<00:01,  4.78it/s]\u001b[A\n"," 93%|█████████▎| 93/100 [00:32<00:01,  4.75it/s]\u001b[A\n"," 94%|█████████▍| 94/100 [00:32<00:01,  4.68it/s]\u001b[A\n"," 95%|█████████▌| 95/100 [00:32<00:01,  4.74it/s]\u001b[A\n"," 96%|█████████▌| 96/100 [00:33<00:00,  4.69it/s]\u001b[A\n"," 97%|█████████▋| 97/100 [00:33<00:00,  4.61it/s]\u001b[A\n"," 98%|█████████▊| 98/100 [00:33<00:00,  4.59it/s]\u001b[A\n"," 99%|█████████▉| 99/100 [00:33<00:00,  4.70it/s]\u001b[A\n","100%|██████████| 100/100 [00:33<00:00,  2.95it/s]\n","\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:01<00:12,  1.34s/it]\u001b[A\n"," 20%|██        | 2/10 [00:02<00:10,  1.33s/it]\u001b[A\n"," 30%|███       | 3/10 [00:03<00:09,  1.33s/it]\u001b[A\n"," 40%|████      | 4/10 [00:05<00:08,  1.36s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:06<00:06,  1.38s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:08<00:05,  1.35s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:09<00:04,  1.36s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:10<00:02,  1.36s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:12<00:01,  1.38s/it]\u001b[A\n","100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"," 20%|██        | 2000/10000 [14:17<1:51:47,  1.19it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","  1%|          | 1/100 [00:01<02:35,  1.57s/it]\u001b[A\n","  2%|▏         | 2/100 [00:03<02:30,  1.53s/it]\u001b[A\n","  3%|▎         | 3/100 [00:04<02:26,  1.51s/it]\u001b[A\n","  4%|▍         | 4/100 [00:05<02:24,  1.50s/it]\u001b[A\n","  5%|▌         | 5/100 [00:06<02:07,  1.34s/it]\u001b[A\n","  6%|▌         | 6/100 [00:08<02:12,  1.41s/it]\u001b[A\n","  7%|▋         | 7/100 [00:10<02:13,  1.44s/it]\u001b[A\n","  8%|▊         | 8/100 [00:11<02:10,  1.42s/it]\u001b[A\n","  9%|▉         | 9/100 [00:12<02:13,  1.47s/it]\u001b[A\n"," 10%|█         | 10/100 [00:14<02:11,  1.47s/it]\u001b[A\n"," 11%|█         | 11/100 [00:14<01:34,  1.07s/it]\u001b[A\n"," 12%|█▏        | 12/100 [00:14<01:11,  1.24it/s]\u001b[A\n"," 13%|█▎        | 13/100 [00:14<00:54,  1.59it/s]\u001b[A\n"," 14%|█▍        | 14/100 [00:15<00:41,  2.05it/s]\u001b[A\n"," 15%|█▌        | 15/100 [00:15<00:34,  2.47it/s]\u001b[A\n"," 16%|█▌        | 16/100 [00:15<00:28,  2.90it/s]\u001b[A\n"," 17%|█▋        | 17/100 [00:15<00:24,  3.34it/s]\u001b[A\n"," 18%|█▊        | 18/100 [00:15<00:22,  3.72it/s]\u001b[A\n"," 19%|█▉        | 19/100 [00:16<00:20,  3.97it/s]\u001b[A\n"," 20%|██        | 20/100 [00:16<00:17,  4.50it/s]\u001b[A\n"," 21%|██        | 21/100 [00:16<00:15,  5.10it/s]\u001b[A\n"," 22%|██▏       | 22/100 [00:16<00:15,  5.11it/s]\u001b[A\n"," 23%|██▎       | 23/100 [00:16<00:14,  5.18it/s]\u001b[A\n"," 24%|██▍       | 24/100 [00:17<00:14,  5.17it/s]\u001b[A\n"," 25%|██▌       | 25/100 [00:17<00:14,  5.07it/s]\u001b[A\n"," 26%|██▌       | 26/100 [00:17<00:14,  5.11it/s]\u001b[A\n"," 27%|██▋       | 27/100 [00:17<00:14,  5.14it/s]\u001b[A\n"," 28%|██▊       | 28/100 [00:17<00:13,  5.22it/s]\u001b[A\n"," 29%|██▉       | 29/100 [00:17<00:13,  5.22it/s]\u001b[A\n"," 30%|███       | 30/100 [00:18<00:13,  5.14it/s]\u001b[A\n"," 31%|███       | 31/100 [00:18<00:13,  5.18it/s]\u001b[A\n"," 32%|███▏      | 32/100 [00:18<00:13,  5.21it/s]\u001b[A\n"," 33%|███▎      | 33/100 [00:18<00:12,  5.22it/s]\u001b[A\n"," 34%|███▍      | 34/100 [00:18<00:11,  5.64it/s]\u001b[A\n"," 35%|███▌      | 35/100 [00:19<00:11,  5.48it/s]\u001b[A\n"," 36%|███▌      | 36/100 [00:19<00:11,  5.35it/s]\u001b[A\n"," 37%|███▋      | 37/100 [00:19<00:11,  5.31it/s]\u001b[A\n"," 38%|███▊      | 38/100 [00:19<00:11,  5.20it/s]\u001b[A\n"," 39%|███▉      | 39/100 [00:19<00:11,  5.17it/s]\u001b[A\n"," 40%|████      | 40/100 [00:20<00:11,  5.15it/s]\u001b[A\n"," 41%|████      | 41/100 [00:20<00:11,  5.11it/s]\u001b[A\n"," 42%|████▏     | 42/100 [00:20<00:11,  5.10it/s]\u001b[A\n"," 43%|████▎     | 43/100 [00:20<00:10,  5.69it/s]\u001b[A\n"," 44%|████▍     | 44/100 [00:20<00:09,  5.65it/s]\u001b[A\n"," 45%|████▌     | 45/100 [00:20<00:08,  6.11it/s]\u001b[A\n"," 46%|████▌     | 46/100 [00:21<00:08,  6.37it/s]\u001b[A\n"," 47%|████▋     | 47/100 [00:21<00:09,  5.87it/s]\u001b[A\n"," 48%|████▊     | 48/100 [00:21<00:09,  5.76it/s]\u001b[A\n"," 49%|████▉     | 49/100 [00:21<00:09,  5.58it/s]\u001b[A\n"," 50%|█████     | 50/100 [00:21<00:09,  5.38it/s]\u001b[A\n"," 51%|█████     | 51/100 [00:22<00:09,  5.29it/s]\u001b[A\n"," 52%|█████▏    | 52/100 [00:22<00:09,  5.06it/s]\u001b[A\n"," 53%|█████▎    | 53/100 [00:22<00:09,  4.93it/s]\u001b[A\n"," 54%|█████▍    | 54/100 [00:22<00:09,  4.92it/s]\u001b[A\n"," 55%|█████▌    | 55/100 [00:22<00:08,  5.32it/s]\u001b[A\n"," 56%|█████▌    | 56/100 [00:23<00:08,  5.17it/s]\u001b[A\n"," 57%|█████▋    | 57/100 [00:23<00:08,  5.11it/s]\u001b[A\n"," 58%|█████▊    | 58/100 [00:23<00:08,  5.10it/s]\u001b[A\n"," 59%|█████▉    | 59/100 [00:23<00:08,  5.09it/s]\u001b[A\n"," 60%|██████    | 60/100 [00:23<00:07,  5.50it/s]\u001b[A\n"," 61%|██████    | 61/100 [00:23<00:07,  5.34it/s]\u001b[A\n"," 62%|██████▏   | 62/100 [00:24<00:07,  5.28it/s]\u001b[A\n"," 63%|██████▎   | 63/100 [00:24<00:07,  5.16it/s]\u001b[A\n"," 64%|██████▍   | 64/100 [00:24<00:07,  5.09it/s]\u001b[A\n"," 65%|██████▌   | 65/100 [00:24<00:07,  4.97it/s]\u001b[A\n"," 66%|██████▌   | 66/100 [00:24<00:06,  4.98it/s]\u001b[A\n"," 67%|██████▋   | 67/100 [00:25<00:06,  4.81it/s]\u001b[A\n"," 68%|██████▊   | 68/100 [00:25<00:06,  4.74it/s]\u001b[A\n"," 69%|██████▉   | 69/100 [00:25<00:06,  4.75it/s]\u001b[A\n"," 70%|███████   | 70/100 [00:25<00:06,  4.80it/s]\u001b[A\n"," 71%|███████   | 71/100 [00:26<00:06,  4.81it/s]\u001b[A\n"," 72%|███████▏  | 72/100 [00:26<00:05,  4.70it/s]\u001b[A\n"," 73%|███████▎  | 73/100 [00:26<00:05,  4.75it/s]\u001b[A\n"," 74%|███████▍  | 74/100 [00:26<00:04,  5.40it/s]\u001b[A\n"," 75%|███████▌  | 75/100 [00:26<00:04,  5.30it/s]\u001b[A\n"," 76%|███████▌  | 76/100 [00:26<00:04,  5.26it/s]\u001b[A\n"," 77%|███████▋  | 77/100 [00:27<00:04,  5.20it/s]\u001b[A\n"," 78%|███████▊  | 78/100 [00:27<00:04,  5.19it/s]\u001b[A\n"," 79%|███████▉  | 79/100 [00:27<00:04,  5.06it/s]\u001b[A\n"," 80%|████████  | 80/100 [00:27<00:03,  5.03it/s]\u001b[A\n"," 81%|████████  | 81/100 [00:28<00:03,  4.93it/s]\u001b[A\n"," 82%|████████▏ | 82/100 [00:28<00:03,  4.85it/s]\u001b[A\n"," 83%|████████▎ | 83/100 [00:28<00:03,  5.29it/s]\u001b[A\n"," 84%|████████▍ | 84/100 [00:28<00:03,  5.12it/s]\u001b[A\n"," 85%|████████▌ | 85/100 [00:28<00:02,  5.12it/s]\u001b[A\n"," 86%|████████▌ | 86/100 [00:28<00:02,  5.13it/s]\u001b[A\n"," 87%|████████▋ | 87/100 [00:29<00:02,  5.10it/s]\u001b[A\n"," 88%|████████▊ | 88/100 [00:29<00:02,  5.69it/s]\u001b[A\n"," 89%|████████▉ | 89/100 [00:29<00:01,  6.14it/s]\u001b[A\n"," 90%|█████████ | 90/100 [00:29<00:01,  5.86it/s]\u001b[A\n"," 91%|█████████ | 91/100 [00:29<00:01,  5.56it/s]\u001b[A\n"," 92%|█████████▏| 92/100 [00:30<00:01,  5.41it/s]\u001b[A\n"," 93%|█████████▎| 93/100 [00:30<00:01,  5.17it/s]\u001b[A\n"," 94%|█████████▍| 94/100 [00:30<00:01,  5.00it/s]\u001b[A\n"," 95%|█████████▌| 95/100 [00:30<00:01,  4.83it/s]\u001b[A\n"," 96%|█████████▌| 96/100 [00:30<00:00,  4.74it/s]\u001b[A\n"," 97%|█████████▋| 97/100 [00:31<00:00,  4.63it/s]\u001b[A\n"," 98%|█████████▊| 98/100 [00:31<00:00,  4.51it/s]\u001b[A\n"," 99%|█████████▉| 99/100 [00:31<00:00,  4.54it/s]\u001b[A\n","100%|██████████| 100/100 [00:31<00:00,  3.15it/s]\n","\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:01<00:13,  1.46s/it]\u001b[A\n"," 20%|██        | 2/10 [00:03<00:11,  1.48s/it]\u001b[A\n"," 30%|███       | 3/10 [00:04<00:10,  1.49s/it]\u001b[A\n"," 40%|████      | 4/10 [00:05<00:08,  1.47s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:07<00:06,  1.36s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:08<00:05,  1.37s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:09<00:04,  1.38s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:11<00:02,  1.37s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:12<00:01,  1.42s/it]\u001b[A\n","100%|██████████| 10/10 [00:14<00:00,  1.41s/it]\n"," 40%|████      | 4000/10000 [37:04<1:01:42,  1.62it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","  1%|          | 1/100 [00:01<02:48,  1.70s/it]\u001b[A\n","  2%|▏         | 2/100 [00:03<02:40,  1.64s/it]\u001b[A\n","  3%|▎         | 3/100 [00:04<02:33,  1.58s/it]\u001b[A\n","  4%|▍         | 4/100 [00:06<02:28,  1.55s/it]\u001b[A\n","  5%|▌         | 5/100 [00:06<02:07,  1.34s/it]\u001b[A\n","  6%|▌         | 6/100 [00:08<02:08,  1.37s/it]\u001b[A\n","  7%|▋         | 7/100 [00:09<02:12,  1.42s/it]\u001b[A\n","  8%|▊         | 8/100 [00:11<02:13,  1.46s/it]\u001b[A\n","  9%|▉         | 9/100 [00:13<02:15,  1.48s/it]\u001b[A\n"," 10%|█         | 10/100 [00:14<02:13,  1.49s/it]\u001b[A\n"," 11%|█         | 11/100 [00:14<01:36,  1.08s/it]\u001b[A\n"," 12%|█▏        | 12/100 [00:14<01:12,  1.21it/s]\u001b[A\n"," 13%|█▎        | 13/100 [00:15<00:55,  1.55it/s]\u001b[A\n"," 14%|█▍        | 14/100 [00:15<00:46,  1.84it/s]\u001b[A\n"," 15%|█▌        | 15/100 [00:15<00:35,  2.37it/s]\u001b[A\n"," 16%|█▌        | 16/100 [00:15<00:32,  2.57it/s]\u001b[A\n"," 17%|█▋        | 17/100 [00:16<00:28,  2.91it/s]\u001b[A\n"," 18%|█▊        | 18/100 [00:16<00:25,  3.25it/s]\u001b[A\n"," 19%|█▉        | 19/100 [00:16<00:22,  3.56it/s]\u001b[A\n"," 20%|██        | 20/100 [00:16<00:19,  4.13it/s]\u001b[A\n"," 21%|██        | 21/100 [00:16<00:18,  4.18it/s]\u001b[A\n"," 22%|██▏       | 22/100 [00:17<00:18,  4.29it/s]\u001b[A\n"," 23%|██▎       | 23/100 [00:17<00:15,  4.92it/s]\u001b[A\n"," 24%|██▍       | 24/100 [00:17<00:15,  4.83it/s]\u001b[A\n"," 25%|██▌       | 25/100 [00:17<00:16,  4.69it/s]\u001b[A\n"," 26%|██▌       | 26/100 [00:17<00:16,  4.61it/s]\u001b[A\n"," 27%|██▋       | 27/100 [00:18<00:15,  4.59it/s]\u001b[A\n"," 28%|██▊       | 28/100 [00:18<00:15,  4.57it/s]\u001b[A\n"," 29%|██▉       | 29/100 [00:18<00:15,  4.56it/s]\u001b[A\n"," 30%|███       | 30/100 [00:18<00:15,  4.53it/s]\u001b[A\n"," 31%|███       | 31/100 [00:19<00:14,  4.71it/s]\u001b[A\n"," 32%|███▏      | 32/100 [00:19<00:14,  4.84it/s]\u001b[A\n"," 33%|███▎      | 33/100 [00:19<00:12,  5.46it/s]\u001b[A\n"," 34%|███▍      | 34/100 [00:19<00:11,  5.99it/s]\u001b[A\n"," 35%|███▌      | 35/100 [00:19<00:11,  5.75it/s]\u001b[A\n"," 36%|███▌      | 36/100 [00:19<00:11,  5.55it/s]\u001b[A\n"," 37%|███▋      | 37/100 [00:20<00:11,  5.39it/s]\u001b[A\n"," 38%|███▊      | 38/100 [00:20<00:11,  5.32it/s]\u001b[A\n"," 39%|███▉      | 39/100 [00:20<00:11,  5.29it/s]\u001b[A\n"," 40%|████      | 40/100 [00:20<00:10,  5.84it/s]\u001b[A\n"," 41%|████      | 41/100 [00:20<00:10,  5.48it/s]\u001b[A\n"," 42%|████▏     | 42/100 [00:21<00:11,  5.03it/s]\u001b[A\n"," 43%|████▎     | 43/100 [00:21<00:10,  5.30it/s]\u001b[A\n"," 44%|████▍     | 44/100 [00:21<00:11,  5.01it/s]\u001b[A\n"," 45%|████▌     | 45/100 [00:21<00:10,  5.46it/s]\u001b[A\n"," 46%|████▌     | 46/100 [00:21<00:09,  5.86it/s]\u001b[A\n"," 47%|████▋     | 47/100 [00:21<00:10,  5.27it/s]\u001b[A\n"," 48%|████▊     | 48/100 [00:22<00:10,  4.92it/s]\u001b[A\n"," 49%|████▉     | 49/100 [00:22<00:09,  5.32it/s]\u001b[A\n"," 50%|█████     | 50/100 [00:22<00:10,  5.00it/s]\u001b[A\n"," 51%|█████     | 51/100 [00:22<00:10,  4.85it/s]\u001b[A\n"," 52%|█████▏    | 52/100 [00:22<00:09,  5.14it/s]\u001b[A\n"," 53%|█████▎    | 53/100 [00:23<00:09,  4.90it/s]\u001b[A\n"," 54%|█████▍    | 54/100 [00:23<00:09,  4.69it/s]\u001b[A\n"," 55%|█████▌    | 55/100 [00:23<00:08,  5.23it/s]\u001b[A\n"," 56%|█████▌    | 56/100 [00:23<00:08,  5.14it/s]\u001b[A\n"," 57%|█████▋    | 57/100 [00:23<00:07,  5.55it/s]\u001b[A\n"," 58%|█████▊    | 58/100 [00:24<00:07,  5.35it/s]\u001b[A\n"," 59%|█████▉    | 59/100 [00:24<00:07,  5.82it/s]\u001b[A\n"," 60%|██████    | 60/100 [00:24<00:06,  6.33it/s]\u001b[A\n"," 61%|██████    | 61/100 [00:24<00:06,  5.79it/s]\u001b[A\n"," 62%|██████▏   | 62/100 [00:24<00:06,  5.60it/s]\u001b[A\n"," 63%|██████▎   | 63/100 [00:24<00:06,  5.91it/s]\u001b[A\n"," 64%|██████▍   | 64/100 [00:25<00:06,  5.55it/s]\u001b[A\n"," 65%|██████▌   | 65/100 [00:25<00:06,  5.43it/s]\u001b[A\n"," 66%|██████▌   | 66/100 [00:25<00:06,  5.34it/s]\u001b[A\n"," 67%|██████▋   | 67/100 [00:25<00:06,  5.17it/s]\u001b[A\n"," 68%|██████▊   | 68/100 [00:25<00:06,  4.96it/s]\u001b[A\n"," 69%|██████▉   | 69/100 [00:26<00:06,  4.84it/s]\u001b[A\n"," 70%|███████   | 70/100 [00:26<00:06,  4.79it/s]\u001b[A\n"," 71%|███████   | 71/100 [00:26<00:05,  4.85it/s]\u001b[A\n"," 72%|███████▏  | 72/100 [00:26<00:05,  4.93it/s]\u001b[A\n"," 73%|███████▎  | 73/100 [00:26<00:05,  4.95it/s]\u001b[A\n"," 74%|███████▍  | 74/100 [00:27<00:04,  5.40it/s]\u001b[A\n"," 75%|███████▌  | 75/100 [00:27<00:04,  5.22it/s]\u001b[A\n"," 76%|███████▌  | 76/100 [00:27<00:04,  5.77it/s]\u001b[A\n"," 77%|███████▋  | 77/100 [00:27<00:04,  5.40it/s]\u001b[A\n"," 78%|███████▊  | 78/100 [00:27<00:04,  5.15it/s]\u001b[A\n"," 79%|███████▉  | 79/100 [00:28<00:03,  5.52it/s]\u001b[A\n"," 80%|████████  | 80/100 [00:28<00:03,  5.17it/s]\u001b[A\n"," 81%|████████  | 81/100 [00:28<00:03,  5.02it/s]\u001b[A\n"," 82%|████████▏ | 82/100 [00:28<00:03,  4.87it/s]\u001b[A\n"," 83%|████████▎ | 83/100 [00:28<00:03,  5.38it/s]\u001b[A\n"," 84%|████████▍ | 84/100 [00:29<00:03,  5.10it/s]\u001b[A\n"," 85%|████████▌ | 85/100 [00:29<00:03,  4.92it/s]\u001b[A\n"," 86%|████████▌ | 86/100 [00:29<00:02,  4.75it/s]\u001b[A\n"," 87%|████████▋ | 87/100 [00:29<00:02,  4.71it/s]\u001b[A\n"," 88%|████████▊ | 88/100 [00:29<00:02,  5.13it/s]\u001b[A\n"," 89%|████████▉ | 89/100 [00:30<00:02,  5.49it/s]\u001b[A\n"," 90%|█████████ | 90/100 [00:30<00:01,  5.10it/s]\u001b[A\n"," 91%|█████████ | 91/100 [00:30<00:01,  5.04it/s]\u001b[A\n"," 92%|█████████▏| 92/100 [00:30<00:01,  4.90it/s]\u001b[A\n"," 93%|█████████▎| 93/100 [00:30<00:01,  4.79it/s]\u001b[A\n"," 94%|█████████▍| 94/100 [00:31<00:01,  4.57it/s]\u001b[A\n"," 95%|█████████▌| 95/100 [00:31<00:01,  4.55it/s]\u001b[A\n"," 96%|█████████▌| 96/100 [00:31<00:00,  4.58it/s]\u001b[A\n"," 97%|█████████▋| 97/100 [00:31<00:00,  4.67it/s]\u001b[A\n"," 98%|█████████▊| 98/100 [00:31<00:00,  4.58it/s]\u001b[A\n"," 99%|█████████▉| 99/100 [00:32<00:00,  4.44it/s]\u001b[A\n","100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n","\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 10%|█         | 1/10 [00:01<00:14,  1.61s/it]\u001b[A\n"," 20%|██        | 2/10 [00:03<00:12,  1.60s/it]\u001b[A\n"," 30%|███       | 3/10 [00:04<00:11,  1.59s/it]\u001b[A\n"," 40%|████      | 4/10 [00:06<00:09,  1.55s/it]\u001b[A\n"," 50%|█████     | 5/10 [00:07<00:06,  1.35s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:08<00:05,  1.38s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:10<00:04,  1.45s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:11<00:02,  1.46s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:13<00:01,  1.50s/it]\u001b[A\n","100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n"," 59%|█████▉    | 5894/10000 [1:00:02<43:56,  1.56it/s]"]}]},{"cell_type":"code","metadata":{"id":"0UZzQJXIHMNc"},"source":["%tensorboard --logdir ./DREAM/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hehNZfIVHMK_"},"source":["# dream.py\n","import argparse\n","import collections\n","import os\n","import shutil\n","\n","import numpy as np\n","import torch\n","import tqdm\n","\n","import config as cfg\n","import grid\n","import policy\n","import relabel\n","import rl\n","import utils\n","\n","def main():\n","  arg_parser = argparse.ArgumentParser()\n","  arg_parser.add_argument(\n","      '-c', '--configs', action='append', default=[\"./dream.json\"])\n","  arg_parser.add_argument(\n","      '-b', '--config_bindings', action='append', default=[],\n","      help=\"bindings to overwrite in the configs.\")\n","  arg_parser.add_argument(\n","      \"-x\", \"--base_dir\", default=\"experiments\",\n","      help=\"directory to log experiments\")\n","  arg_parser.add_argument(\n","      \"-p\", \"--checkpoint\", default=None,\n","      help=\"path to checkpoint directory to load from or None\")\n","  arg_parser.add_argument(\n","      \"-f\", \"--force_overwrite\", action=\"store_true\",\n","      help=\"Overwrites experiment under this experiment name, if it exists.\")\n","  arg_parser.add_argument(\n","      \"-s\", \"--seed\", default=0, help=\"random seed to use.\", type=int)\n","  arg_parser.add_argument(\"exp_name\", help=\"name of the experiment to run\")\n","  args = arg_parser.parse_args()\n","  config = cfg.Config.from_files_and_bindings(\n","      args.configs, args.config_bindings)\n","\n","  np.random.seed(args.seed)\n","  torch.manual_seed(args.seed)\n","\n","  exp_dir = './DREAM'\n","  if os.path.exists(exp_dir) and not args.force_overwrite:\n","    raise ValueError(\"Experiment already exists at: {}\".format(exp_dir))\n","  shutil.rmtree(exp_dir, ignore_errors=True)  # remove directory if exists\n","  os.makedirs(exp_dir)\n","\n","  with open(os.path.join(exp_dir, \"config.json\"), \"w+\") as f:\n","    config.to_file(f)\n","  print(config)\n","\n","  env_class = get_env_class(config.get(\"environment\"))\n","\n","  # Use GPU if possible\n","  device = torch.device(\"cpu\")\n","  if torch.cuda.is_available():\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","    device = torch.device(\"cuda:0\")\n","\n","  print(\"Device: {}\".format(device))\n","  tb_writer = utils.EpisodeAndStepWriter(os.path.join(exp_dir, \"tensorboard\"))\n","\n","  text_dir = os.path.join(exp_dir, \"text\")\n","  os.makedirs(text_dir)\n","\n","  checkpoint_dir = os.path.join(exp_dir, \"checkpoints\")\n","  os.makedirs(checkpoint_dir)\n","\n","  create_env = env_class.create_env\n","  exploration_env = create_env(0)\n","  instruction_env = env_class.instruction_wrapper()(exploration_env, [])\n","  instruction_config = config.get(\"instruction_agent\")\n","  instruction_agent = get_instruction_agent(instruction_config, instruction_env)\n","\n","  exploration_config = config.get(\"exploration_agent\")\n","  exploration_agent = get_exploration_agent(exploration_config, exploration_env)\n","\n","  # Should probably expose this more gracefully\n","  trajectory_embedder = (\n","      instruction_agent._dqn._Q._state_embedder._trajectory_embedder)\n","  exploration_agent.set_reward_relabeler(trajectory_embedder)\n","\n","  # Due to the above hack, the trajectory embedder is being loaded twice.\n","  if args.checkpoint is not None:\n","    print(\"Loading checkpoint: {}\".format(args.checkpoint))\n","    instruction_agent.load_state_dict(\n","        torch.load(os.path.join(args.checkpoint, \"instruction.pt\")))\n","    exploration_agent.load_state_dict(\n","        torch.load(os.path.join(args.checkpoint, \"exploration.pt\")))\n","\n","  batch_size = 32\n","  rewards = collections.deque(maxlen=200)\n","  relabel_rewards = collections.deque(maxlen=200)\n","  exploration_lengths = collections.deque(maxlen=200)\n","  exploration_steps = 0\n","  instruction_steps = 0\n","  for step in tqdm.tqdm(range(10000)):\n","    exploration_env = create_env(step)\n","    exploration_episode, _ = run_episode(\n","        # Exploration episode gets ignored\n","        env_class.instruction_wrapper()(\n","            exploration_env, [], seed=max(0, step - 1)),\n","        exploration_agent)\n","\n","    # Needed to keep references to the trajectory and index for reward labeling\n","    for index, exp in enumerate(exploration_episode):\n","      exploration_agent.update(\n","          relabel.TrajectoryExperience(exp, exploration_episode, index))\n","\n","    exploration_steps += len(exploration_episode)\n","    exploration_lengths.append(len(exploration_episode))\n","\n","    # Don't share same random seed between exploration env and instructions\n","    instruction_env = env_class.instruction_wrapper()(\n","        exploration_env, exploration_episode, seed=step + 1)\n","\n","    if step % 2 == 0:\n","      trajectory_embedder.use_ids(False)\n","    episode, _ = run_episode(\n","        instruction_env, instruction_agent,\n","        experience_observers=[instruction_agent.update])\n","    instruction_steps += len(episode)\n","    trajectory_embedder.use_ids(True)\n","\n","    rewards.append(sum(exp.reward for exp in episode))\n","\n","    # Log reward for exploration agent\n","    exploration_rewards, distances = trajectory_embedder.label_rewards(\n","        [exploration_episode])\n","    exploration_rewards = exploration_rewards[0]\n","    distances = distances[0]\n","    relabel_rewards.append(exploration_rewards.sum().item())\n","\n","    if step % 100 == 0:\n","      path = os.path.join(text_dir, \"{}.txt\".format(step))\n","      log_episode(exploration_episode, exploration_rewards, distances, path)\n","\n","    if step % 100 == 0:\n","      for k, v in instruction_agent.stats.items():\n","        if v is not None:\n","          tb_writer.add_scalar(\n","              \"{}_{}\".format(\"instruction\", k), v, step,\n","              exploration_steps + instruction_steps)\n","\n","      for k, v in exploration_agent.stats.items():\n","        if v is not None:\n","          tb_writer.add_scalar(\n","              \"{}_{}\".format(\"exploration\", k), v, step,\n","              exploration_steps + instruction_steps)\n","\n","      tb_writer.add_scalar(\n","          \"steps/exploration\", exploration_steps, step,\n","          exploration_steps + instruction_steps)\n","      tb_writer.add_scalar(\n","          \"steps/instruction\", instruction_steps, step,\n","          exploration_steps + instruction_steps)\n","      tb_writer.add_scalar(\n","          \"reward/train\", np.mean(rewards), step,\n","          exploration_steps + instruction_steps)\n","      tb_writer.add_scalar(\n","          \"reward/exploration\", np.mean(relabel_rewards), step,\n","          exploration_steps + instruction_steps)\n","      tb_writer.add_scalar(\n","          \"steps/exploration_per_episode\", np.mean(exploration_lengths),\n","          step, exploration_steps + instruction_steps)\n","\n","    if step % 2000 == 0:\n","      visualize_dir = os.path.join(exp_dir, \"visualize\", str(step))\n","      os.makedirs(visualize_dir, exist_ok=True)\n","\n","      test_rewards = []\n","      test_exploration_lengths = []\n","      trajectory_embedder.use_ids(False)\n","      for test_index in tqdm.tqdm(range(100)):\n","        exploration_env = create_env(test_index, test=True)\n","        exploration_episode, exploration_render = run_episode(\n","            env_class.instruction_wrapper()(\n","                exploration_env, [], seed=max(0, test_index - 1), test=True),\n","            exploration_agent, test=True)\n","        test_exploration_lengths.append(len(exploration_episode))\n","\n","        instruction_env = env_class.instruction_wrapper()(\n","            exploration_env, exploration_episode, seed=test_index + 1, test=True)\n","        episode, render = run_episode(\n","            instruction_env, instruction_agent, test=True)\n","        test_rewards.append(sum(exp.reward for exp in episode))\n","\n","        if test_index < 10:\n","          frames = [frame.image() for frame in render]\n","          save_path = os.path.join(\n","              visualize_dir, \"{}-instruction.gif\".format(test_index))\n","          frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                         duration=750, loop=0, optimize=True, quality=20)\n","\n","          frames = [frame.image() for frame in exploration_render]\n","          save_path = os.path.join(\n","              visualize_dir, \"{}-exploration.gif\".format(test_index))\n","          frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                         duration=750, loop=0, optimize=True, quality=20)\n","\n","      tb_writer.add_scalar(\n","          \"reward/test\", np.mean(test_rewards), step,\n","          exploration_steps + instruction_steps)\n","      tb_writer.add_scalar(\n","          \"steps/test_exploration_per_episode\",\n","          np.mean(test_exploration_lengths), step,\n","          exploration_steps + instruction_steps)\n","\n","      # Visualize training split\n","      visualize_dir = os.path.join(exp_dir, \"visualize\", str(step), \"train\")\n","      os.makedirs(visualize_dir, exist_ok=True)\n","      for train_index in tqdm.tqdm(range(10)):\n","        exploration_env = create_env(train_index)\n","        # Test flags here only refer to making agent act with test flag and\n","        # not test split environments\n","        exploration_episode, exploration_render = run_episode(\n","            env_class.instruction_wrapper()(\n","                exploration_env, [], seed=max(0, train_index - 1)),\n","            exploration_agent, test=True)\n","\n","        instruction_env = env_class.instruction_wrapper()(\n","            exploration_env, exploration_episode, seed=train_index + 1)\n","        episode, render = run_episode(\n","            instruction_env, instruction_agent, test=True)\n","\n","        frames = [frame.image() for frame in render]\n","        save_path = os.path.join(\n","            visualize_dir, \"{}-instruction.gif\".format(train_index))\n","        frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                       duration=750, loop=0)\n","\n","        frames = [frame.image() for frame in exploration_render]\n","        save_path = os.path.join(\n","            visualize_dir, \"{}-exploration.gif\".format(train_index))\n","        frames[0].save(save_path, save_all=True, append_images=frames[1:],\n","                       duration=750, loop=0)\n","      trajectory_embedder.use_ids(True)\n","\n","      if exploration_steps + instruction_steps > int(5e6):\n","        return\n","\n","    if step != 0 and step % 20000 == 0:\n","      print(\"Saving checkpoint\")\n","      save_dir = os.path.join(checkpoint_dir, str(step))\n","      os.makedirs(save_dir)\n","\n","      torch.save(instruction_agent.state_dict(),\n","                 os.path.join(save_dir, \"instruction.pt\"))\n","      torch.save(exploration_agent.state_dict(),\n","                 os.path.join(save_dir, \"exploration.pt\"))\n","\n","\n","if __name__ == '__main__':\n","  main()\n"],"execution_count":null,"outputs":[]}]}