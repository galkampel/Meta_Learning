{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS330 Homework3_ Stencil.ipynb","provenance":[{"file_id":"1uYqzq4Ix91DD4QdElyrbC_If0TKDf_3t","timestamp":1629824967704},{"file_id":"1VDBw9P1Bs1Xpzp8B3mxmQAdcXrh15yOZ","timestamp":1602965367864}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fwmQwFeQo2qW"},"source":["## CS 330 Homework 3 Installation\n","\n","The following code blocks will install the required libraries.\n"]},{"cell_type":"markdown","metadata":{"id":"EB7xAlJMrHwA"},"source":["## Setup for Google Drive and Required Libraries\n"]},{"cell_type":"code","metadata":{"id":"pa0Ri_edrNIT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579331409,"user_tz":-180,"elapsed":24674,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"f59a6bd2-4676-4d53-a7dc-ef570298a862"},"source":["#@title Mount Google Drive\n","#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \n","#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#@title set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs330_fall2020'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"f-raFHMIpUun","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579379261,"user_tz":-180,"elapsed":44190,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"6a0e974e-f0bc-48b3-c1d8-596ae452ff8c"},"source":["#@title Install Requirements\n","#@markdown Requirements for the assignment and display drivers\n","\n","# Robot sim\n","!pip install gym==0.15.4\n","!pip install pygame\n","\n","# Various things for render\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","\n","# Rendering Environment\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!sudo apt-get install -y xvfb ffmpeg\n","!pip install imageio\n","!pip install PILLOW"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.15.4\n","  Downloading gym-0.15.4.tar.gz (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n","Collecting pyglet<=1.3.2,>=1.2.0\n","  Downloading pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 33.1 MB/s \n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.15.4-py3-none-any.whl size=1648483 sha256=dcdc453f025e0b1a303cae7a8e07779fd07aaa7c8f58b2cccda607741cf6cd4c\n","  Stored in directory: /root/.cache/pip/wheels/27/97/51/3adbfe67f40bce89b8eba2d3b8f42ec1c9f9c1e6305a73510d\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: pyglet\n","    Found existing installation: pyglet 1.5.0\n","    Uninstalling pyglet-1.5.0:\n","      Successfully uninstalled pyglet-1.5.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.13.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n","Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2\n","Collecting pygame\n","  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[K     |████████████████████████████████| 11.8 MB 147 kB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.0.1\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 2s (286 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 155013 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,270 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n","Fetched 784 kB in 2s (419 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 157368 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n","Collecting EasyProcess\n","  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.2\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates\n","  Downloading piglet_templates-1.2.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.2.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.4.7)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.2.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.19.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n","Requirement already satisfied: PILLOW in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"5yTRSgI-ryd-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579379262,"user_tz":-180,"elapsed":27,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"f9b9cfa8-7ecf-46be-bd07-8088ee492179"},"source":["#@title Download Mujoco from an online repository\n","\n","MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n","if not os.path.exists(MJC_PATH):\n","  %mkdir $MJC_PATH\n","%cd $MJC_PATH\n","if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n","  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n","  !unzip -q mujoco200_linux.zip\n","  %mv mujoco200_linux mujoco200\n","  %rm mujoco200_linux.zip"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n"]}]},{"cell_type":"code","metadata":{"id":"fQJfUoanr3pm","executionInfo":{"status":"ok","timestamp":1632579386623,"user_tz":-180,"elapsed":7381,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["#@title Important: ACTION Required BEFORE running this cell\n","#@markdown Place the mujoco key we have given you into a text file called mjkey.txt \n","#@markdown and ensure that the mujoco key is in the Google Drive path `cs330_fall2020/mujoco`.\n","\n","import os\n","\n","os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n","\n","## installation on colab does not find *.so files\n","## in LD_LIBRARY_PATH, copy over manually instead\n","!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDYMDKI8hrHF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579428667,"user_tz":-180,"elapsed":42061,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"07ae6a71-18ca-422e-c495-a15888e5ff56"},"source":["#@title Important system updates for mujoco-py\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg > /dev/null 2>&1"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [67.9 kB]\n","Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,335 kB]\n","Hit:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n","Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [581 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,770 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [613 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n","Fetched 13.0 MB in 6s (2,103 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","63 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"]}]},{"cell_type":"code","metadata":{"id":"tcG4cdIysCu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579484723,"user_tz":-180,"elapsed":56068,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"9e2f85e4-9038-42f8-b374-6c9b9640b83f"},"source":["#@title Clone and install mujoco-py\n","#@markdown Remember that you need to put the key in the appropriate location as described above\n","%cd $MJC_PATH\n","if not os.path.exists('mujoco-py'):\n","  !git clone https://github.com/openai/mujoco-py.git\n","%cd mujoco-py\n","%pip install -e .\n","\n","## cythonize at the first import\n","import mujoco_py"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n","/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n","Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py.\u001b[0m\n","\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (1.14.6)\n","Collecting glfw>=1.4.0\n","  Using cached glfw-2.2.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (204 kB)\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (1.19.5)\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (0.29.24)\n","Collecting fasteners~=0.15\n","  Using cached fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.1.2)\n","Installing collected packages: glfw, fasteners, mujoco-py\n","  Running setup.py develop for mujoco-py\n","Successfully installed fasteners-0.16.3 glfw-2.2.0 mujoco-py-2.0.2.13\n"]}]},{"cell_type":"code","metadata":{"id":"AGGFqjdcsX3g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579506919,"user_tz":-180,"elapsed":22209,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"825a95dd-845c-437a-d3a9-7f032640f379"},"source":["#@title Clone and install multiworld\n","%cd $SYM_PATH\n","!git clone https://github.com/vitchyr/multiworld.git\n","\n","%cd multiworld\n","%pip install -e .\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs330_fall2020\n","fatal: destination path 'multiworld' already exists and is not an empty directory.\n","/content/gdrive/My Drive/cs330_fall2020/multiworld\n","Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/multiworld\n","Installing collected packages: multiworld\n","  Running setup.py develop for multiworld\n","Successfully installed multiworld-0.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"0q6Swy46pzyg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579506921,"user_tz":-180,"elapsed":28,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"7e0aeec9-75a0-469e-cf93-c48e9c48bd94"},"source":["#@title Sets up virtual display\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fb6b4ac5f10>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"2Zc9_-pRitwk","executionInfo":{"status":"ok","timestamp":1632579508644,"user_tz":-180,"elapsed":1736,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["#@title Check imports and add helper functions for display\n","\n","import os\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) # error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3uYa0KHs0LB","colab":{"base_uri":"https://localhost:8080/","height":438},"executionInfo":{"status":"ok","timestamp":1632579511664,"user_tz":-180,"elapsed":3023,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"69903351-991a-49d3-ac28-3e00510c0a3c"},"source":["#@title After running, you should see a video play\n","matplotlib.use('Agg')\n","\n","env = wrap_env(gym.make(\"Ant-v2\"))\n","\n","observation = env.reset()\n","for i in range(10):\n","    env.render(mode='rgb_array')\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading video...\n"]},{"output_type":"display_data","data":{"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"v6uncjbtGb_s","executionInfo":{"status":"ok","timestamp":1632579511665,"user_tz":-180,"elapsed":10,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["#@title Random seed is set to be fixed\n","import tensorflow\n","tensorflow.random.set_seed(330)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mD5hrg3cPn7"},"source":["# BitFlip Goal Conditioned RL\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpGZxJIg9ghR"},"source":["## BitFlipEnv\n","\n","Familiarize yourself with what the bit flip environment does and what each method does.\n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"kKw1cs-ZiAjt"},"source":["class BitFlipEnv():\n","    '''bit flipping environment for reinforcement learning.\n","    The environment is a 1D vector of binary values (state vector).\n","    At each step, the actor can flip a single bit (0 to 1 or 1 to 0).\n","    The goal is to flip bits until the state vector matches the\n","    goal vector (also a 1D vector of binary values). At each step,\n","    the actor receives a goal of 0 if the state and goal vector\n","    do not match and a reward of 1 if the state and goal vector\n","    match.\n","\n","    Internally the state and goal vector are a numpy array, which\n","    allows the vectors to be printed by the show_goal and show_state\n","    methods. When '''\n","\n","    def __init__(self, num_bits, verbose = False):\n","        '''Initialize new instance of BitFlip class.\n","        inputs: num_bits - number of bits in the environment; must\n","                be an integer\n","                verbose - prints state and goal vector after each\n","                          step if True'''\n","\n","        # check that num_bits is a positive integer\n","        if (num_bits < 0) or (type(num_bits) != type(0)):\n","            print(\"Invalid number of bits -  must be positive integer\")\n","            return\n","\n","        # number of bits in the environment\n","        self.num_bits = num_bits\n","        # randomly set the state vector\n","        self.state_vector = np.random.randint(0, 2, num_bits)\n","        # randomly set the goal vector\n","        self.goal_vector = np.random.randint(0, 2, num_bits)\n","        # whether to print debugging info\n","        self.verbose = verbose\n","        # TODO set dimensions of observation space\n","        self.observation_space = self.state_vector\n","        # TODO create action space; may use gym type\n","        self.action_space = num_bits\n","        # space of the goal vector\n","        self.goal_space = self.goal_vector\n","        # number of steps taken\n","        self.steps = 0\n","\n","        return\n","\n","    def show_goal(self):\n","        '''Returns the goal as a numpy array. Used for debugging.'''\n","        return self.goal_vector\n","\n","    def show_state(self):\n","        '''Returns the state as a numpy array. Used for debugging.'''\n","        return self.state_vector\n","\n","    def reset(self):\n","        '''resets the environment. Returns a reset state_vector\n","        and goal_vector as tf tensors'''\n","\n","        # randomly reset both the state and the goal vectors\n","        self.state_vector = np.random.randint(0, 2, self.num_bits)\n","        self.goal_vector = np.random.randint(0, 2, self.num_bits)\n","        self.steps = 0\n","\n","        # return as np array\n","        return self.state_vector, self.goal_vector\n","\n","\n","    def step(self, action):\n","        '''take a step and flip one of the bits.\n","\n","        inputs: action - integer index of the bit to flip\n","        outputs: state - new state_vector (tensor)\n","                 reward - 0 if state != goal and 1 if state == goal\n","                 done - boolean value indicating if the goal has been reached'''\n","        self.steps += 1\n","\n","\n","        if action < 0 or action >= self.num_bits:\n","            # check argument is in range\n","            print(\"Invalid action! Must be integer ranging from \\\n","                0 to num_bits-1\")\n","            return\n","\n","        # flip the bit with index action\n","        if self.state_vector[action] == 1:\n","            self.state_vector[action] = 0\n","        else:\n","            self.state_vector[action] = 1\n","\n","        # initial values of reward and done - may change\n","        # depending on state and goal vectors\n","        reward = 0\n","        done = True\n","\n","        # check if state and goal vectors are identical\n","        if False in (self.state_vector == self.goal_vector):\n","            reward = -1\n","            done = False\n","\n","        # print additional info if verbose mode is on\n","        if self.verbose:\n","            print(\"Bit flipped:   \", action)\n","            print(\"Goal vector:   \", self.goal_vector)\n","            print(\"Updated state: \", self.state_vector)\n","            print(\"Reward:        \", reward)\n","\n","        if done:\n","            #print(\"Solved in: \", self.steps)\n","            pass\n","\n","        # return state as numpy arrays\n","        # return goal_vector in info field\n","        return np.copy(self.state_vector), reward, done, self.steps\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZOi6L7Pdgd0"},"source":["## Buffer\n","Familiarize yourself with what the buffer does \n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"J14LUXqwkC9e"},"source":["import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPtYOkhghoz-"},"source":["## BitFlip Goal Condition RL and Training\n","\n","Implement the changes you need for Problems 1-3 here in the cells below."]},{"cell_type":"code","metadata":{"id":"jgoB2zVqi2G8"},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_bits):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_bits,activation = None)\n","\n","  def call(self, inputs):\n","\n","    x = self.dense1(inputs)\n","    return self.out(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXD2QYkHh56I"},"source":["# ************   Helper functions    ************ #\n","\n","def updateTarget(model, target_model, tau=0.95) :\n","    model_weights = model.get_weights()\n","    target_weights = target_model.get_weights()\n","    new_weights = []\n","    for i, weight in enumerate(model_weights):\n","      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n","\n","    target_model.set_weights(new_weights)\n","\n","def solve_environment(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit flipping environment using the current policy\n","\n","    inputs: num_bits - number of bits to be looped over  # size of an episode\n","        model - DQN to run prediction on\n","        bit_env - environment for bitflip\n","        state - current state\n","        goal_state - desired state\n","        total_reward - cumulative reward so far\n","    '''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","      \n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the size of the vector\n","      \n","      # ======================== TODO modify code ========================\n","      inputs = np.expand_dims(state, axis=0)\n","      # forward pass to find action\n","      out = model(inputs)\n","      action = np.argmax(out ,axis=1)[0]  # action as an integer\n","      # add to the episode experience (what happened)\n","      next_state, reward, done, steps = bit_env.step(action)\n","      ### NEW UPDATE ######\n","      next_state = np.concatenate([next_state, goal_state[:num_bits]])\n","      ### NEW UPDATE ######\n","      # calculate total reward\n","      episode_experience.append((state, action, reward, next_state, goal_state))\n","      total_reward += reward\n","      # update state\n","      state = next_state\n","      # mark that we've finished the episode and succeeded with training\n","      if done:\n","          succeeded = True\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","def solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit_flip env using no goal'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","        # attempt to solve the state - number of steps given to solve the\n","        # state is equal to the passed argument steps_per_episode.\n","\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        # forward pass to find action\n","        out = model(inputs)\n","        action = np.argmax(out,axis = 1)\n","        # take the action\n","        next_state,reward,done, _ = bit_env.step(action)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward += reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def create_her_states_reward(experience, idx, s, s_, num_bits):\n","    '''\n","    given an (episode) experience and an index, we set a new goal (the state at time \"index\") \n","    and create a new state, next state and reward in accordance to that goal\n","\n","    inputs: experience - list of transitions from the last episode\n","            idx - the index of the goal of the new task\n","            s - current state\n","            s_ - next state\n","            num_bits - size of new goal\n","    outputs: a tri-tuple of state, reward and next state \n","    '''\n","    _, _, _, s_fin, _ = experience[idx]  \n","    goal_state = create_her_state(s_fin, s_fin, num_bits)  # set new goal to final state in episode\n","    state = create_her_state(s, s_fin, num_bits)  # update state (goal-conditioned with new goal)\n","    next_state = create_her_state(s_, s_fin, num_bits)  # update next state (goal-conditioned with new goal)\n","    r_ = set_her_reward(next_state, goal_state)\n","    return state, r_, next_state\n","\n","def create_her_state(state, goal_state, num_bits):\n","    her_state = np.copy(state)\n","    her_state[num_bits:] = goal_state[:num_bits]  # set new goal state\n","    return her_state\n","\n","def set_her_reward(next_state, goal_state):\n","    r = -1  \n","    if np.equal(next_state, goal_state).all(): # update r if reached new goal\n","        r = 0   \n","    return r\n","\n","\n","def update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, relabeled experiences are also added to the replay buffer\n","\n","    inputs: num_bits - number of bits to be looped over \n","            replay_buffer - the buffer to store past experience in\n","            episode_experience - list of transitions from the last episode\n","            HER -  type of hindsight experience replay to use\n","    modifies: replay_buffer\n","    outputs: None'''\n","\n","    for t in range(num_bits) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = s\n","        # next state\n","        inputs_ = s_\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)   # (inputs, a, r, inputs_)\n","\n","        # when HER is used, each call to update_replay_buffer should add num_relabeled\n","        # relabeled points to the replay buffer\n","\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            continue  # just add to replay buffer\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            state, r_, next_state = create_her_states_reward(episode_experience, -1, s, s_, num_bits)\n","            replay_buffer.add(state,a,r_, next_state)   # (inputs, a, r, inputs_)\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            num_goals = min(num_bits - t , num_relabeled)  # create a legal number of goals per step\n","            g_idxes = random.sample(range(t, num_bits), num_goals)\n","            for g_idx in g_idxes:\n","                state, r_, next_state = create_her_states_reward(episode_experience, g_idx, s, s_, num_bits)\n","                replay_buffer.add(state, a, r_, next_state)\n","\n","        elif HER == 'random':\n","             # random - relabel based on a random state in the episode\n","            num_goals = min(num_bits , num_relabeled)  # create a legal number of goals per step\n","            g_idxes = random.sample(range(num_bits), num_goals)\n","            for g_idx in g_idxes:\n","                state, r_, next_state = create_her_states_reward(episode_experience, g_idx, s, s_, num_bits)\n","                replay_buffer.add(state, a, r_, next_state)\n","        # ========================      END TODO       ========================\n","\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL4Cedzih-eg"},"source":["\n","# ************   Main training loop    ************ #\n","\n","\n","def flip_bits(num_bits, num_epochs, buffer_size = 1e6, batch_size = 128, \n","              num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40, HER = \"None\"):\n","    '''Main loop for running in the bit flipping environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","\n","    print(\"Running bit flip environment with %d bits and HER policy: %s\" %(num_bits, HER))\n","\n","    # create bit flipping environment and replay buffer\n","    bit_env = BitFlipEnv(num_bits)\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(num_bits)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","    target_model = Model(num_bits)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    state, s = bit_env.reset()\n","    ########  IF HER\n","    state = np.concatenate([state, goal_state])  # state: (n,) -> (2*n,)\n","    goal_state = np.concatenate([goal_state, goal_state])  # goal_state: (n,) -> (2*n,)\n","    inputs = state\n","    inputs = np.expand_dims(inputs, axis=0)  # (1, 2 * n)\n","    model(inputs)\n","    target_model(inputs)\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model, tau=0.0)\n","    # ========================      END TODO       ========================\n","\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            # Run in the environment for num_episodes  \n","            state, goal_state = bit_env.reset()             # reset the environment     \n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            state = np.concatenate([state, goal_state])\n","            goal_state = np.concatenate([goal_state, goal_state])\n","            # succeeded, episode_experience, total_reward = solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward)\n","            succeeded, episode_experience, total_reward = solve_environment(num_bits, model, bit_env, state, goal_state, total_reward)\n","            # ========================     END TODO     ========================\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            # forward pass through target network   \n","            # target_net_Q = sess.run(target_model.out,feed_dict = {target_model.inp : next_state})\n","            with tf.GradientTape() as tape:\n","              # target_input = np.expand_dims(next_state, axis=0)\n","              # target_net_Q = target_model(target_input)\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate predictions and loss\n","            #   inputs = np.expand_dims(state, axis=0)\n","            #   model_predict = target_model(inputs)\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, num_bits)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","                \n","    return success_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TTzkATl3g22","executionInfo":{"status":"ok","timestamp":1632489253573,"user_tz":-180,"elapsed":47637,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"d6ec77d1-af4b-4e06-94b1-3861d411f60e"},"source":["success_rate  = flip_bits(num_bits=7, num_epochs=150, HER='None') "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running bit flip environment with 7 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1603\n","Epoch: 5  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 10  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 15  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 20  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0048\n","Epoch: 25  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 30  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0066\n","Epoch: 35  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0048\n","Epoch: 40  Cumulative reward: -108.000000  Success rate: 0.0625 Mean loss: 0.0072\n","Epoch: 45  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0063\n","Epoch: 50  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0070\n","Epoch: 55  Cumulative reward: -101.000000  Success rate: 0.1875 Mean loss: 0.0075\n","Epoch: 60  Cumulative reward: -106.000000  Success rate: 0.1250 Mean loss: 0.0077\n","Epoch: 65  Cumulative reward: -108.000000  Success rate: 0.0625 Mean loss: 0.0078\n","Epoch: 70  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 75  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 80  Cumulative reward: -104.000000  Success rate: 0.2500 Mean loss: 0.0091\n","Epoch: 85  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 90  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0107\n","Epoch: 95  Cumulative reward: -107.000000  Success rate: 0.1250 Mean loss: 0.0108\n","Epoch: 100  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0106\n","Epoch: 105  Cumulative reward: -98.000000  Success rate: 0.3125 Mean loss: 0.0119\n","Epoch: 110  Cumulative reward: -105.000000  Success rate: 0.1250 Mean loss: 0.0120\n","Epoch: 115  Cumulative reward: -107.000000  Success rate: 0.1875 Mean loss: 0.0128\n","Epoch: 120  Cumulative reward: -102.000000  Success rate: 0.2500 Mean loss: 0.0151\n","Epoch: 125  Cumulative reward: -107.000000  Success rate: 0.1250 Mean loss: 0.0149\n","Epoch: 130  Cumulative reward: -94.000000  Success rate: 0.4375 Mean loss: 0.0169\n","Epoch: 135  Cumulative reward: -90.000000  Success rate: 0.5625 Mean loss: 0.0184\n","Epoch: 140  Cumulative reward: -89.000000  Success rate: 0.5625 Mean loss: 0.0167\n","Epoch: 145  Cumulative reward: -82.000000  Success rate: 0.8125 Mean loss: 0.0226\n"]}]},{"cell_type":"code","metadata":{"id":"l-l99ooh8nMR"},"source":["from IPython.display import HTML\n","from plotly import graph_objs as go\n","def plot_success(exp_to_accuracies, text):\n","    # Creates the Figure\n","    fig = go.Figure()\n","    data = []\n","    for experiment, accuracies in exp_to_accuracies.items():\n","        steps = range(len(accuracies))\n","        steps = [5 * x for x in steps]\n","        data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n","\n","    # Applies a custom layout\n","    layout = go.Layout(\n","        title=go.layout.Title(\n","            text=text,\n","            x=0.5\n","        ),\n","        xaxis=go.layout.XAxis(\n","            title=go.layout.xaxis.Title(\n","                text='Epoch',\n","                font=dict(\n","                    family='Courier New, monospace',\n","                    size=18,\n","                    color='#7f7f7f'\n","                )\n","            )\n","        ),\n","        yaxis=go.layout.YAxis(\n","            title=go.layout.yaxis.Title(\n","                text='Success Rate',\n","                font=dict(\n","                    family='Courier New, monospace',\n","                    size=18,\n","                    color='#7f7f7f'\n","                )\n","            )\n","        )\n","    )\n","\n","    fig = go.Figure(data=data, layout=layout)\n","    return fig\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjsBHXyj8VCd","executionInfo":{"status":"ok","timestamp":1632489297463,"user_tz":-180,"elapsed":259,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"bef1b1e3-8cbb-4d75-a974-d70e8cd54026"},"source":["type(success_rate), len(success_rate)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, 150)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"uaqw28hxi6Zi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632489593627,"user_tz":-180,"elapsed":157916,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"0011b590-7cff-40c4-be06-74a106082bb2"},"source":["# Sample commands have been provided to you below\n","# run with type of HER specified\n","success_rate  = flip_bits(num_bits=7, num_epochs=250, HER='None') \n","success_rate_final = flip_bits(num_bits=7, num_epochs=250, HER='final')\n","# pass success rate for each run as first argument and labels as second list"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running bit flip environment with 7 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1779\n","Epoch: 5  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 10  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 15  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0042\n","Epoch: 20  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 25  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 30  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 35  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0027\n","Epoch: 40  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 45  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0042\n","Epoch: 50  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0045\n","Epoch: 55  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0060\n","Epoch: 60  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0062\n","Epoch: 65  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0061\n","Epoch: 70  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0067\n","Epoch: 75  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0075\n","Epoch: 80  Cumulative reward: -111.000000  Success rate: 0.0625 Mean loss: 0.0071\n","Epoch: 85  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 90  Cumulative reward: -106.000000  Success rate: 0.1250 Mean loss: 0.0095\n","Epoch: 95  Cumulative reward: -102.000000  Success rate: 0.1875 Mean loss: 0.0104\n","Epoch: 100  Cumulative reward: -104.000000  Success rate: 0.1875 Mean loss: 0.0109\n","Epoch: 105  Cumulative reward: -109.000000  Success rate: 0.1250 Mean loss: 0.0124\n","Epoch: 110  Cumulative reward: -105.000000  Success rate: 0.1250 Mean loss: 0.0134\n","Epoch: 115  Cumulative reward: -97.000000  Success rate: 0.3750 Mean loss: 0.0141\n","Epoch: 120  Cumulative reward: -99.000000  Success rate: 0.3125 Mean loss: 0.0147\n","Epoch: 125  Cumulative reward: -96.000000  Success rate: 0.3750 Mean loss: 0.0175\n","Epoch: 130  Cumulative reward: -100.000000  Success rate: 0.3125 Mean loss: 0.0179\n","Epoch: 135  Cumulative reward: -102.000000  Success rate: 0.3750 Mean loss: 0.0206\n","Epoch: 140  Cumulative reward: -81.000000  Success rate: 0.6875 Mean loss: 0.0186\n","Epoch: 145  Cumulative reward: -93.000000  Success rate: 0.4375 Mean loss: 0.0191\n","Epoch: 150  Cumulative reward: -92.000000  Success rate: 0.4375 Mean loss: 0.0210\n","Epoch: 155  Cumulative reward: -83.000000  Success rate: 0.6875 Mean loss: 0.0206\n","Epoch: 160  Cumulative reward: -88.000000  Success rate: 0.7500 Mean loss: 0.0191\n","Epoch: 165  Cumulative reward: -81.000000  Success rate: 0.7500 Mean loss: 0.0174\n","Epoch: 170  Cumulative reward: -79.000000  Success rate: 0.8125 Mean loss: 0.0200\n","Epoch: 175  Cumulative reward: -79.000000  Success rate: 0.8750 Mean loss: 0.0165\n","Epoch: 180  Cumulative reward: -83.000000  Success rate: 0.6875 Mean loss: 0.0159\n","Epoch: 185  Cumulative reward: -80.000000  Success rate: 0.7500 Mean loss: 0.0142\n","Epoch: 190  Cumulative reward: -86.000000  Success rate: 0.6250 Mean loss: 0.0138\n","Epoch: 195  Cumulative reward: -82.000000  Success rate: 0.6875 Mean loss: 0.0132\n","Epoch: 200  Cumulative reward: -91.000000  Success rate: 0.6250 Mean loss: 0.0120\n","Epoch: 205  Cumulative reward: -74.000000  Success rate: 0.9375 Mean loss: 0.0121\n","Epoch: 210  Cumulative reward: -91.000000  Success rate: 0.5000 Mean loss: 0.0108\n","Epoch: 215  Cumulative reward: -80.000000  Success rate: 0.8125 Mean loss: 0.0104\n","Epoch: 220  Cumulative reward: -84.000000  Success rate: 0.7500 Mean loss: 0.0110\n","Epoch: 225  Cumulative reward: -75.000000  Success rate: 0.9375 Mean loss: 0.0099\n","Epoch: 230  Cumulative reward: -81.000000  Success rate: 0.7500 Mean loss: 0.0101\n","Epoch: 235  Cumulative reward: -77.000000  Success rate: 0.8750 Mean loss: 0.0090\n","Epoch: 240  Cumulative reward: -80.000000  Success rate: 0.8125 Mean loss: 0.0095\n","Epoch: 245  Cumulative reward: -83.000000  Success rate: 0.8125 Mean loss: 0.0086\n","Running bit flip environment with 7 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1992\n","Epoch: 5  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0865\n","Epoch: 10  Cumulative reward: -98.000000  Success rate: 0.3125 Mean loss: 0.0681\n","Epoch: 15  Cumulative reward: -87.000000  Success rate: 0.6250 Mean loss: 0.0560\n","Epoch: 20  Cumulative reward: -87.000000  Success rate: 0.6250 Mean loss: 0.0396\n","Epoch: 25  Cumulative reward: -73.000000  Success rate: 0.9375 Mean loss: 0.0339\n","Epoch: 30  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0229\n","Epoch: 35  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0151\n","Epoch: 40  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0103\n","Epoch: 45  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 50  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 55  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0047\n","Epoch: 60  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0050\n","Epoch: 65  Cumulative reward: -74.000000  Success rate: 0.9375 Mean loss: 0.0043\n","Epoch: 70  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0041\n","Epoch: 75  Cumulative reward: -75.000000  Success rate: 1.0000 Mean loss: 0.0039\n","Epoch: 80  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0036\n","Epoch: 85  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0033\n","Epoch: 90  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0029\n","Epoch: 95  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0033\n","Epoch: 100  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0026\n","Epoch: 105  Cumulative reward: -70.000000  Success rate: 1.0000 Mean loss: 0.0029\n","Epoch: 110  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0029\n","Epoch: 115  Cumulative reward: -68.000000  Success rate: 1.0000 Mean loss: 0.0028\n","Epoch: 120  Cumulative reward: -75.000000  Success rate: 1.0000 Mean loss: 0.0024\n","Epoch: 125  Cumulative reward: -75.000000  Success rate: 1.0000 Mean loss: 0.0024\n","Epoch: 130  Cumulative reward: -65.000000  Success rate: 1.0000 Mean loss: 0.0023\n","Epoch: 135  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0022\n","Epoch: 140  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0019\n","Epoch: 145  Cumulative reward: -70.000000  Success rate: 1.0000 Mean loss: 0.0020\n","Epoch: 150  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0021\n","Epoch: 155  Cumulative reward: -67.000000  Success rate: 1.0000 Mean loss: 0.0021\n","Epoch: 160  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0017\n","Epoch: 165  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0018\n","Epoch: 170  Cumulative reward: -76.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 175  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0016\n","Epoch: 180  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0018\n","Epoch: 185  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 190  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0014\n","Epoch: 195  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 200  Cumulative reward: -68.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 205  Cumulative reward: -70.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 210  Cumulative reward: -78.000000  Success rate: 1.0000 Mean loss: 0.0013\n","Epoch: 215  Cumulative reward: -68.000000  Success rate: 1.0000 Mean loss: 0.0015\n","Epoch: 220  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0012\n","Epoch: 225  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0016\n","Epoch: 230  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0011\n","Epoch: 235  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0013\n","Epoch: 240  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0013\n","Epoch: 245  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0010\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"1kC5fcNyG3zz8wD8TIBQK9KSd3VAB_cmu"},"id":"Q7X-wW688xu0","executionInfo":{"status":"ok","timestamp":1632490434999,"user_tz":-180,"elapsed":1919,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"e794d273-1126-487a-f3f5-9d17577e1a11"},"source":["exp_to_accuracies = {\n","    \"bitflip_7_her_none\": success_rate,\n","    \"bitflip_7_her_final\": success_rate_final\n","}\n","text = 'Bitflip with 7 bits'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wkrxwimi18Gj","executionInfo":{"status":"ok","timestamp":1632490889138,"user_tz":-180,"elapsed":426030,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"f817b1a2-3ad1-4196-8d43-9d8f10e241d6"},"source":["success_rate = flip_bits(num_bits=15, num_epochs=500, HER='None')\n","success_rate_final = flip_bits(num_bits=15, num_epochs=500, HER='final')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running bit flip environment with 15 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1258\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0010\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0010\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0010\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 70  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 80  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 85  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 90  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 100  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 105  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 110  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 115  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 120  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 125  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 130  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 135  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0018\n","Epoch: 140  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 145  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 150  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 155  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 160  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 165  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 170  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 175  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 180  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 185  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 190  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 195  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 200  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 205  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 210  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 215  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 220  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 225  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 230  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 235  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 240  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 245  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 250  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 255  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 260  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 265  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 270  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 275  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 280  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 285  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 290  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 295  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 300  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 305  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 310  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 315  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 320  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 325  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 330  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 335  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 340  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Epoch: 345  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 350  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 355  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Epoch: 360  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 365  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 370  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 375  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 380  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 385  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 390  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 395  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 400  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 405  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 410  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 415  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 420  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 425  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 430  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 435  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 440  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 445  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 450  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 455  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 460  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0055\n","Epoch: 465  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 470  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 475  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 480  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 485  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 490  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 495  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Running bit flip environment with 15 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1317\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0665\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0753\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0688\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0601\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0600\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0563\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0530\n","Epoch: 40  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.0519\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0500\n","Epoch: 50  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.0483\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0480\n","Epoch: 60  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0470\n","Epoch: 65  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0436\n","Epoch: 70  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0427\n","Epoch: 75  Cumulative reward: -210.000000  Success rate: 0.3750 Mean loss: 0.0414\n","Epoch: 80  Cumulative reward: -225.000000  Success rate: 0.2500 Mean loss: 0.0385\n","Epoch: 85  Cumulative reward: -203.000000  Success rate: 0.5000 Mean loss: 0.0360\n","Epoch: 90  Cumulative reward: -215.000000  Success rate: 0.3750 Mean loss: 0.0357\n","Epoch: 95  Cumulative reward: -175.000000  Success rate: 0.8125 Mean loss: 0.0326\n","Epoch: 100  Cumulative reward: -189.000000  Success rate: 0.6875 Mean loss: 0.0313\n","Epoch: 105  Cumulative reward: -204.000000  Success rate: 0.5000 Mean loss: 0.0299\n","Epoch: 110  Cumulative reward: -186.000000  Success rate: 0.7500 Mean loss: 0.0279\n","Epoch: 115  Cumulative reward: -197.000000  Success rate: 0.6250 Mean loss: 0.0268\n","Epoch: 120  Cumulative reward: -188.000000  Success rate: 0.7500 Mean loss: 0.0240\n","Epoch: 125  Cumulative reward: -184.000000  Success rate: 0.7500 Mean loss: 0.0227\n","Epoch: 130  Cumulative reward: -169.000000  Success rate: 0.9375 Mean loss: 0.0205\n","Epoch: 135  Cumulative reward: -169.000000  Success rate: 0.9375 Mean loss: 0.0200\n","Epoch: 140  Cumulative reward: -174.000000  Success rate: 0.8750 Mean loss: 0.0187\n","Epoch: 145  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0177\n","Epoch: 150  Cumulative reward: -183.000000  Success rate: 0.8125 Mean loss: 0.0184\n","Epoch: 155  Cumulative reward: -168.000000  Success rate: 0.9375 Mean loss: 0.0164\n","Epoch: 160  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0174\n","Epoch: 165  Cumulative reward: -183.000000  Success rate: 0.8125 Mean loss: 0.0156\n","Epoch: 170  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0151\n","Epoch: 175  Cumulative reward: -176.000000  Success rate: 0.9375 Mean loss: 0.0143\n","Epoch: 180  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0141\n","Epoch: 185  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0138\n","Epoch: 190  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0137\n","Epoch: 195  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0134\n","Epoch: 200  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0133\n","Epoch: 205  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0130\n","Epoch: 210  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0131\n","Epoch: 215  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0124\n","Epoch: 220  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0121\n","Epoch: 225  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0123\n","Epoch: 230  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0118\n","Epoch: 235  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0107\n","Epoch: 240  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0112\n","Epoch: 245  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 250  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0111\n","Epoch: 255  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0112\n","Epoch: 260  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0109\n","Epoch: 265  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0098\n","Epoch: 270  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0110\n","Epoch: 275  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0100\n","Epoch: 280  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0103\n","Epoch: 285  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0098\n","Epoch: 290  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0124\n","Epoch: 295  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0101\n","Epoch: 300  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 305  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0083\n","Epoch: 310  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 315  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0081\n","Epoch: 320  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0088\n","Epoch: 325  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0103\n","Epoch: 330  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0084\n","Epoch: 335  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0087\n","Epoch: 340  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0081\n","Epoch: 345  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0085\n","Epoch: 350  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0084\n","Epoch: 355  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 360  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 365  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0082\n","Epoch: 370  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 375  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0080\n","Epoch: 380  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0074\n","Epoch: 385  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0075\n","Epoch: 390  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0079\n","Epoch: 395  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0073\n","Epoch: 400  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0067\n","Epoch: 405  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 410  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0073\n","Epoch: 415  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0063\n","Epoch: 420  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0062\n","Epoch: 425  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0062\n","Epoch: 430  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 435  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0076\n","Epoch: 440  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0063\n","Epoch: 445  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 450  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0061\n","Epoch: 455  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0060\n","Epoch: 460  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0072\n","Epoch: 465  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0063\n","Epoch: 470  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 475  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0061\n","Epoch: 480  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 485  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 490  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 495  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0066\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"17vduDDDcwrBSK3GxVAqkUwjGKfaE2TIU"},"id":"x_GY1i6-9h-L","executionInfo":{"status":"ok","timestamp":1632490899560,"user_tz":-180,"elapsed":1834,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"6f7ad231-786e-4647-c301-61da003daca0"},"source":["exp_to_accuracies = {\n","    \"bitflip_15_her_none\": success_rate,\n","    \"bitflip_15_her_final\": success_rate_final\n","}\n","text = 'Bitflip with 15 bits'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MzYCB7Q20p9","executionInfo":{"status":"ok","timestamp":1632493629767,"user_tz":-180,"elapsed":1164883,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"ea42197b-cfdc-46a9-de88-c937d85a3f41"},"source":["success_rate = flip_bits(num_bits=25, num_epochs=1000, HER='None')\n","success_rate_final = flip_bits(num_bits=25, num_epochs=1000, HER='final')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running bit flip environment with 25 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.1010\n","Epoch: 5  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0062\n","Epoch: 10  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 15  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 20  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 25  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 30  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 35  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 40  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 45  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 50  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 55  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 60  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 65  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 70  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 75  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 80  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 85  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 90  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 95  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 100  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 105  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 110  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 115  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 120  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 125  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 130  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 135  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 140  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 145  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 150  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 155  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 160  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 165  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 170  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 175  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 180  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 185  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 190  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 195  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 200  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 205  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 210  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 215  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 220  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 225  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 230  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 235  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 240  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 245  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 250  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 255  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 260  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0059\n","Epoch: 265  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 270  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 275  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 280  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0086\n","Epoch: 285  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 290  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 295  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0060\n","Epoch: 300  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 305  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 310  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Epoch: 315  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 320  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 325  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 330  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 335  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 340  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 345  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 350  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 355  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 360  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 365  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 370  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Epoch: 375  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0059\n","Epoch: 380  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0055\n","Epoch: 385  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0062\n","Epoch: 390  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 395  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 400  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 405  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 410  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0056\n","Epoch: 415  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Epoch: 420  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0063\n","Epoch: 425  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 430  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0073\n","Epoch: 435  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0060\n","Epoch: 440  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 445  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 450  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 455  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 460  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 465  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 470  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 475  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 480  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 485  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0064\n","Epoch: 490  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 495  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 500  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0076\n","Epoch: 505  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 510  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 515  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 520  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 525  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 530  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 535  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 540  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0080\n","Epoch: 545  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 550  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0061\n","Epoch: 555  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0073\n","Epoch: 560  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0082\n","Epoch: 565  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 570  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0063\n","Epoch: 575  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0079\n","Epoch: 580  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0107\n","Epoch: 585  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 590  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 595  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 600  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0088\n","Epoch: 605  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 610  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 615  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 620  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 625  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 630  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0129\n","Epoch: 635  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0091\n","Epoch: 640  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 645  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 650  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0119\n","Epoch: 655  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 660  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0079\n","Epoch: 665  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0067\n","Epoch: 670  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 675  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 680  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0084\n","Epoch: 685  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0082\n","Epoch: 690  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0088\n","Epoch: 695  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0064\n","Epoch: 700  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 705  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0081\n","Epoch: 710  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 715  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 720  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 725  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0094\n","Epoch: 730  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0119\n","Epoch: 735  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 740  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0099\n","Epoch: 745  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 750  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0102\n","Epoch: 755  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 760  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 765  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0093\n","Epoch: 770  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 775  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 780  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0119\n","Epoch: 785  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0086\n","Epoch: 790  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 795  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0149\n","Epoch: 800  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0084\n","Epoch: 805  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 810  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 815  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 820  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 825  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 830  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 835  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 840  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0088\n","Epoch: 845  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 850  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 855  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 860  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 865  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0096\n","Epoch: 870  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0140\n","Epoch: 875  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 880  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0091\n","Epoch: 885  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0113\n","Epoch: 890  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 895  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 900  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0116\n","Epoch: 905  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 910  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0112\n","Epoch: 915  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 920  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 925  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 930  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0082\n","Epoch: 935  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0107\n","Epoch: 940  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 945  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 950  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0108\n","Epoch: 955  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0111\n","Epoch: 960  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0100\n","Epoch: 965  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 970  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 975  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0100\n","Epoch: 980  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 985  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0116\n","Epoch: 990  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0102\n","Epoch: 995  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Running bit flip environment with 25 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0977\n","Epoch: 5  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0200\n","Epoch: 10  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0320\n","Epoch: 15  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0474\n","Epoch: 20  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0616\n","Epoch: 25  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0670\n","Epoch: 30  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0644\n","Epoch: 35  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0598\n","Epoch: 40  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0590\n","Epoch: 45  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0568\n","Epoch: 50  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0531\n","Epoch: 55  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0549\n","Epoch: 60  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0556\n","Epoch: 65  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0569\n","Epoch: 70  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0581\n","Epoch: 75  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0575\n","Epoch: 80  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0601\n","Epoch: 85  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0551\n","Epoch: 90  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0541\n","Epoch: 95  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0534\n","Epoch: 100  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0526\n","Epoch: 105  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0523\n","Epoch: 110  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0516\n","Epoch: 115  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0510\n","Epoch: 120  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0479\n","Epoch: 125  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0518\n","Epoch: 130  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0500\n","Epoch: 135  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0461\n","Epoch: 140  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0483\n","Epoch: 145  Cumulative reward: -391.000000  Success rate: 0.0625 Mean loss: 0.0457\n","Epoch: 150  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0466\n","Epoch: 155  Cumulative reward: -391.000000  Success rate: 0.0625 Mean loss: 0.0456\n","Epoch: 160  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0435\n","Epoch: 165  Cumulative reward: -390.000000  Success rate: 0.0625 Mean loss: 0.0415\n","Epoch: 170  Cumulative reward: -392.000000  Success rate: 0.0625 Mean loss: 0.0418\n","Epoch: 175  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0432\n","Epoch: 180  Cumulative reward: -391.000000  Success rate: 0.0625 Mean loss: 0.0435\n","Epoch: 185  Cumulative reward: -394.000000  Success rate: 0.0625 Mean loss: 0.0420\n","Epoch: 190  Cumulative reward: -385.000000  Success rate: 0.1250 Mean loss: 0.0402\n","Epoch: 195  Cumulative reward: -385.000000  Success rate: 0.1250 Mean loss: 0.0407\n","Epoch: 200  Cumulative reward: -392.000000  Success rate: 0.0625 Mean loss: 0.0404\n","Epoch: 205  Cumulative reward: -391.000000  Success rate: 0.0625 Mean loss: 0.0409\n","Epoch: 210  Cumulative reward: -394.000000  Success rate: 0.0625 Mean loss: 0.0400\n","Epoch: 215  Cumulative reward: -386.000000  Success rate: 0.1250 Mean loss: 0.0390\n","Epoch: 220  Cumulative reward: -387.000000  Success rate: 0.1250 Mean loss: 0.0385\n","Epoch: 225  Cumulative reward: -368.000000  Success rate: 0.2500 Mean loss: 0.0385\n","Epoch: 230  Cumulative reward: -368.000000  Success rate: 0.2500 Mean loss: 0.0376\n","Epoch: 235  Cumulative reward: -371.000000  Success rate: 0.2500 Mean loss: 0.0378\n","Epoch: 240  Cumulative reward: -383.000000  Success rate: 0.1250 Mean loss: 0.0370\n","Epoch: 245  Cumulative reward: -362.000000  Success rate: 0.3125 Mean loss: 0.0356\n","Epoch: 250  Cumulative reward: -354.000000  Success rate: 0.3750 Mean loss: 0.0364\n","Epoch: 255  Cumulative reward: -348.000000  Success rate: 0.4375 Mean loss: 0.0341\n","Epoch: 260  Cumulative reward: -349.000000  Success rate: 0.4375 Mean loss: 0.0357\n","Epoch: 265  Cumulative reward: -364.000000  Success rate: 0.3125 Mean loss: 0.0366\n","Epoch: 270  Cumulative reward: -371.000000  Success rate: 0.2500 Mean loss: 0.0351\n","Epoch: 275  Cumulative reward: -343.000000  Success rate: 0.5000 Mean loss: 0.0355\n","Epoch: 280  Cumulative reward: -349.000000  Success rate: 0.4375 Mean loss: 0.0341\n","Epoch: 285  Cumulative reward: -341.000000  Success rate: 0.5000 Mean loss: 0.0359\n","Epoch: 290  Cumulative reward: -354.000000  Success rate: 0.3750 Mean loss: 0.0365\n","Epoch: 295  Cumulative reward: -346.000000  Success rate: 0.4375 Mean loss: 0.0355\n","Epoch: 300  Cumulative reward: -329.000000  Success rate: 0.6250 Mean loss: 0.0336\n","Epoch: 305  Cumulative reward: -342.000000  Success rate: 0.4375 Mean loss: 0.0342\n","Epoch: 310  Cumulative reward: -351.000000  Success rate: 0.3750 Mean loss: 0.0327\n","Epoch: 315  Cumulative reward: -363.000000  Success rate: 0.3125 Mean loss: 0.0324\n","Epoch: 320  Cumulative reward: -321.000000  Success rate: 0.6875 Mean loss: 0.0326\n","Epoch: 325  Cumulative reward: -331.000000  Success rate: 0.6250 Mean loss: 0.0322\n","Epoch: 330  Cumulative reward: -341.000000  Success rate: 0.5000 Mean loss: 0.0357\n","Epoch: 335  Cumulative reward: -327.000000  Success rate: 0.6875 Mean loss: 0.0353\n","Epoch: 340  Cumulative reward: -311.000000  Success rate: 0.7500 Mean loss: 0.0328\n","Epoch: 345  Cumulative reward: -338.000000  Success rate: 0.5000 Mean loss: 0.0360\n","Epoch: 350  Cumulative reward: -369.000000  Success rate: 0.3125 Mean loss: 0.0349\n","Epoch: 355  Cumulative reward: -347.000000  Success rate: 0.4375 Mean loss: 0.0347\n","Epoch: 360  Cumulative reward: -337.000000  Success rate: 0.5000 Mean loss: 0.0339\n","Epoch: 365  Cumulative reward: -353.000000  Success rate: 0.3750 Mean loss: 0.0316\n","Epoch: 370  Cumulative reward: -320.000000  Success rate: 0.6250 Mean loss: 0.0339\n","Epoch: 375  Cumulative reward: -304.000000  Success rate: 0.7500 Mean loss: 0.0309\n","Epoch: 380  Cumulative reward: -294.000000  Success rate: 0.8750 Mean loss: 0.0346\n","Epoch: 385  Cumulative reward: -344.000000  Success rate: 0.5000 Mean loss: 0.0328\n","Epoch: 390  Cumulative reward: -325.000000  Success rate: 0.6250 Mean loss: 0.0313\n","Epoch: 395  Cumulative reward: -307.000000  Success rate: 0.8125 Mean loss: 0.0334\n","Epoch: 400  Cumulative reward: -316.000000  Success rate: 0.6875 Mean loss: 0.0320\n","Epoch: 405  Cumulative reward: -352.000000  Success rate: 0.5000 Mean loss: 0.0288\n","Epoch: 410  Cumulative reward: -318.000000  Success rate: 0.6875 Mean loss: 0.0316\n","Epoch: 415  Cumulative reward: -344.000000  Success rate: 0.5000 Mean loss: 0.0294\n","Epoch: 420  Cumulative reward: -314.000000  Success rate: 0.7500 Mean loss: 0.0297\n","Epoch: 425  Cumulative reward: -289.000000  Success rate: 0.9375 Mean loss: 0.0298\n","Epoch: 430  Cumulative reward: -288.000000  Success rate: 0.9375 Mean loss: 0.0333\n","Epoch: 435  Cumulative reward: -300.000000  Success rate: 0.8125 Mean loss: 0.0314\n","Epoch: 440  Cumulative reward: -324.000000  Success rate: 0.6875 Mean loss: 0.0315\n","Epoch: 445  Cumulative reward: -295.000000  Success rate: 0.8750 Mean loss: 0.0302\n","Epoch: 450  Cumulative reward: -324.000000  Success rate: 0.6250 Mean loss: 0.0297\n","Epoch: 455  Cumulative reward: -302.000000  Success rate: 0.8125 Mean loss: 0.0287\n","Epoch: 460  Cumulative reward: -292.000000  Success rate: 0.9375 Mean loss: 0.0295\n","Epoch: 465  Cumulative reward: -286.000000  Success rate: 0.9375 Mean loss: 0.0298\n","Epoch: 470  Cumulative reward: -313.000000  Success rate: 0.7500 Mean loss: 0.0290\n","Epoch: 475  Cumulative reward: -293.000000  Success rate: 0.9375 Mean loss: 0.0319\n","Epoch: 480  Cumulative reward: -316.000000  Success rate: 0.7500 Mean loss: 0.0288\n","Epoch: 485  Cumulative reward: -294.000000  Success rate: 0.8750 Mean loss: 0.0295\n","Epoch: 490  Cumulative reward: -301.000000  Success rate: 0.8750 Mean loss: 0.0295\n","Epoch: 495  Cumulative reward: -295.000000  Success rate: 0.8750 Mean loss: 0.0287\n","Epoch: 500  Cumulative reward: -304.000000  Success rate: 0.8750 Mean loss: 0.0290\n","Epoch: 505  Cumulative reward: -293.000000  Success rate: 0.9375 Mean loss: 0.0300\n","Epoch: 510  Cumulative reward: -302.000000  Success rate: 0.8750 Mean loss: 0.0290\n","Epoch: 515  Cumulative reward: -299.000000  Success rate: 0.8750 Mean loss: 0.0284\n","Epoch: 520  Cumulative reward: -309.000000  Success rate: 0.8125 Mean loss: 0.0274\n","Epoch: 525  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0296\n","Epoch: 530  Cumulative reward: -296.000000  Success rate: 0.9375 Mean loss: 0.0267\n","Epoch: 535  Cumulative reward: -299.000000  Success rate: 0.9375 Mean loss: 0.0274\n","Epoch: 540  Cumulative reward: -295.000000  Success rate: 0.8750 Mean loss: 0.0266\n","Epoch: 545  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0264\n","Epoch: 550  Cumulative reward: -292.000000  Success rate: 0.9375 Mean loss: 0.0252\n","Epoch: 555  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0258\n","Epoch: 560  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0252\n","Epoch: 565  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0258\n","Epoch: 570  Cumulative reward: -294.000000  Success rate: 1.0000 Mean loss: 0.0289\n","Epoch: 575  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0304\n","Epoch: 580  Cumulative reward: -308.000000  Success rate: 0.8750 Mean loss: 0.0242\n","Epoch: 585  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0239\n","Epoch: 590  Cumulative reward: -286.000000  Success rate: 1.0000 Mean loss: 0.0260\n","Epoch: 595  Cumulative reward: -289.000000  Success rate: 0.9375 Mean loss: 0.0248\n","Epoch: 600  Cumulative reward: -303.000000  Success rate: 0.9375 Mean loss: 0.0238\n","Epoch: 605  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0237\n","Epoch: 610  Cumulative reward: -292.000000  Success rate: 0.9375 Mean loss: 0.0247\n","Epoch: 615  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0238\n","Epoch: 620  Cumulative reward: -294.000000  Success rate: 1.0000 Mean loss: 0.0224\n","Epoch: 625  Cumulative reward: -296.000000  Success rate: 0.9375 Mean loss: 0.0236\n","Epoch: 630  Cumulative reward: -287.000000  Success rate: 1.0000 Mean loss: 0.0235\n","Epoch: 635  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0236\n","Epoch: 640  Cumulative reward: -305.000000  Success rate: 0.8750 Mean loss: 0.0235\n","Epoch: 645  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0239\n","Epoch: 650  Cumulative reward: -290.000000  Success rate: 0.9375 Mean loss: 0.0223\n","Epoch: 655  Cumulative reward: -288.000000  Success rate: 1.0000 Mean loss: 0.0219\n","Epoch: 660  Cumulative reward: -304.000000  Success rate: 0.9375 Mean loss: 0.0222\n","Epoch: 665  Cumulative reward: -295.000000  Success rate: 1.0000 Mean loss: 0.0214\n","Epoch: 670  Cumulative reward: -282.000000  Success rate: 1.0000 Mean loss: 0.0222\n","Epoch: 675  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0206\n","Epoch: 680  Cumulative reward: -296.000000  Success rate: 1.0000 Mean loss: 0.0210\n","Epoch: 685  Cumulative reward: -296.000000  Success rate: 0.9375 Mean loss: 0.0193\n","Epoch: 690  Cumulative reward: -284.000000  Success rate: 1.0000 Mean loss: 0.0225\n","Epoch: 695  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0202\n","Epoch: 700  Cumulative reward: -282.000000  Success rate: 1.0000 Mean loss: 0.0180\n","Epoch: 705  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0195\n","Epoch: 710  Cumulative reward: -297.000000  Success rate: 1.0000 Mean loss: 0.0184\n","Epoch: 715  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0199\n","Epoch: 720  Cumulative reward: -290.000000  Success rate: 0.9375 Mean loss: 0.0209\n","Epoch: 725  Cumulative reward: -287.000000  Success rate: 1.0000 Mean loss: 0.0196\n","Epoch: 730  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0196\n","Epoch: 735  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0181\n","Epoch: 740  Cumulative reward: -287.000000  Success rate: 1.0000 Mean loss: 0.0172\n","Epoch: 745  Cumulative reward: -286.000000  Success rate: 1.0000 Mean loss: 0.0187\n","Epoch: 750  Cumulative reward: -306.000000  Success rate: 0.8750 Mean loss: 0.0184\n","Epoch: 755  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0197\n","Epoch: 760  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0175\n","Epoch: 765  Cumulative reward: -289.000000  Success rate: 0.9375 Mean loss: 0.0173\n","Epoch: 770  Cumulative reward: -286.000000  Success rate: 1.0000 Mean loss: 0.0179\n","Epoch: 775  Cumulative reward: -297.000000  Success rate: 1.0000 Mean loss: 0.0178\n","Epoch: 780  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0190\n","Epoch: 785  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0160\n","Epoch: 790  Cumulative reward: -297.000000  Success rate: 1.0000 Mean loss: 0.0157\n","Epoch: 795  Cumulative reward: -286.000000  Success rate: 1.0000 Mean loss: 0.0166\n","Epoch: 800  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0191\n","Epoch: 805  Cumulative reward: -295.000000  Success rate: 0.9375 Mean loss: 0.0160\n","Epoch: 810  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0150\n","Epoch: 815  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0148\n","Epoch: 820  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0168\n","Epoch: 825  Cumulative reward: -294.000000  Success rate: 1.0000 Mean loss: 0.0150\n","Epoch: 830  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0144\n","Epoch: 835  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0148\n","Epoch: 840  Cumulative reward: -292.000000  Success rate: 1.0000 Mean loss: 0.0170\n","Epoch: 845  Cumulative reward: -284.000000  Success rate: 1.0000 Mean loss: 0.0153\n","Epoch: 850  Cumulative reward: -284.000000  Success rate: 1.0000 Mean loss: 0.0134\n","Epoch: 855  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0149\n","Epoch: 860  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0152\n","Epoch: 865  Cumulative reward: -302.000000  Success rate: 1.0000 Mean loss: 0.0151\n","Epoch: 870  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0151\n","Epoch: 875  Cumulative reward: -280.000000  Success rate: 1.0000 Mean loss: 0.0137\n","Epoch: 880  Cumulative reward: -282.000000  Success rate: 1.0000 Mean loss: 0.0139\n","Epoch: 885  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0150\n","Epoch: 890  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0153\n","Epoch: 895  Cumulative reward: -286.000000  Success rate: 1.0000 Mean loss: 0.0132\n","Epoch: 900  Cumulative reward: -281.000000  Success rate: 1.0000 Mean loss: 0.0129\n","Epoch: 905  Cumulative reward: -296.000000  Success rate: 1.0000 Mean loss: 0.0129\n","Epoch: 910  Cumulative reward: -301.000000  Success rate: 1.0000 Mean loss: 0.0136\n","Epoch: 915  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0130\n","Epoch: 920  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0131\n","Epoch: 925  Cumulative reward: -284.000000  Success rate: 1.0000 Mean loss: 0.0130\n","Epoch: 930  Cumulative reward: -288.000000  Success rate: 1.0000 Mean loss: 0.0127\n","Epoch: 935  Cumulative reward: -285.000000  Success rate: 1.0000 Mean loss: 0.0122\n","Epoch: 940  Cumulative reward: -278.000000  Success rate: 1.0000 Mean loss: 0.0135\n","Epoch: 945  Cumulative reward: -284.000000  Success rate: 1.0000 Mean loss: 0.0123\n","Epoch: 950  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0123\n","Epoch: 955  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0137\n","Epoch: 960  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0133\n","Epoch: 965  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0126\n","Epoch: 970  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0119\n","Epoch: 975  Cumulative reward: -287.000000  Success rate: 1.0000 Mean loss: 0.0124\n","Epoch: 980  Cumulative reward: -290.000000  Success rate: 1.0000 Mean loss: 0.0122\n","Epoch: 985  Cumulative reward: -289.000000  Success rate: 1.0000 Mean loss: 0.0139\n","Epoch: 990  Cumulative reward: -291.000000  Success rate: 1.0000 Mean loss: 0.0125\n","Epoch: 995  Cumulative reward: -283.000000  Success rate: 1.0000 Mean loss: 0.0128\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"1Aa0hZRkWsC42yd-pzGmCD6F-d4YK-JYZ"},"id":"9JuXzboB9lqL","executionInfo":{"status":"ok","timestamp":1632493631961,"user_tz":-180,"elapsed":2209,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"e96a2b6a-db01-4dd7-85c1-364abfc86c17"},"source":["exp_to_accuracies = {\n","    \"bitflip_25_her_none\": success_rate,\n","    \"bitflip_25_her_final\": success_rate_final\n","}\n","text = 'Bitflip with 25 bits'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbgVQqIG3KKI","executionInfo":{"status":"ok","timestamp":1632494742520,"user_tz":-180,"elapsed":894326,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"67047a3e-45cc-4459-84bb-eaccb4cab112"},"source":["success_rate = flip_bits(num_bits=15, num_epochs=500, HER='None')\n","success_rate_final = flip_bits(num_bits=15, num_epochs=500, HER='final')\n","success_rate_future = flip_bits(num_bits=15, num_epochs=500, HER='future')\n","success_rate_random = flip_bits(num_bits=15, num_epochs=500, HER='random')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running bit flip environment with 15 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1270\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 70  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 80  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 85  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 90  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 100  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 105  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0018\n","Epoch: 110  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 115  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 120  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 125  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 130  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 135  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 140  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 145  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 150  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 155  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 160  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 165  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 170  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 175  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 180  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 185  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 190  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 195  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 200  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 205  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 210  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 215  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 220  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 225  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 230  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 235  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 240  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 245  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 250  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 255  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 260  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 265  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 270  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 275  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 280  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 285  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 290  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 295  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 300  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 305  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 310  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 315  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 320  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0059\n","Epoch: 325  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 330  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 335  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 340  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 345  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 350  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 355  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 360  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 365  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 370  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 375  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 380  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 385  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 390  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 395  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 400  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 405  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 410  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 415  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 420  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 425  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 430  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 435  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 440  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 445  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 450  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 455  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 460  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 465  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 470  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 475  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 480  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 485  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Epoch: 490  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 495  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Running bit flip environment with 15 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1151\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0514\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0583\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0664\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0605\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0564\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0544\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0555\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0500\n","Epoch: 45  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0479\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0457\n","Epoch: 55  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0435\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0439\n","Epoch: 65  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0400\n","Epoch: 70  Cumulative reward: -224.000000  Success rate: 0.1875 Mean loss: 0.0409\n","Epoch: 75  Cumulative reward: -223.000000  Success rate: 0.1875 Mean loss: 0.0361\n","Epoch: 80  Cumulative reward: -214.000000  Success rate: 0.3125 Mean loss: 0.0332\n","Epoch: 85  Cumulative reward: -205.000000  Success rate: 0.4375 Mean loss: 0.0337\n","Epoch: 90  Cumulative reward: -186.000000  Success rate: 0.6875 Mean loss: 0.0325\n","Epoch: 95  Cumulative reward: -210.000000  Success rate: 0.3750 Mean loss: 0.0317\n","Epoch: 100  Cumulative reward: -206.000000  Success rate: 0.4375 Mean loss: 0.0276\n","Epoch: 105  Cumulative reward: -191.000000  Success rate: 0.7500 Mean loss: 0.0282\n","Epoch: 110  Cumulative reward: -188.000000  Success rate: 0.6875 Mean loss: 0.0253\n","Epoch: 115  Cumulative reward: -187.000000  Success rate: 0.7500 Mean loss: 0.0236\n","Epoch: 120  Cumulative reward: -199.000000  Success rate: 0.5625 Mean loss: 0.0221\n","Epoch: 125  Cumulative reward: -185.000000  Success rate: 0.7500 Mean loss: 0.0222\n","Epoch: 130  Cumulative reward: -175.000000  Success rate: 0.8750 Mean loss: 0.0208\n","Epoch: 135  Cumulative reward: -166.000000  Success rate: 0.9375 Mean loss: 0.0184\n","Epoch: 140  Cumulative reward: -182.000000  Success rate: 0.7500 Mean loss: 0.0177\n","Epoch: 145  Cumulative reward: -172.000000  Success rate: 0.9375 Mean loss: 0.0182\n","Epoch: 150  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0167\n","Epoch: 155  Cumulative reward: -179.000000  Success rate: 0.8125 Mean loss: 0.0159\n","Epoch: 160  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0162\n","Epoch: 165  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0162\n","Epoch: 170  Cumulative reward: -172.000000  Success rate: 0.9375 Mean loss: 0.0150\n","Epoch: 175  Cumulative reward: -176.000000  Success rate: 0.8750 Mean loss: 0.0147\n","Epoch: 180  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0150\n","Epoch: 185  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0140\n","Epoch: 190  Cumulative reward: -172.000000  Success rate: 0.9375 Mean loss: 0.0136\n","Epoch: 195  Cumulative reward: -175.000000  Success rate: 0.9375 Mean loss: 0.0139\n","Epoch: 200  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0130\n","Epoch: 205  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0137\n","Epoch: 210  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0135\n","Epoch: 215  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0124\n","Epoch: 220  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0133\n","Epoch: 225  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0128\n","Epoch: 230  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0122\n","Epoch: 235  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0128\n","Epoch: 240  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0123\n","Epoch: 245  Cumulative reward: -176.000000  Success rate: 0.9375 Mean loss: 0.0119\n","Epoch: 250  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0110\n","Epoch: 255  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0127\n","Epoch: 260  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0113\n","Epoch: 265  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0111\n","Epoch: 270  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0114\n","Epoch: 275  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0108\n","Epoch: 280  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0111\n","Epoch: 285  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0097\n","Epoch: 290  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0122\n","Epoch: 295  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0098\n","Epoch: 300  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0095\n","Epoch: 305  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0096\n","Epoch: 310  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0097\n","Epoch: 315  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0092\n","Epoch: 320  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 325  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0092\n","Epoch: 330  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0097\n","Epoch: 335  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0095\n","Epoch: 340  Cumulative reward: -176.000000  Success rate: 1.0000 Mean loss: 0.0089\n","Epoch: 345  Cumulative reward: -160.000000  Success rate: 1.0000 Mean loss: 0.0088\n","Epoch: 350  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 355  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0098\n","Epoch: 360  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0084\n","Epoch: 365  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0082\n","Epoch: 370  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0088\n","Epoch: 375  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0079\n","Epoch: 380  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0079\n","Epoch: 385  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0076\n","Epoch: 390  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0072\n","Epoch: 395  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0078\n","Epoch: 400  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0076\n","Epoch: 405  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0071\n","Epoch: 410  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0075\n","Epoch: 415  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 420  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0074\n","Epoch: 425  Cumulative reward: -176.000000  Success rate: 1.0000 Mean loss: 0.0066\n","Epoch: 430  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0072\n","Epoch: 435  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 440  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 445  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 450  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0068\n","Epoch: 455  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0070\n","Epoch: 460  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0069\n","Epoch: 465  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0065\n","Epoch: 470  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 475  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 480  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 485  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0060\n","Epoch: 490  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0056\n","Epoch: 495  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0053\n","Running bit flip environment with 15 bits and HER policy: future\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1352\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0838\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1058\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1027\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0950\n","Epoch: 25  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0756\n","Epoch: 30  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.0649\n","Epoch: 35  Cumulative reward: -225.000000  Success rate: 0.1875 Mean loss: 0.0620\n","Epoch: 40  Cumulative reward: -226.000000  Success rate: 0.1875 Mean loss: 0.0490\n","Epoch: 45  Cumulative reward: -220.000000  Success rate: 0.2500 Mean loss: 0.0408\n","Epoch: 50  Cumulative reward: -215.000000  Success rate: 0.3125 Mean loss: 0.0392\n","Epoch: 55  Cumulative reward: -202.000000  Success rate: 0.5000 Mean loss: 0.0339\n","Epoch: 60  Cumulative reward: -195.000000  Success rate: 0.5625 Mean loss: 0.0318\n","Epoch: 65  Cumulative reward: -201.000000  Success rate: 0.5625 Mean loss: 0.0298\n","Epoch: 70  Cumulative reward: -199.000000  Success rate: 0.5625 Mean loss: 0.0276\n","Epoch: 75  Cumulative reward: -192.000000  Success rate: 0.7500 Mean loss: 0.0250\n","Epoch: 80  Cumulative reward: -179.000000  Success rate: 0.7500 Mean loss: 0.0220\n","Epoch: 85  Cumulative reward: -201.000000  Success rate: 0.5625 Mean loss: 0.0210\n","Epoch: 90  Cumulative reward: -182.000000  Success rate: 0.8125 Mean loss: 0.0199\n","Epoch: 95  Cumulative reward: -175.000000  Success rate: 0.8750 Mean loss: 0.0179\n","Epoch: 100  Cumulative reward: -178.000000  Success rate: 0.8125 Mean loss: 0.0164\n","Epoch: 105  Cumulative reward: -186.000000  Success rate: 0.7500 Mean loss: 0.0158\n","Epoch: 110  Cumulative reward: -176.000000  Success rate: 0.8750 Mean loss: 0.0159\n","Epoch: 115  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0151\n","Epoch: 120  Cumulative reward: -187.000000  Success rate: 0.7500 Mean loss: 0.0137\n","Epoch: 125  Cumulative reward: -173.000000  Success rate: 0.8125 Mean loss: 0.0129\n","Epoch: 130  Cumulative reward: -178.000000  Success rate: 0.8750 Mean loss: 0.0122\n","Epoch: 135  Cumulative reward: -172.000000  Success rate: 0.9375 Mean loss: 0.0118\n","Epoch: 140  Cumulative reward: -178.000000  Success rate: 0.8750 Mean loss: 0.0123\n","Epoch: 145  Cumulative reward: -176.000000  Success rate: 0.9375 Mean loss: 0.0115\n","Epoch: 150  Cumulative reward: -166.000000  Success rate: 0.9375 Mean loss: 0.0114\n","Epoch: 155  Cumulative reward: -180.000000  Success rate: 0.8125 Mean loss: 0.0112\n","Epoch: 160  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0113\n","Epoch: 165  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0110\n","Epoch: 170  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0115\n","Epoch: 175  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 180  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0096\n","Epoch: 185  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0106\n","Epoch: 190  Cumulative reward: -180.000000  Success rate: 0.9375 Mean loss: 0.0095\n","Epoch: 195  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0094\n","Epoch: 200  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0102\n","Epoch: 205  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0094\n","Epoch: 210  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 215  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0091\n","Epoch: 220  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0090\n","Epoch: 225  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0086\n","Epoch: 230  Cumulative reward: -180.000000  Success rate: 0.9375 Mean loss: 0.0085\n","Epoch: 235  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0089\n","Epoch: 240  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0090\n","Epoch: 245  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0084\n","Epoch: 250  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0081\n","Epoch: 255  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0072\n","Epoch: 260  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0080\n","Epoch: 265  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0084\n","Epoch: 270  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0073\n","Epoch: 275  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0071\n","Epoch: 280  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0072\n","Epoch: 285  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0071\n","Epoch: 290  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0071\n","Epoch: 295  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0075\n","Epoch: 300  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0073\n","Epoch: 305  Cumulative reward: -160.000000  Success rate: 1.0000 Mean loss: 0.0069\n","Epoch: 310  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0062\n","Epoch: 315  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0064\n","Epoch: 320  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0064\n","Epoch: 325  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0062\n","Epoch: 330  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0062\n","Epoch: 335  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0064\n","Epoch: 340  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0061\n","Epoch: 345  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0054\n","Epoch: 350  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0057\n","Epoch: 355  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0067\n","Epoch: 360  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0057\n","Epoch: 365  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0054\n","Epoch: 370  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0053\n","Epoch: 375  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0060\n","Epoch: 380  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 385  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0052\n","Epoch: 390  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0046\n","Epoch: 395  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0052\n","Epoch: 400  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0060\n","Epoch: 405  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0049\n","Epoch: 410  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0049\n","Epoch: 415  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0043\n","Epoch: 420  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0049\n","Epoch: 425  Cumulative reward: -161.000000  Success rate: 1.0000 Mean loss: 0.0043\n","Epoch: 430  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0050\n","Epoch: 435  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0049\n","Epoch: 440  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0045\n","Epoch: 445  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0053\n","Epoch: 450  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0044\n","Epoch: 455  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0042\n","Epoch: 460  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0042\n","Epoch: 465  Cumulative reward: -160.000000  Success rate: 1.0000 Mean loss: 0.0039\n","Epoch: 470  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0046\n","Epoch: 475  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0041\n","Epoch: 480  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0050\n","Epoch: 485  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0038\n","Epoch: 490  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0047\n","Epoch: 495  Cumulative reward: -172.000000  Success rate: 1.0000 Mean loss: 0.0039\n","Running bit flip environment with 15 bits and HER policy: random\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1272\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1007\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1139\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1105\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0955\n","Epoch: 25  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0802\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0701\n","Epoch: 35  Cumulative reward: -223.000000  Success rate: 0.1875 Mean loss: 0.0559\n","Epoch: 40  Cumulative reward: -219.000000  Success rate: 0.2500 Mean loss: 0.0494\n","Epoch: 45  Cumulative reward: -216.000000  Success rate: 0.2500 Mean loss: 0.0466\n","Epoch: 50  Cumulative reward: -209.000000  Success rate: 0.3750 Mean loss: 0.0446\n","Epoch: 55  Cumulative reward: -206.000000  Success rate: 0.5000 Mean loss: 0.0405\n","Epoch: 60  Cumulative reward: -214.000000  Success rate: 0.3750 Mean loss: 0.0376\n","Epoch: 65  Cumulative reward: -196.000000  Success rate: 0.5625 Mean loss: 0.0338\n","Epoch: 70  Cumulative reward: -189.000000  Success rate: 0.6875 Mean loss: 0.0328\n","Epoch: 75  Cumulative reward: -210.000000  Success rate: 0.4375 Mean loss: 0.0297\n","Epoch: 80  Cumulative reward: -182.000000  Success rate: 0.7500 Mean loss: 0.0292\n","Epoch: 85  Cumulative reward: -176.000000  Success rate: 0.8750 Mean loss: 0.0257\n","Epoch: 90  Cumulative reward: -182.000000  Success rate: 0.7500 Mean loss: 0.0241\n","Epoch: 95  Cumulative reward: -178.000000  Success rate: 0.8125 Mean loss: 0.0226\n","Epoch: 100  Cumulative reward: -176.000000  Success rate: 0.8125 Mean loss: 0.0219\n","Epoch: 105  Cumulative reward: -174.000000  Success rate: 0.9375 Mean loss: 0.0211\n","Epoch: 110  Cumulative reward: -180.000000  Success rate: 0.8125 Mean loss: 0.0202\n","Epoch: 115  Cumulative reward: -182.000000  Success rate: 0.8125 Mean loss: 0.0178\n","Epoch: 120  Cumulative reward: -185.000000  Success rate: 0.7500 Mean loss: 0.0177\n","Epoch: 125  Cumulative reward: -176.000000  Success rate: 0.8125 Mean loss: 0.0161\n","Epoch: 130  Cumulative reward: -171.000000  Success rate: 0.9375 Mean loss: 0.0156\n","Epoch: 135  Cumulative reward: -168.000000  Success rate: 0.9375 Mean loss: 0.0154\n","Epoch: 140  Cumulative reward: -170.000000  Success rate: 0.9375 Mean loss: 0.0144\n","Epoch: 145  Cumulative reward: -186.000000  Success rate: 0.7500 Mean loss: 0.0141\n","Epoch: 150  Cumulative reward: -175.000000  Success rate: 0.9375 Mean loss: 0.0136\n","Epoch: 155  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0156\n","Epoch: 160  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0122\n","Epoch: 165  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0129\n","Epoch: 170  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0134\n","Epoch: 175  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0131\n","Epoch: 180  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0120\n","Epoch: 185  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0119\n","Epoch: 190  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0111\n","Epoch: 195  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0112\n","Epoch: 200  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0106\n","Epoch: 205  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0111\n","Epoch: 210  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0110\n","Epoch: 215  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0114\n","Epoch: 220  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0107\n","Epoch: 225  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0115\n","Epoch: 230  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0105\n","Epoch: 235  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0106\n","Epoch: 240  Cumulative reward: -159.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 245  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0099\n","Epoch: 250  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 255  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0104\n","Epoch: 260  Cumulative reward: -155.000000  Success rate: 1.0000 Mean loss: 0.0112\n","Epoch: 265  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0102\n","Epoch: 270  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0099\n","Epoch: 275  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 280  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 285  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0086\n","Epoch: 290  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0086\n","Epoch: 295  Cumulative reward: -164.000000  Success rate: 1.0000 Mean loss: 0.0093\n","Epoch: 300  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0081\n","Epoch: 305  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0092\n","Epoch: 310  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0079\n","Epoch: 315  Cumulative reward: -163.000000  Success rate: 1.0000 Mean loss: 0.0081\n","Epoch: 320  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0086\n","Epoch: 325  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 330  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0078\n","Epoch: 335  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0091\n","Epoch: 340  Cumulative reward: -159.000000  Success rate: 1.0000 Mean loss: 0.0071\n","Epoch: 345  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0077\n","Epoch: 350  Cumulative reward: -166.000000  Success rate: 1.0000 Mean loss: 0.0073\n","Epoch: 355  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0075\n","Epoch: 360  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0067\n","Epoch: 365  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0075\n","Epoch: 370  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0079\n","Epoch: 375  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0069\n","Epoch: 380  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0065\n","Epoch: 385  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0067\n","Epoch: 390  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0059\n","Epoch: 395  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0061\n","Epoch: 400  Cumulative reward: -169.000000  Success rate: 1.0000 Mean loss: 0.0064\n","Epoch: 405  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0060\n","Epoch: 410  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0069\n","Epoch: 415  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0063\n","Epoch: 420  Cumulative reward: -174.000000  Success rate: 1.0000 Mean loss: 0.0056\n","Epoch: 425  Cumulative reward: -162.000000  Success rate: 1.0000 Mean loss: 0.0056\n","Epoch: 430  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0054\n","Epoch: 435  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 440  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0054\n","Epoch: 445  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0058\n","Epoch: 450  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0056\n","Epoch: 455  Cumulative reward: -171.000000  Success rate: 1.0000 Mean loss: 0.0051\n","Epoch: 460  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0053\n","Epoch: 465  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0053\n","Epoch: 470  Cumulative reward: -168.000000  Success rate: 1.0000 Mean loss: 0.0051\n","Epoch: 475  Cumulative reward: -175.000000  Success rate: 1.0000 Mean loss: 0.0048\n","Epoch: 480  Cumulative reward: -170.000000  Success rate: 1.0000 Mean loss: 0.0052\n","Epoch: 485  Cumulative reward: -167.000000  Success rate: 1.0000 Mean loss: 0.0054\n","Epoch: 490  Cumulative reward: -173.000000  Success rate: 1.0000 Mean loss: 0.0048\n","Epoch: 495  Cumulative reward: -165.000000  Success rate: 1.0000 Mean loss: 0.0060\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"1jFwySrqHu-CtOB92WXhMoK31_rmrQxHf"},"id":"FD7XDdgtBKkk","executionInfo":{"status":"ok","timestamp":1632494744367,"user_tz":-180,"elapsed":1862,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"a88087e8-cde1-4fc4-a53a-b3a0d15ccf40"},"source":["exp_to_accuracies = {\n","    \"bitflip_15_her_none\": success_rate,\n","    \"bitflip_15_her_final\": success_rate_final,\n","    \"bitflip_15_her_future\": success_rate_future,\n","    \"bitflip_15_her_random\": success_rate_random,\n","}\n","text = 'Bitflip with 15 bits'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"lua6jJR-cnv2"},"source":["# Sawyer Environment Goal-Conditioned RL"]},{"cell_type":"code","metadata":{"id":"7ojAJB_4colS","executionInfo":{"status":"ok","timestamp":1632579525529,"user_tz":-180,"elapsed":469,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["#@title Buffer\n","#@markdown Same as the Buffer class before but placed here in the event you want to run the sections separately\n","import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4b_b31-lQFp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632579551271,"user_tz":-180,"elapsed":25340,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"4617290f-0028-4db7-bcb6-2c3488df9ce9"},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import multiworld\n","import glfw\n","\n","multiworld.register_all_envs()   \n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_act):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_act,activation = None)\n","\n","  def call(self, inputs):\n","    x = self.dense1(inputs)\n","    return self.out(x)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["pygame 2.0.1 (SDL 2.0.14, Python 3.7.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"]}]},{"cell_type":"code","metadata":{"id":"AVtTlZFSlSi2","executionInfo":{"status":"ok","timestamp":1632582031799,"user_tz":-180,"elapsed":1287,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["# ************   Helper functions    ************ #\n","# Globals\n","\n","NUM_DIM = 2\n","NUM_ACT = 4\n","done_threshold = -0.01\n","Sawyer_Env = env = wrap_env(gym.make('SawyerReachXYEnv-v1'))\n","\n","def updateTarget(model, target_model, tau=0.95) :\n","    model_weights = model.get_weights()\n","    target_weights = target_model.get_weights()\n","    new_weights = []\n","    for i, weight in enumerate(model_weights):\n","      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n","\n","    target_model.set_weights(new_weights)\n","\n","def take_action(action, render):\n","    '''passes the discrete action selected by the Q-network to the Sawyer Arm.\n","    The function returns the next state, the reward, and whether the environment\n","    was solved. The environment done returned is not the same as the environment\n","    done returned by the Sawyer environment. Due to discretization, it may not be\n","    possible to exactly reach the goal. The done flag returns true if the end\n","    state is within done_threshold of the final goal\n","\n","    inputs:  action - integer (0 to NUM_ACT-1) selected by the Q-network\n","    outputs: next_state - new state (x, y) location of arm\n","             reward - reward returned by Sawyer environment\n","             done - boolean whether environment is solved'''\n","\n","    # maps actions selected by Q-network to Sawyer arm actions\n","    # array MUST be length NUM_ACT\n","    action_dic = {0:[-1, 0], 1:[1, 0], 2:[0, -1], 3:[0, 1]}\n","    # look up which action in Sawyer arm space corresponds to the selected integer action\n","    action_sawyer = np.array(action_dic[action], dtype=np.float32)\n","    # take the action\n","    ob, reward, done, info = Sawyer_Env.step(action_sawyer)\n","    # if rendering is turned on, render the environment\n","    if render:\n","        Sawyer_Env.render(mode='rgb_array')\n","    # check if we're \"close enough\" to declare done\n","    if reward > done_threshold:\n","        done = True\n","\n","    # pull the observed state off\n","    next_state = ob['observation'][0:2]\n","\n","    return next_state, reward, done, info\n","\n","def solve_environment(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the passed argument steps_per_episode.\n","\n","      # ======================== TODO modify code ========================\n","      inputs = state\n","      inputs = np.expand_dims(inputs, axis=0)\n","      inputs = np.array(inputs, dtype=np.float32) \n","      # forward pass to find action\n","      out = model(inputs)\n","      # take the action - use helper function to convert discrete actions to\n","      # actions in the Sawyer environment\n","      action = np.argmax(out,axis = 1)[0]\n","      next_state,reward,done, _ = take_action(action, render)\n","      # add to the episode experience (what happened)\n","      ### NEW UPDATE ######\n","      goal_orig_size = int(goal_state.size / 2)\n","      next_state = np.concatenate([next_state, goal_state[:goal_orig_size]])\n","      ### NEW UPDATE ######\n","      # calculate total reward\n","      episode_experience.append((state, action, reward, next_state, goal_state))\n","      total_reward += reward\n","      # update state\n","      state = next_state\n","      # mark that we've finished the episode and succeeded with training\n","      if done:\n","          succeeded = True\n","\n","      \n","\n","      # add to the episode experience (what happened)\n","\n","      # calculate total reward\n","\n","      # update state\n","\n","      # mark that we've finished the episode and succeeded with training\n","\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy with no goal condition'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        inputs = np.array(inputs, dtype=np.float32) \n","        # forward pass to find action\n","        out = model(inputs)\n","        action = np.argmax(out,axis = 1)[0]\n","        next_state,reward,done, _ = take_action(action, render)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward += reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","    else:\n","         env.stats_recorder.save_complete()\n","         env.stats_recorder.done = True\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def create_sawyer_her_states_reward(experience, idx, s, s_, vec_orig_size):\n","    '''\n","    given an (episode) experience and an index, we set a new goal (the state at time \"index\") \n","    and create a new state, next state and reward in accordance to that goal\n","\n","    inputs: experience - list of transitions from the last episode\n","            idx - the index of the goal of the new task\n","            s - current state\n","            s_ - next state\n","            vec_orig_size - size of new goal\n","    outputs: a tri-tuple of state, reward and next state \n","    '''\n","    _, _, _, s_fin, _ = experience[idx]  \n","    goal_state = create_her_state(s_fin, s_fin, vec_orig_size)  # set new goal to final state in episode\n","    state = create_her_state(s, s_fin, vec_orig_size)  # update state (goal-conditioned with new goal)\n","    next_state = create_her_state(s_, s_fin, vec_orig_size)  # update next state (goal-conditioned with new goal)\n","    reward = set_sawyer_her_reward(next_state, goal_state)\n","    return state, reward, next_state\n","\n","\n","def create_her_state(state, goal_state, vec_orig_size):\n","    her_state = np.copy(state)\n","    her_state[vec_orig_size:] = goal_state[:vec_orig_size]  # set new goal state\n","    return her_state\n","\n","def set_sawyer_her_reward(next_state, goal_state):\n","    r = - np.sqrt(np.sum((next_state - goal_state) ** 2))  # reward as negative euclidean distance\n","    return r\n","\n","def update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, num_relabeled additional relabeled data points are also added\n","    to the replay buffer\n","\n","    inputs:    epsidode_experience - list of transitions from the last episode\n","    modifies:  replay_buffer\n","    outputs:   None'''\n","    for t in range(steps_per_episode) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = s\n","        # next state\n","        inputs_ = s_\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)\n","\n","\n","        # when HER is used, each call to update_replay_buffer should add num_relabeled\n","        # relabeled points to the replay buffer per step\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            continue\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            vec_orig_size = int(g.size / 2)\n","            s_fin, r_fin, next_s_fin = create_sawyer_her_states_reward(episode_experience, -1, s, s_, vec_orig_size)\n","            replay_buffer.add(s_fin, a, r_fin, next_s_fin)\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            vec_orig_size = int(g.size / 2)\n","            num_goals = min(steps_per_episode - t , num_relabeled)  # create a legal number of goals per step\n","            g_idxes = random.sample(range(t, steps_per_episode), num_goals)\n","            for g_idx in g_idxes:\n","                s_fut, r_fut, next_s_fut = create_sawyer_her_states_reward(episode_experience, g_idx, s, s_, vec_orig_size)\n","                replay_buffer.add(s_fut, a, r_fut, next_s_fut)\n","\n","        elif HER == 'random':\n","            # random - relabel based on a random state in the episode\n","            vec_orig_size = int(g.size / 2)\n","            num_goals = min(steps_per_episode , num_relabeled)  # create a legal number of goals per step\n","            g_idxes = random.sample(range(steps_per_episode), num_goals)\n","            for g_idx in g_idxes:\n","                s_rand, r_rand, next_s_rand = create_sawyer_her_states_reward(episode_experience, g_idx, s, s_, vec_orig_size)\n","                replay_buffer.add(s_rand, a, r_rand, next_s_rand)\n","\n","        # ========================      END TODO       ========================\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"xX0tbP_w9ViT","executionInfo":{"status":"ok","timestamp":1632580292968,"user_tz":-180,"elapsed":6,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["# ************   Main Training Loop    ************ #\n","\n","def run_sawyer(num_epochs, buffer_size=1e6, batch_size=128, \n","               num_episodes=16, num_relabeled=4, gamma=0.98, log_interval=5, opt_steps=40,\n","               steps_per_episode=50, render=False, HER=\"None\"):\n","    '''Main loop for running in the Sawyer environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","    # create Sawyer arm environment and replay buffer\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(NUM_ACT)\n","    target_model = Model(NUM_ACT)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    reset_state = Sawyer_Env.reset()  \n","    state = reset_state['observation'][:2]          # look up the state\n","    goal_state = reset_state['desired_goal'][:2]\n","    #########  NEW ############\n","    state = np.concatenate([state, goal_state])\n","    goal_state = np.concatenate([goal_state, goal_state])\n","    #########  NEW ############\n","    inputs = np.expand_dims(state, axis=0)\n","    model(inputs)\n","    target_model(inputs)\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model, tau=0.0)\n","\n","    # ========================      END TODO       ========================\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            reset_state = Sawyer_Env.reset()                # reset the environment\n","            state = reset_state['observation'][:2]          # look up the state\n","            goal_state = reset_state['desired_goal'][:2]    # look up the goal\n","\n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            #########  NEW ############\n","            state = np.concatenate([state, goal_state])\n","            goal_state = np.concatenate([goal_state, goal_state])\n","            #########  NEW ############\n","            # succeeded, episode_experience, total_reward = solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render)\n","            succeeded, episode_experience, total_reward = solve_environment(model, state, goal_state, total_reward, steps_per_episode, render)\n","            # ========================      END TODO       ========================\n","\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","            env.close() \n","            glfw.terminate()\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            state = np.array(state, dtype=np.float32) \n","            next_state = np.array(next_state, dtype=np.float32) \n","            # forward pass through target network   \n","\n","            with tf.GradientTape() as tape:\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate loss\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, NUM_ACT)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","    return success_rate"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCMq8sfUub0j","executionInfo":{"status":"ok","timestamp":1632580295363,"user_tz":-180,"elapsed":398,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}}},"source":["from IPython.display import HTML\n","from plotly import graph_objs as go\n","def plot_success(exp_to_accuracies, text):\n","    # Creates the Figure\n","    fig = go.Figure()\n","    data = []\n","    for experiment, accuracies in exp_to_accuracies.items():\n","        steps = range(len(accuracies))\n","        steps = [5 * x for x in steps]\n","        data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n","\n","    # Applies a custom layout\n","    layout = go.Layout(\n","        title=go.layout.Title(\n","            text=text,\n","            x=0.5\n","        ),\n","        xaxis=go.layout.XAxis(\n","            title=go.layout.xaxis.Title(\n","                text='Epoch',\n","                font=dict(\n","                    family='Courier New, monospace',\n","                    size=18,\n","                    color='#7f7f7f'\n","                )\n","            )\n","        ),\n","        yaxis=go.layout.YAxis(\n","            title=go.layout.yaxis.Title(\n","                text='Success Rate',\n","                font=dict(\n","                    family='Courier New, monospace',\n","                    size=18,\n","                    color='#7f7f7f'\n","                )\n","            )\n","        )\n","    )\n","\n","    fig = go.Figure(data=data, layout=layout)\n","    return fig\n","    "],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLZxnMUGXm_2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMNnVQsNAgni","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632580156666,"user_tz":-180,"elapsed":599690,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"f3fe883c-0b1a-4695-c230-8925ba4fdf8d"},"source":["#@title Runs\n","#@markdown run on Sawyer Environment (no goal-conditioned and no HER)\n","success_rate = run_sawyer(num_epochs=150, HER = \"None\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  Cumulative reward: -107.565840  Success rate: 0.0625 Mean loss: 0.0048\n","Epoch: 5  Cumulative reward: -128.197736  Success rate: 0.0625 Mean loss: 0.0052\n","Epoch: 10  Cumulative reward: -143.172667  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 15  Cumulative reward: -124.509480  Success rate: 0.0625 Mean loss: 0.0040\n","Epoch: 20  Cumulative reward: -112.911280  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 25  Cumulative reward: -118.367723  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 30  Cumulative reward: -110.712553  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 35  Cumulative reward: -118.898256  Success rate: 0.0625 Mean loss: 0.0035\n","Epoch: 40  Cumulative reward: -144.065362  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 45  Cumulative reward: -110.821143  Success rate: 0.0625 Mean loss: 0.0035\n","Epoch: 50  Cumulative reward: -129.904861  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 55  Cumulative reward: -126.549859  Success rate: 0.0625 Mean loss: 0.0034\n","Epoch: 60  Cumulative reward: -130.496342  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 65  Cumulative reward: -118.476390  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 70  Cumulative reward: -106.477288  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 75  Cumulative reward: -128.955349  Success rate: 0.0625 Mean loss: 0.0035\n","Epoch: 80  Cumulative reward: -132.972476  Success rate: 0.0625 Mean loss: 0.0034\n","Epoch: 85  Cumulative reward: -113.183923  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 90  Cumulative reward: -122.782342  Success rate: 0.0625 Mean loss: 0.0035\n","Epoch: 95  Cumulative reward: -117.431482  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 100  Cumulative reward: -120.233862  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 105  Cumulative reward: -118.875144  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 110  Cumulative reward: -112.056276  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 115  Cumulative reward: -126.033473  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 120  Cumulative reward: -123.076144  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 125  Cumulative reward: -112.569524  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 130  Cumulative reward: -111.430364  Success rate: 0.0625 Mean loss: 0.0035\n","Epoch: 135  Cumulative reward: -118.150741  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 140  Cumulative reward: -124.607833  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 145  Cumulative reward: -135.953591  Success rate: 0.0000 Mean loss: 0.0034\n"]}]},{"cell_type":"code","metadata":{"id":"Sbhq3UiESmon"},"source":["# If you chose to render:\n","# show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"1WSER-LSaN1jpY5eZ4odWfyHvaLqX1KSL"},"id":"PEIqeKErVMzA","executionInfo":{"status":"ok","timestamp":1632580207873,"user_tz":-180,"elapsed":6969,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"6d0d0c93-c704-4289-93c2-0c05c847eebf"},"source":["exp_to_accuracies = {\n","    \"sawyer_none\": success_rate,\n","}\n","text = 'Sawyer'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"jNHzqampMuHD","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1632581702040,"user_tz":-180,"elapsed":1230107,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"831e6e7f-ad91-4224-fc7b-f860bc08ca85"},"source":["#@title Goal Conditioned runs\n","#@markdown runs on Sawyer Environment with 4 variations of HER\n","#@markdown * **None**- do nothing\n","#@markdown * **Final**- for each sample in episode create another sample where the last state (in episode) is set to be the new goal\n","#@markdown * **Future**- for each sample in episode choose randomly num_relabeled \"tasks\" where the goals are drawn from future states in episode\n","#@markdown * **Random**- for each sample in episode choose randomly num_relabeled \"tasks\" where the goals are drawn from all states in episode\n","success_rate = run_sawyer(num_epochs=150, HER = \"None\")  # , render=True)\n","# show_video()\n","success_rate_final = run_sawyer(num_epochs=150, HER = \"final\")  # , render=True)\n","# show_video()\n","success_rate_future = run_sawyer(num_epochs=150, HER = \"future\")  # , render=True)\n","# show_video()\n","success_rate_random = run_sawyer(num_epochs=150, HER = \"random\")  # , render=True)\n","# show_video()"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  Cumulative reward: -165.408168  Success rate: 0.0625 Mean loss: 0.0088\n","Epoch: 5  Cumulative reward: -112.240136  Success rate: 0.0000 Mean loss: 0.0007\n","Epoch: 10  Cumulative reward: -110.875117  Success rate: 0.0000 Mean loss: 0.0004\n","Epoch: 15  Cumulative reward: -107.054125  Success rate: 0.0000 Mean loss: 0.0003\n","Epoch: 20  Cumulative reward: -91.146846  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 25  Cumulative reward: -73.888707  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 30  Cumulative reward: -77.863155  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 35  Cumulative reward: -58.276404  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 40  Cumulative reward: -57.413492  Success rate: 0.1250 Mean loss: 0.0001\n","Epoch: 45  Cumulative reward: -57.647278  Success rate: 0.0000 Mean loss: 0.0001\n","Epoch: 50  Cumulative reward: -56.713131  Success rate: 0.0625 Mean loss: 0.0001\n","Epoch: 55  Cumulative reward: -51.096889  Success rate: 0.1250 Mean loss: 0.0001\n","Epoch: 60  Cumulative reward: -53.026955  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 65  Cumulative reward: -60.318439  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 70  Cumulative reward: -50.278055  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 75  Cumulative reward: -52.776641  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 80  Cumulative reward: -55.291369  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 85  Cumulative reward: -55.037193  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 90  Cumulative reward: -60.469197  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 95  Cumulative reward: -51.017871  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 100  Cumulative reward: -55.542101  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 105  Cumulative reward: -51.779597  Success rate: 0.2500 Mean loss: 0.0003\n","Epoch: 110  Cumulative reward: -49.701859  Success rate: 0.1875 Mean loss: 0.0003\n","Epoch: 115  Cumulative reward: -54.688640  Success rate: 0.1875 Mean loss: 0.0003\n","Epoch: 120  Cumulative reward: -50.288927  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 125  Cumulative reward: -54.040080  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 130  Cumulative reward: -67.206695  Success rate: 0.0625 Mean loss: 0.0003\n","Epoch: 135  Cumulative reward: -42.249526  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 140  Cumulative reward: -43.667116  Success rate: 0.0625 Mean loss: 0.0003\n","Epoch: 145  Cumulative reward: -40.449740  Success rate: 0.2500 Mean loss: 0.0003\n","Epoch: 0  Cumulative reward: -103.010576  Success rate: 0.0625 Mean loss: 0.0036\n","Epoch: 5  Cumulative reward: -101.465603  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 10  Cumulative reward: -90.938502  Success rate: 0.0625 Mean loss: 0.0005\n","Epoch: 15  Cumulative reward: -98.936271  Success rate: 0.0000 Mean loss: 0.0003\n","Epoch: 20  Cumulative reward: -66.293316  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 25  Cumulative reward: -62.578073  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 30  Cumulative reward: -84.922746  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 35  Cumulative reward: -81.206396  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 40  Cumulative reward: -88.385723  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 45  Cumulative reward: -68.910877  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 50  Cumulative reward: -73.645150  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 55  Cumulative reward: -64.543153  Success rate: 0.0625 Mean loss: 0.0002\n","Epoch: 60  Cumulative reward: -52.151755  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 65  Cumulative reward: -60.125717  Success rate: 0.2500 Mean loss: 0.0002\n","Epoch: 70  Cumulative reward: -61.300128  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 75  Cumulative reward: -38.361527  Success rate: 0.2500 Mean loss: 0.0003\n","Epoch: 80  Cumulative reward: -62.668124  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 85  Cumulative reward: -41.977871  Success rate: 0.3750 Mean loss: 0.0003\n","Epoch: 90  Cumulative reward: -34.662109  Success rate: 0.2500 Mean loss: 0.0003\n","Epoch: 95  Cumulative reward: -37.003232  Success rate: 0.3125 Mean loss: 0.0003\n","Epoch: 100  Cumulative reward: -31.249273  Success rate: 0.3750 Mean loss: 0.0003\n","Epoch: 105  Cumulative reward: -26.710206  Success rate: 0.7500 Mean loss: 0.0003\n","Epoch: 110  Cumulative reward: -35.782173  Success rate: 0.4375 Mean loss: 0.0004\n","Epoch: 115  Cumulative reward: -32.984322  Success rate: 0.5000 Mean loss: 0.0003\n","Epoch: 120  Cumulative reward: -30.950028  Success rate: 0.3125 Mean loss: 0.0003\n","Epoch: 125  Cumulative reward: -27.051724  Success rate: 0.6250 Mean loss: 0.0003\n","Epoch: 130  Cumulative reward: -24.631850  Success rate: 0.6250 Mean loss: 0.0003\n","Epoch: 135  Cumulative reward: -25.096892  Success rate: 0.8750 Mean loss: 0.0003\n","Epoch: 140  Cumulative reward: -22.331322  Success rate: 0.7500 Mean loss: 0.0003\n","Epoch: 145  Cumulative reward: -23.756331  Success rate: 0.7500 Mean loss: 0.0003\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-01d0d20d8b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msuccess_rate_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sawyer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"final\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , render=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# show_video()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msuccess_rate_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sawyer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"future\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , render=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# show_video()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msuccess_rate_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sawyer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , render=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-874f2b6ee97a>\u001b[0m in \u001b[0;36mrun_sawyer\u001b[0;34m(num_epochs, buffer_size, batch_size, num_episodes, num_relabeled, gamma, log_interval, opt_steps, steps_per_episode, render, HER)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0msuccesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msucceeded\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# track whether we succeeded in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mupdate_replay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_relabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_experience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHER\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# add to the replay buffer; use specified  HER policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-893d5f607cec>\u001b[0m in \u001b[0;36mupdate_replay_buffer\u001b[0;34m(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mvec_orig_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mnum_goals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_bits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_relabeled\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# create a legal number of goals per step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mg_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_goals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mg_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg_idxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_bits' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HO17hu3dXga","executionInfo":{"status":"ok","timestamp":1632583337890,"user_tz":-180,"elapsed":1299440,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"ce213af3-399e-435a-ab80-85a1c6c1ae1c"},"source":["success_rate_future = run_sawyer(num_epochs=150, HER = \"future\")  # , render=True)\n","# show_video()\n","success_rate_random = run_sawyer(num_epochs=150, HER = \"random\")  # , render=True)"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0  Cumulative reward: -110.252157  Success rate: 0.0000 Mean loss: 0.0018\n","Epoch: 5  Cumulative reward: -74.104295  Success rate: 0.0000 Mean loss: 0.0006\n","Epoch: 10  Cumulative reward: -69.927301  Success rate: 0.1250 Mean loss: 0.0003\n","Epoch: 15  Cumulative reward: -67.824209  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 20  Cumulative reward: -76.929618  Success rate: 0.1250 Mean loss: 0.0001\n","Epoch: 25  Cumulative reward: -77.338564  Success rate: 0.0625 Mean loss: 0.0001\n","Epoch: 30  Cumulative reward: -68.177702  Success rate: 0.0625 Mean loss: 0.0001\n","Epoch: 35  Cumulative reward: -60.446472  Success rate: 0.0625 Mean loss: 0.0001\n","Epoch: 40  Cumulative reward: -82.217323  Success rate: 0.3750 Mean loss: 0.0001\n","Epoch: 45  Cumulative reward: -49.072385  Success rate: 0.2500 Mean loss: 0.0001\n","Epoch: 50  Cumulative reward: -61.398574  Success rate: 0.3125 Mean loss: 0.0001\n","Epoch: 55  Cumulative reward: -54.703290  Success rate: 0.1250 Mean loss: 0.0001\n","Epoch: 60  Cumulative reward: -56.316149  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 65  Cumulative reward: -46.341916  Success rate: 0.4375 Mean loss: 0.0002\n","Epoch: 70  Cumulative reward: -52.660208  Success rate: 0.4375 Mean loss: 0.0002\n","Epoch: 75  Cumulative reward: -50.192656  Success rate: 0.3125 Mean loss: 0.0003\n","Epoch: 80  Cumulative reward: -39.432385  Success rate: 0.2500 Mean loss: 0.0003\n","Epoch: 85  Cumulative reward: -29.842194  Success rate: 0.3750 Mean loss: 0.0003\n","Epoch: 90  Cumulative reward: -26.618233  Success rate: 0.4375 Mean loss: 0.0003\n","Epoch: 95  Cumulative reward: -34.130563  Success rate: 0.5000 Mean loss: 0.0003\n","Epoch: 100  Cumulative reward: -31.055833  Success rate: 0.4375 Mean loss: 0.0004\n","Epoch: 105  Cumulative reward: -32.919310  Success rate: 0.3125 Mean loss: 0.0003\n","Epoch: 110  Cumulative reward: -25.257614  Success rate: 0.6875 Mean loss: 0.0003\n","Epoch: 115  Cumulative reward: -28.795661  Success rate: 0.5000 Mean loss: 0.0003\n","Epoch: 120  Cumulative reward: -22.185052  Success rate: 0.6250 Mean loss: 0.0003\n","Epoch: 125  Cumulative reward: -28.804668  Success rate: 0.5000 Mean loss: 0.0003\n","Epoch: 130  Cumulative reward: -30.504275  Success rate: 0.6250 Mean loss: 0.0003\n","Epoch: 135  Cumulative reward: -27.020540  Success rate: 0.6875 Mean loss: 0.0003\n","Epoch: 140  Cumulative reward: -23.664943  Success rate: 0.7500 Mean loss: 0.0003\n","Epoch: 145  Cumulative reward: -21.134365  Success rate: 0.8750 Mean loss: 0.0003\n","Epoch: 0  Cumulative reward: -126.869085  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 5  Cumulative reward: -83.621010  Success rate: 0.0000 Mean loss: 0.0006\n","Epoch: 10  Cumulative reward: -62.412405  Success rate: 0.1875 Mean loss: 0.0003\n","Epoch: 15  Cumulative reward: -74.165490  Success rate: 0.0000 Mean loss: 0.0002\n","Epoch: 20  Cumulative reward: -59.934593  Success rate: 0.0625 Mean loss: 0.0001\n","Epoch: 25  Cumulative reward: -51.271969  Success rate: 0.2500 Mean loss: 0.0001\n","Epoch: 30  Cumulative reward: -44.746804  Success rate: 0.1875 Mean loss: 0.0001\n","Epoch: 35  Cumulative reward: -49.355825  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 40  Cumulative reward: -48.249593  Success rate: 0.1250 Mean loss: 0.0002\n","Epoch: 45  Cumulative reward: -60.285556  Success rate: 0.3125 Mean loss: 0.0002\n","Epoch: 50  Cumulative reward: -44.017302  Success rate: 0.3750 Mean loss: 0.0002\n","Epoch: 55  Cumulative reward: -50.664662  Success rate: 0.5000 Mean loss: 0.0002\n","Epoch: 60  Cumulative reward: -37.184032  Success rate: 0.3750 Mean loss: 0.0002\n","Epoch: 65  Cumulative reward: -31.677057  Success rate: 0.3750 Mean loss: 0.0002\n","Epoch: 70  Cumulative reward: -24.343589  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 75  Cumulative reward: -26.853540  Success rate: 0.5625 Mean loss: 0.0002\n","Epoch: 80  Cumulative reward: -36.304145  Success rate: 0.2500 Mean loss: 0.0002\n","Epoch: 85  Cumulative reward: -23.171179  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 90  Cumulative reward: -24.027505  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 95  Cumulative reward: -21.325478  Success rate: 0.7500 Mean loss: 0.0002\n","Epoch: 100  Cumulative reward: -25.976973  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 105  Cumulative reward: -29.294548  Success rate: 0.6875 Mean loss: 0.0003\n","Epoch: 110  Cumulative reward: -23.025328  Success rate: 0.5625 Mean loss: 0.0002\n","Epoch: 115  Cumulative reward: -23.933591  Success rate: 0.8125 Mean loss: 0.0002\n","Epoch: 120  Cumulative reward: -25.446318  Success rate: 0.7500 Mean loss: 0.0002\n","Epoch: 125  Cumulative reward: -22.180495  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 130  Cumulative reward: -29.379814  Success rate: 0.6250 Mean loss: 0.0002\n","Epoch: 135  Cumulative reward: -27.721279  Success rate: 0.5000 Mean loss: 0.0002\n","Epoch: 140  Cumulative reward: -24.660403  Success rate: 0.6250 Mean loss: 0.0003\n","Epoch: 145  Cumulative reward: -24.147565  Success rate: 0.8125 Mean loss: 0.0003\n"]}]},{"cell_type":"code","metadata":{"id":"JMZkjOuwNAas","colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"1BcBPCvfgfmiXJUkqami7-UfVLGeHYRi9"},"executionInfo":{"status":"ok","timestamp":1632583375598,"user_tz":-180,"elapsed":6374,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"575589b5-0f18-4113-ab32-f96b865c1647"},"source":["exp_to_accuracies = {\n","    \"sawyer_her_none\": success_rate,\n","    \"sawyer_her_final\": success_rate_final,\n","    \"sawyer_her_future\": success_rate_future,\n","    \"sawyer_her_random\": success_rate_random,\n","}\n","text = 'Sawyer (Goal-conditioned)'\n","fig = plot_success(exp_to_accuracies, text)\n","HTML(fig.to_html())"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"2UKXENISPtf5"},"source":["# Plotting Code\n","\n","We've provided some sample plotting code for you. Feel free to customize it per the assignment specifications. The code will not be graded."]},{"cell_type":"code","metadata":{"id":"NOzYeO5EAo6c"},"source":["# !pip install plotly"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NC1PTQAxtwP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrP-V9ZJHW-3","colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"14BU5XYv-XOkVl0DBpk_ZdfV3T6IwDbTD"},"executionInfo":{"status":"ok","timestamp":1632490309914,"user_tz":-180,"elapsed":2112,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"133bdb0f-3367-4aa1-d602-d7dc36d3ef9a"},"source":["from IPython.display import HTML\n","from plotly import graph_objs as go\n","\n","# Sample plotting clode (replace successes where necessary)\n","exp_to_accuracies = {\n","    \"bitflip_7_her_none\": success_rate,\n","    \"bitflip_7_her_ final\": success_rate_final\n","}\n","\n","# Creates the Figure\n","fig = go.Figure()\n","data = []\n","for experiment, accuracies in exp_to_accuracies.items():\n","  steps = range(len(accuracies))\n","  steps = [5 * x for x in steps]\n","  data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n","\n","# Applies a custom layout\n","layout = go.Layout(\n","    title=go.layout.Title(\n","        text='Bitflip with 7 bits',\n","        x=0.5\n","    ),\n","    xaxis=go.layout.XAxis(\n","        title=go.layout.xaxis.Title(\n","            text='Epoch',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    ),\n","    yaxis=go.layout.YAxis(\n","        title=go.layout.yaxis.Title(\n","            text='Success Rate',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    )\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"GqMV9wr0Jdn9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632489871677,"user_tz":-180,"elapsed":247,"user":{"displayName":"Gal Kampel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13734932312114013687"}},"outputId":"046111ff-3268-4fcb-ac9b-e6b0f9c451b3"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs330_fall2020/multiworld\n"]}]},{"cell_type":"code","metadata":{"id":"BVjlEccH4Ao6"},"source":[""],"execution_count":null,"outputs":[]}]}